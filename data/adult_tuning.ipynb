{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xgboost\n",
    "import copy\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"raw/adult.0.train.csv\", encoding='latin1', \n",
    "                 names=['age','workclass','fnlwgt','education',\n",
    "                         'education_num','marital_status','occupation',\n",
    "                         'relationship','race','sex','capital_gain','capital_loss',\n",
    "                         'hours_per_week','native_country','income'],\n",
    "                 na_values='?',\n",
    "                 low_memory=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"raw/adult.0.test\", encoding='latin1', \n",
    "                 names=['age','workclass','fnlwgt','education',\n",
    "                         'education_num','marital_status','occupation',\n",
    "                         'relationship','race','sex','capital_gain','capital_loss',\n",
    "                         'hours_per_week','native_country','income'],\n",
    "                 na_values='?',\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covert the output as binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['over_50k'] = np.where(train.income == '>50K', 1, 0)\n",
    "train=train.drop(['income'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['over_50k'] = np.where(test.income == '>50K', 1, 0)\n",
    "test=test.drop(['income'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the numeric number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,'age'] = pd.to_numeric(train['age'], downcast='integer', errors='coerce')\n",
    "train.loc[:,'fnlwgt'] = pd.to_numeric(train['fnlwgt'], downcast='float', errors='coerce')\n",
    "train.loc[:,'age'] = pd.to_numeric(train['age'], downcast='integer', errors='coerce')\n",
    "train.loc[:,'capital_gain'] = pd.to_numeric(train['capital_gain'], downcast='float', errors='coerce')\n",
    "train.loc[:,'capital_loss'] = pd.to_numeric(train['capital_loss'], downcast='float', errors='coerce')\n",
    "train.loc[:,'hours_per_week'] = pd.to_numeric(train['hours_per_week'], downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:,'age'] = pd.to_numeric(test['age'], downcast='integer', errors='coerce')\n",
    "test.loc[:,'fnlwgt'] = pd.to_numeric(test['fnlwgt'], downcast='float', errors='coerce')\n",
    "test.loc[:,'age'] = pd.to_numeric(test['age'], downcast='integer', errors='coerce')\n",
    "test.loc[:,'capital_gain'] = pd.to_numeric(test['capital_gain'], downcast='float', errors='coerce')\n",
    "test.loc[:,'capital_loss'] = pd.to_numeric(test['capital_loss'], downcast='float', errors='coerce')\n",
    "test.loc[:,'hours_per_week'] = pd.to_numeric(test['hours_per_week'], downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         1379\n",
       "fnlwgt               0\n",
       "education            0\n",
       "education_num        0\n",
       "marital_status       0\n",
       "occupation        1384\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital_gain         0\n",
       "capital_loss         0\n",
       "hours_per_week       0\n",
       "native_country     431\n",
       "over_50k             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         1420\n",
       "fnlwgt               0\n",
       "education            0\n",
       "education_num        0\n",
       "marital_status       0\n",
       "occupation        1425\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital_gain         0\n",
       "capital_loss         0\n",
       "hours_per_week       0\n",
       "native_country     426\n",
       "over_50k             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3291: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "train['education'] = train['education'].astype('category',\n",
    "                                               categories=['Bachelors', 'Some-college', '11th', 'HS-grad', \n",
    "                                                           'Prof-school', \n",
    "                                                           'Assoc-acdm', 'Assoc-voc', '9th', '7th-8th',\n",
    "                                                           '12th', 'Masters', '1st-4th', '10th', \n",
    "                                                           'Doctorate', '5th-6th', 'Preschool'])\n",
    "train['marital_status'] = train['marital_status'].astype('category',\n",
    "                                                         categories=['Married-civ-spouse', 'Divorced', \n",
    "                                                                     'Never-married', 'Separated', \n",
    "                                                                     'Widowed', 'Married-spouse-absent', \n",
    "                                                                     'Married-AF-spouse'])\n",
    "train['relationship'] = train['relationship'].astype('category',\n",
    "                                                     categories=['Wife', 'Own-child', 'Husband', \n",
    "                                                                 'Not-in-family', 'Other-relative', 'Unmarried'])\n",
    "train['race'] = train['race'].astype('category',\n",
    "                                     categories=['White', 'Asian-Pac-Islander', \n",
    "                                                 'Amer-Indian-Eskimo', 'Other', 'Black'])\n",
    "train['sex'] = train['sex'].astype('category', \n",
    "                                   categories=['Female', 'Male'])\n",
    "\n",
    "\n",
    "train['workclass'] = train['workclass'].astype('category',\n",
    "                                               categories=['Private', 'Self-emp-not-inc', \n",
    "                                                           'Self-emp-inc', 'Federal-gov', \n",
    "                                                           'Local-gov', 'State-gov', \n",
    "                                                           'Without-pay', 'Never-worked'])\n",
    "train['occupation'] = train['occupation'].astype('category',\n",
    "                                                 categories=['Tech-support', 'Craft-repair', \n",
    "                                                             'Other-service', 'Sales', 'Exec-managerial',\n",
    "                                                             'Prof-specialty', 'Handlers-cleaners', \n",
    "                                                             'Machine-op-inspct', 'Adm-clerical',\n",
    "                                                             'Farming-fishing', 'Transport-moving', \n",
    "                                                             'Priv-house-serv',\n",
    "                                                             'Protective-serv', 'Armed-Forces'])\n",
    "train['native_country'] = train['native_country'].astype('category',\n",
    "                                                         categories=['United-States',\n",
    "                                                                                 'Cambodia',\n",
    "                                                                                 'England',\n",
    "                                                                                 'Puerto-Rico',\n",
    "                                                                                 'Canada',\n",
    "                                                                                 'Germany',\n",
    "                                                                                 'Outlying-US(Guam-USVI-etc)',\n",
    "                                                                                 'India',\n",
    "                                                                                 'Japan',\n",
    "                                                                                 'Greece',\n",
    "                                                                                 'South',\n",
    "                                                                                 'China',\n",
    "                                                                                 'Cuba',\n",
    "                                                                                 'Iran',\n",
    "                                                                                 'Honduras',\n",
    "                                                                                 'Philippines',\n",
    "                                                                                 'Italy',\n",
    "                                                                                 'Poland',\n",
    "                                                                                 'Jamaica',\n",
    "                                                                                 'Vietnam',\n",
    "                                                                                 'Mexico',\n",
    "                                                                                 'Portugal',\n",
    "                                                                                 'Ireland',\n",
    "                                                                                 'France',\n",
    "                                                                                 'Dominican-Republic',\n",
    "                                                                                 'Laos',\n",
    "                                                                                 'Ecuador',\n",
    "                                                                                 'Taiwan',\n",
    "                                                                                 'Haiti',\n",
    "                                                                                 'Columbia',\n",
    "                                                                                 'Hungary',\n",
    "                                                                                 'Guatemala',\n",
    "                                                                                 'Nicaragua',\n",
    "                                                                                 'Scotland',\n",
    "                                                                                 'Thailand',\n",
    "                                                                                 'Yugoslavia',\n",
    "                                                                                 'El-Salvador',\n",
    "                                                                                 'Trinadad&Tobago',\n",
    "                                                                                 'Peru',\n",
    "                                                                                 'Hong',\n",
    "                                                                                 'Holand-Netherlands'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=['education','marital_status','relationship','race','sex'])\n",
    "train = pd.get_dummies(train, columns=['workclass','occupation','native_country'], dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['education'] = test['education'].astype('category',\n",
    "                                               categories=['Bachelors', 'Some-college', '11th', 'HS-grad', \n",
    "                                                           'Prof-school', \n",
    "                                                           'Assoc-acdm', 'Assoc-voc', '9th', '7th-8th',\n",
    "                                                           '12th', 'Masters', '1st-4th', '10th', \n",
    "                                                           'Doctorate', '5th-6th', 'Preschool'])\n",
    "test['marital_status'] = test['marital_status'].astype('category',\n",
    "                                                         categories=['Married-civ-spouse', 'Divorced', \n",
    "                                                                     'Never-married', 'Separated', \n",
    "                                                                     'Widowed', 'Married-spouse-absent', \n",
    "                                                                     'Married-AF-spouse'])\n",
    "test['relationship'] = test['relationship'].astype('category',\n",
    "                                                     categories=['Wife', 'Own-child', 'Husband', \n",
    "                                                                 'Not-in-family', 'Other-relative', 'Unmarried'])\n",
    "test['race'] = test['race'].astype('category',\n",
    "                                    categories=['White', 'Asian-Pac-Islander', \n",
    "                                                 'Amer-Indian-Eskimo', 'Other', 'Black'])\n",
    "test['sex'] = test['sex'].astype('category',\n",
    "                                   categories=['Female', 'Male'])\n",
    "\n",
    "\n",
    "test['workclass'] = test['workclass'].astype('category',\n",
    "                                               categories=['Private', 'Self-emp-not-inc', \n",
    "                                                           'Self-emp-inc', 'Federal-gov', \n",
    "                                                           'Local-gov', 'State-gov', \n",
    "                                                           'Without-pay', 'Never-worked'])\n",
    "test['occupation'] = test['occupation'].astype('category',\n",
    "                                                categories=['Tech-support', 'Craft-repair', \n",
    "                                                             'Other-service', 'Sales', 'Exec-managerial',\n",
    "                                                             'Prof-specialty', 'Handlers-cleaners', \n",
    "                                                             'Machine-op-inspct', 'Adm-clerical',\n",
    "                                                             'Farming-fishing', 'Transport-moving', \n",
    "                                                             'Priv-house-serv',\n",
    "                                                             'Protective-serv', 'Armed-Forces'])\n",
    "test['native_country'] = test['native_country'].astype('category',                                                         \n",
    "                                                         categories=['United-States',\n",
    "                                                                                 'Cambodia',\n",
    "                                                                                 'England',\n",
    "                                                                                 'Puerto-Rico',\n",
    "                                                                                 'Canada',\n",
    "                                                                                 'Germany',\n",
    "                                                                                 'Outlying-US(Guam-USVI-etc)',\n",
    "                                                                                 'India',\n",
    "                                                                                 'Japan',\n",
    "                                                                                 'Greece',\n",
    "                                                                                 'South',\n",
    "                                                                                 'China',\n",
    "                                                                                 'Cuba',\n",
    "                                                                                 'Iran',\n",
    "                                                                                 'Honduras',\n",
    "                                                                                 'Philippines',\n",
    "                                                                                 'Italy',\n",
    "                                                                                 'Poland',\n",
    "                                                                                 'Jamaica',\n",
    "                                                                                 'Vietnam',\n",
    "                                                                                 'Mexico',\n",
    "                                                                                 'Portugal',\n",
    "                                                                                 'Ireland',\n",
    "                                                                                 'France',\n",
    "                                                                                 'Dominican-Republic',\n",
    "                                                                                 'Laos',\n",
    "                                                                                 'Ecuador',\n",
    "                                                                                 'Taiwan',\n",
    "                                                                                 'Haiti',\n",
    "                                                                                 'Columbia',\n",
    "                                                                                 'Hungary',\n",
    "                                                                                 'Guatemala',\n",
    "                                                                                 'Nicaragua',\n",
    "                                                                                 'Scotland',\n",
    "                                                                                 'Thailand',\n",
    "                                                                                 'Yugoslavia',\n",
    "                                                                                 'El-Salvador',\n",
    "                                                                                 'Trinadad&Tobago',\n",
    "                                                                                 'Peru',\n",
    "                                                                                 'Hong',\n",
    "                                                                                 'Holand-Netherlands'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.get_dummies(test, columns=['education','marital_status','relationship','race','sex'])\n",
    "test = pd.get_dummies(test, columns=['workclass','occupation','native_country'], dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['over_50k'], axis=1)\n",
    "y_train = train.over_50k\n",
    "\n",
    "X_test = test.drop(['over_50k'], axis=1)\n",
    "y_test = test.over_50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = X_train.align(X_test, join='outer', fill_value=0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24421, 108)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24421, 108)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss',\n",
       "       'hours_per_week', 'education_Bachelors', 'education_Some-college',\n",
       "       'education_11th', 'education_HS-grad',\n",
       "       ...\n",
       "       'native_country_Nicaragua', 'native_country_Scotland',\n",
       "       'native_country_Thailand', 'native_country_Yugoslavia',\n",
       "       'native_country_El-Salvador', 'native_country_Trinadad&Tobago',\n",
       "       'native_country_Peru', 'native_country_Hong',\n",
       "       'native_country_Holand-Netherlands', 'native_country_nan'],\n",
       "      dtype='object', length=108)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss',\n",
       "       'hours_per_week', 'education_Bachelors', 'education_Some-college',\n",
       "       'education_11th', 'education_HS-grad',\n",
       "       ...\n",
       "       'native_country_Nicaragua', 'native_country_Scotland',\n",
       "       'native_country_Thailand', 'native_country_Yugoslavia',\n",
       "       'native_country_El-Salvador', 'native_country_Trinadad&Tobago',\n",
       "       'native_country_Peru', 'native_country_Hong',\n",
       "       'native_country_Holand-Netherlands', 'native_country_nan'],\n",
       "      dtype='object', length=108)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tuning on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.167069</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.172639</td>\n",
       "      <td>0.012472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144885</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.148725</td>\n",
       "      <td>0.004196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147342</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.151468</td>\n",
       "      <td>0.005173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.145203</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.148315</td>\n",
       "      <td>0.005444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.144445</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.147742</td>\n",
       "      <td>0.004791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.144251</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.147455</td>\n",
       "      <td>0.005365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.143708</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.147046</td>\n",
       "      <td>0.005066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.143545</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.004553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.143033</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.146268</td>\n",
       "      <td>0.004691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.143381</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.003970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.142214</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.144958</td>\n",
       "      <td>0.004884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.142132</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.145408</td>\n",
       "      <td>0.004074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.141784</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.145285</td>\n",
       "      <td>0.004178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.141487</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.143975</td>\n",
       "      <td>0.004840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.141098</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.143442</td>\n",
       "      <td>0.005158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.140791</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.143197</td>\n",
       "      <td>0.004545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.140484</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.143278</td>\n",
       "      <td>0.004398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.140371</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.143033</td>\n",
       "      <td>0.004303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.140207</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.142542</td>\n",
       "      <td>0.004103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.140033</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.142419</td>\n",
       "      <td>0.004352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.140013</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.142337</td>\n",
       "      <td>0.003690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.139777</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.141927</td>\n",
       "      <td>0.004264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.139440</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.141968</td>\n",
       "      <td>0.004179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.138887</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.141886</td>\n",
       "      <td>0.004283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.138805</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.141804</td>\n",
       "      <td>0.004335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.138262</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.141354</td>\n",
       "      <td>0.004248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.138037</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.141231</td>\n",
       "      <td>0.003817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.137965</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.140986</td>\n",
       "      <td>0.003712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.137628</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.140781</td>\n",
       "      <td>0.003557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.137269</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.140535</td>\n",
       "      <td>0.003664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.115935</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.115822</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.129520</td>\n",
       "      <td>0.001518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.115577</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.129479</td>\n",
       "      <td>0.001450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.115434</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.129602</td>\n",
       "      <td>0.001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.115484</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.129479</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.115198</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.129724</td>\n",
       "      <td>0.001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.115003</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.114891</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.114819</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.129397</td>\n",
       "      <td>0.001303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.114666</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>0.001362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.114594</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.129315</td>\n",
       "      <td>0.001394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.114543</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.114430</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>0.001446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.114154</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.129274</td>\n",
       "      <td>0.001338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.113929</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.129643</td>\n",
       "      <td>0.001563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.113765</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.113580</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.129602</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.113468</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.129520</td>\n",
       "      <td>0.001404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.113345</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.129479</td>\n",
       "      <td>0.001452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.113222</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.113099</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.129315</td>\n",
       "      <td>0.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.113089</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.129192</td>\n",
       "      <td>0.001432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.112936</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.129192</td>\n",
       "      <td>0.001578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.112915</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.129233</td>\n",
       "      <td>0.001446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.112772</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.129274</td>\n",
       "      <td>0.001452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.112649</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.129397</td>\n",
       "      <td>0.001377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.112731</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>0.001361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.112690</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.129397</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.112516</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.112321</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.129110</td>\n",
       "      <td>0.001411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.167069         0.008100         0.172639        0.012472\n",
       "1            0.144885         0.002585         0.148725        0.004196\n",
       "2            0.147342         0.002714         0.151468        0.005173\n",
       "3            0.145203         0.000719         0.148315        0.005444\n",
       "4            0.144445         0.001555         0.147742        0.004791\n",
       "5            0.144251         0.001563         0.147455        0.005365\n",
       "6            0.143708         0.001650         0.147046        0.005066\n",
       "7            0.143545         0.001366         0.146800        0.004553\n",
       "8            0.143033         0.001004         0.146268        0.004691\n",
       "9            0.143381         0.001173         0.146186        0.003970\n",
       "10           0.142214         0.001088         0.144958        0.004884\n",
       "11           0.142132         0.001469         0.145408        0.004074\n",
       "12           0.141784         0.001858         0.145285        0.004178\n",
       "13           0.141487         0.001324         0.143975        0.004840\n",
       "14           0.141098         0.001480         0.143442        0.005158\n",
       "15           0.140791         0.000961         0.143197        0.004545\n",
       "16           0.140484         0.000897         0.143278        0.004398\n",
       "17           0.140371         0.000767         0.143033        0.004303\n",
       "18           0.140207         0.000854         0.142542        0.004103\n",
       "19           0.140033         0.000987         0.142419        0.004352\n",
       "20           0.140013         0.001066         0.142337        0.003690\n",
       "21           0.139777         0.001213         0.141927        0.004264\n",
       "22           0.139440         0.001378         0.141968        0.004179\n",
       "23           0.138887         0.001354         0.141886        0.004283\n",
       "24           0.138805         0.001348         0.141804        0.004335\n",
       "25           0.138262         0.001232         0.141354        0.004248\n",
       "26           0.138037         0.001152         0.141231        0.003817\n",
       "27           0.137965         0.001109         0.140986        0.003712\n",
       "28           0.137628         0.001342         0.140781        0.003557\n",
       "29           0.137269         0.001207         0.140535        0.003664\n",
       "..                ...              ...              ...             ...\n",
       "130          0.115935         0.000840         0.129438        0.001421\n",
       "131          0.115822         0.000807         0.129520        0.001518\n",
       "132          0.115577         0.000781         0.129479        0.001450\n",
       "133          0.115434         0.000759         0.129602        0.001214\n",
       "134          0.115484         0.000826         0.129479        0.001211\n",
       "135          0.115198         0.000847         0.129724        0.001025\n",
       "136          0.115003         0.000749         0.129438        0.001217\n",
       "137          0.114891         0.000757         0.129438        0.001374\n",
       "138          0.114819         0.000721         0.129397        0.001303\n",
       "139          0.114666         0.000690         0.129356        0.001362\n",
       "140          0.114594         0.000758         0.129315        0.001394\n",
       "141          0.114543         0.000809         0.129438        0.001343\n",
       "142          0.114430         0.000747         0.129356        0.001446\n",
       "143          0.114154         0.000844         0.129274        0.001338\n",
       "144          0.113929         0.000677         0.129643        0.001563\n",
       "145          0.113765         0.000744         0.129438        0.001600\n",
       "146          0.113580         0.000533         0.129602        0.001609\n",
       "147          0.113468         0.000482         0.129520        0.001404\n",
       "148          0.113345         0.000533         0.129479        0.001452\n",
       "149          0.113222         0.000533         0.129356        0.001462\n",
       "150          0.113099         0.000515         0.129315        0.001465\n",
       "151          0.113089         0.000596         0.129192        0.001432\n",
       "152          0.112936         0.000629         0.129192        0.001578\n",
       "153          0.112915         0.000539         0.129233        0.001446\n",
       "154          0.112772         0.000663         0.129274        0.001452\n",
       "155          0.112649         0.000653         0.129397        0.001377\n",
       "156          0.112731         0.000799         0.129151        0.001361\n",
       "157          0.112690         0.000705         0.129397        0.001489\n",
       "158          0.112516         0.000569         0.129438        0.001548\n",
       "159          0.112321         0.000551         0.129110        0.001411\n",
       "\n",
       "[160 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 0 best score:  0.8694566152082225\n",
      "Run 1 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 1 best score:  0.8706441177674952\n",
      "Run 2 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 2 best score:  0.8706031694033823\n",
      "Run 3 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 3 best score:  0.8713811883215266\n",
      "Run 4 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 4 best score:  0.8698251504852381\n",
      "Run 5 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 5 best score:  0.8696204086646738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 6 best score:  0.8694975635723353\n",
      "Run 7 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 7 best score:  0.8697842021211253\n",
      "Run 8 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 8 best score:  0.8706031694033823\n",
      "Best params:  params               {'max_depth': 5, 'min_child_weight': 1}\n",
      "mean_test_score_0                                   0.869457\n",
      "mean_test_score_1                                   0.870644\n",
      "mean_test_score_2                                   0.870603\n",
      "mean_test_score_3                                   0.871381\n",
      "mean_test_score_4                                   0.869825\n",
      "mean_test_score_5                                    0.86962\n",
      "mean_test_score_6                                   0.869498\n",
      "mean_test_score_7                                   0.869784\n",
      "mean_test_score_8                                   0.870603\n",
      "avg                                                 0.870157\n",
      "Name: 12, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1 = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1,300,50)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=159,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1.fit(X_train,y_train)    \n",
    "    if grid_score1.empty:\n",
    "        grid_score1 = pd.DataFrame(gsearch1.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1.best_score_)\n",
    "\n",
    "grid_score1['avg'] = grid_score1.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1.loc[grid_score1.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 0 best score:  0.8699889439416896\n",
      "Run 1 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 1 best score:  0.870685066131608\n",
      "Run 2 best param:  {'max_depth': 5, 'min_child_weight': 5}\n",
      "Run 2 best score:  0.8714221366856394\n",
      "Run 3 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 3 best score:  0.8713811883215266\n",
      "Run 4 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 4 best score:  0.8704393759469309\n",
      "Run 5 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 5 best score:  0.8696204086646738\n",
      "Run 6 best param:  {'max_depth': 5, 'min_child_weight': 5}\n",
      "Run 6 best score:  0.869866098849351\n",
      "Run 7 best param:  {'max_depth': 5, 'min_child_weight': 5}\n",
      "Run 7 best score:  0.8698251504852381\n",
      "Run 8 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 8 best score:  0.8706031694033823\n",
      "Best params:  params               {'max_depth': 5, 'min_child_weight': 3}\n",
      "mean_test_score_0                                   0.869989\n",
      "mean_test_score_1                                   0.870685\n",
      "mean_test_score_2                                   0.871299\n",
      "mean_test_score_3                                   0.871054\n",
      "mean_test_score_4                                   0.870439\n",
      "mean_test_score_5                                   0.869375\n",
      "mean_test_score_6                                   0.869702\n",
      "mean_test_score_7                                    0.86962\n",
      "mean_test_score_8                                   0.869661\n",
      "avg                                                 0.870203\n",
      "Name: 11, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1b = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1, 10, 2)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1b = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=159,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1b = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1b,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1b.fit(X_train,y_train)    \n",
    "    if grid_score1b.empty:\n",
    "        grid_score1b = pd.DataFrame(gsearch1b.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1b.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1b['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1b.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1b.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1b.best_score_)\n",
    "\n",
    "grid_score1b['avg'] = grid_score1b.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1b.loc[grid_score1b.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 0 best score:  0.8699889439416896\n",
      "Run 1 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 1 best score:  0.870685066131608\n",
      "Run 2 best param:  {'max_depth': 5, 'min_child_weight': 4}\n",
      "Run 2 best score:  0.8717087752344294\n",
      "Run 3 best param:  {'max_depth': 6, 'min_child_weight': 3}\n",
      "Run 3 best score:  0.8713811883215266\n",
      "Run 4 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 4 best score:  0.8704393759469309\n",
      "Run 5 best param:  {'max_depth': 6, 'min_child_weight': 3}\n",
      "Run 5 best score:  0.8704393759469309\n",
      "Run 6 best param:  {'max_depth': 6, 'min_child_weight': 3}\n",
      "Run 6 best score:  0.8706031694033823\n",
      "Run 7 best param:  {'max_depth': 6, 'min_child_weight': 2}\n",
      "Run 7 best score:  0.8708079112239466\n",
      "Run 8 best param:  {'max_depth': 5, 'min_child_weight': 4}\n",
      "Run 8 best score:  0.8702346341263667\n",
      "Best params:  params               {'max_depth': 5, 'min_child_weight': 3}\n",
      "mean_test_score_0                                   0.869989\n",
      "mean_test_score_1                                   0.870685\n",
      "mean_test_score_2                                   0.871299\n",
      "mean_test_score_3                                   0.871054\n",
      "mean_test_score_4                                   0.870439\n",
      "mean_test_score_5                                   0.869375\n",
      "mean_test_score_6                                   0.869702\n",
      "mean_test_score_7                                    0.86962\n",
      "mean_test_score_8                                   0.869661\n",
      "avg                                                 0.870203\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look carefully again the neigbor values\n",
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test2 = {\n",
    " 'max_depth':[4, 5, 6],\n",
    " 'min_child_weight':[2, 3 ,4]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score2 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=159,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,        \n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test2,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch2.fit(X_train,y_train)    \n",
    "    if grid_score2.empty:\n",
    "        grid_score2 = pd.DataFrame(gsearch2.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score2.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score2['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch2.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch2.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch2.best_score_)\n",
    "\n",
    "grid_score2['avg'] = grid_score2.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score2.loc[grid_score2.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score3 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=148,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch3 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test3,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch3.fit(X_train,y_train)    \n",
    "    if grid_score3.empty:\n",
    "        grid_score3 = pd.DataFrame(gsearch3.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score3.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score3['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch3.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch3.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch3.best_score_)\n",
    "\n",
    "grid_score3['avg'] = grid_score3.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score3.loc[grid_score3.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recablirating the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=2,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score4 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=195,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test4,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch4.fit(X_train,y_train)    \n",
    "    if grid_score4.empty:\n",
    "        grid_score4 = pd.DataFrame(gsearch4.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score4.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score4['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch4.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch4.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch4.best_score_)\n",
    "\n",
    "grid_score4['avg'] = grid_score4.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score4.loc[grid_score4.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carefully search for each neighboring 0.05\n",
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(85,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(55,70,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score5 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=195,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch5 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test5,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch5.fit(X_train,y_train)    \n",
    "    if grid_score5.empty:\n",
    "        grid_score5 = pd.DataFrame(gsearch5.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score5.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score5['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch5.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch5.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch5.best_score_)\n",
    "\n",
    "grid_score5['avg'] = grid_score5.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score5.loc[grid_score5.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score6 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=195,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        gamma=0.1,\n",
    "        subsample=0.95,\n",
    "        colsample_bytree=0.6,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch6 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test6,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch6.fit(X_train,y_train)    \n",
    "    if grid_score6.empty:\n",
    "        grid_score6 = pd.DataFrame(gsearch6.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score6.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score6['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch6.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch6.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch6.best_score_)\n",
    "\n",
    "grid_score6['avg'] = grid_score6.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score6.loc[grid_score6.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score7 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=195,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        gamma=0.1,\n",
    "        subsample=0.95,\n",
    "        colsample_bytree=0.6,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch7 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test7,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch7.fit(X_train,y_train)    \n",
    "    if grid_score7.empty:\n",
    "        grid_score7 = pd.DataFrame(gsearch7.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score7.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score7['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch7.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch7.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch7.best_score_)\n",
    "\n",
    "grid_score7['avg'] = grid_score7.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score7.loc[grid_score7.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the learning rate and tune n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.168912</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.173458</td>\n",
       "      <td>0.009540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.150885</td>\n",
       "      <td>0.008856</td>\n",
       "      <td>0.154826</td>\n",
       "      <td>0.012632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.155123</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.158798</td>\n",
       "      <td>0.010664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148837</td>\n",
       "      <td>0.005181</td>\n",
       "      <td>0.152860</td>\n",
       "      <td>0.007619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.150751</td>\n",
       "      <td>0.007658</td>\n",
       "      <td>0.155113</td>\n",
       "      <td>0.009786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.008566</td>\n",
       "      <td>0.153188</td>\n",
       "      <td>0.010349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.150117</td>\n",
       "      <td>0.005968</td>\n",
       "      <td>0.153106</td>\n",
       "      <td>0.007861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.149380</td>\n",
       "      <td>0.005130</td>\n",
       "      <td>0.152451</td>\n",
       "      <td>0.007020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.148428</td>\n",
       "      <td>0.004871</td>\n",
       "      <td>0.151099</td>\n",
       "      <td>0.005631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.148939</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>0.151304</td>\n",
       "      <td>0.006314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.147506</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>0.150158</td>\n",
       "      <td>0.006367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.147691</td>\n",
       "      <td>0.002135</td>\n",
       "      <td>0.149462</td>\n",
       "      <td>0.005369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.147578</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>0.149994</td>\n",
       "      <td>0.004851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.147414</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.148888</td>\n",
       "      <td>0.004417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.147629</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.149953</td>\n",
       "      <td>0.004767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.147117</td>\n",
       "      <td>0.001851</td>\n",
       "      <td>0.149544</td>\n",
       "      <td>0.004658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.146196</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.148356</td>\n",
       "      <td>0.004013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.146012</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.148766</td>\n",
       "      <td>0.003114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.145541</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.147988</td>\n",
       "      <td>0.003358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.145971</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.147947</td>\n",
       "      <td>0.003694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.145438</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.147988</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.145786</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.147824</td>\n",
       "      <td>0.003236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.145572</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.147864</td>\n",
       "      <td>0.003169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.145561</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.148356</td>\n",
       "      <td>0.002918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.145377</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.147865</td>\n",
       "      <td>0.002910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.145643</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.148151</td>\n",
       "      <td>0.002694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.145827</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.148151</td>\n",
       "      <td>0.002807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.145715</td>\n",
       "      <td>0.001944</td>\n",
       "      <td>0.147742</td>\n",
       "      <td>0.003068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.145960</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.147783</td>\n",
       "      <td>0.002833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.145367</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.147742</td>\n",
       "      <td>0.002744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>0.125578</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.131690</td>\n",
       "      <td>0.003486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>0.125527</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.131649</td>\n",
       "      <td>0.003382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>0.125496</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.131567</td>\n",
       "      <td>0.003367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697</th>\n",
       "      <td>0.125466</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.131608</td>\n",
       "      <td>0.003280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>0.125435</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.131649</td>\n",
       "      <td>0.003207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.125435</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.131608</td>\n",
       "      <td>0.003249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.125455</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.131649</td>\n",
       "      <td>0.003248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>0.125415</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.131649</td>\n",
       "      <td>0.003312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.125404</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>0.131649</td>\n",
       "      <td>0.003355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.125292</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.131649</td>\n",
       "      <td>0.003312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>0.125271</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.131608</td>\n",
       "      <td>0.003356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>0.125261</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.131690</td>\n",
       "      <td>0.003278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.131731</td>\n",
       "      <td>0.003237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>0.125210</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.131690</td>\n",
       "      <td>0.003306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>0.125179</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.131731</td>\n",
       "      <td>0.003237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>0.125138</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.131813</td>\n",
       "      <td>0.003260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>0.125138</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.131772</td>\n",
       "      <td>0.003235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>0.125138</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.131690</td>\n",
       "      <td>0.003334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>0.125128</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.131690</td>\n",
       "      <td>0.003312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>0.125107</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.131813</td>\n",
       "      <td>0.003382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>0.125128</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.131690</td>\n",
       "      <td>0.003253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>0.125118</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.131649</td>\n",
       "      <td>0.003305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>0.125056</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.131649</td>\n",
       "      <td>0.003290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0.125005</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.131567</td>\n",
       "      <td>0.003263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>0.125036</td>\n",
       "      <td>0.000644</td>\n",
       "      <td>0.131526</td>\n",
       "      <td>0.003302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>0.125015</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.131485</td>\n",
       "      <td>0.003299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.125015</td>\n",
       "      <td>0.000655</td>\n",
       "      <td>0.131485</td>\n",
       "      <td>0.003240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>0.124995</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.131526</td>\n",
       "      <td>0.003304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>0.124974</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.131485</td>\n",
       "      <td>0.003240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>0.124933</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.131403</td>\n",
       "      <td>0.003270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>724 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.168912         0.005300         0.173458        0.009540\n",
       "1            0.150885         0.008856         0.154826        0.012632\n",
       "2            0.155123         0.006793         0.158798        0.010664\n",
       "3            0.148837         0.005181         0.152860        0.007619\n",
       "4            0.150751         0.007658         0.155113        0.009786\n",
       "5            0.150700         0.008566         0.153188        0.010349\n",
       "6            0.150117         0.005968         0.153106        0.007861\n",
       "7            0.149380         0.005130         0.152451        0.007020\n",
       "8            0.148428         0.004871         0.151099        0.005631\n",
       "9            0.148939         0.004061         0.151304        0.006314\n",
       "10           0.147506         0.003586         0.150158        0.006367\n",
       "11           0.147691         0.002135         0.149462        0.005369\n",
       "12           0.147578         0.002371         0.149994        0.004851\n",
       "13           0.147414         0.001870         0.148888        0.004417\n",
       "14           0.147629         0.001905         0.149953        0.004767\n",
       "15           0.147117         0.001851         0.149544        0.004658\n",
       "16           0.146196         0.000900         0.148356        0.004013\n",
       "17           0.146012         0.001254         0.148766        0.003114\n",
       "18           0.145541         0.001949         0.147988        0.003358\n",
       "19           0.145971         0.001941         0.147947        0.003694\n",
       "20           0.145438         0.002229         0.147988        0.003510\n",
       "21           0.145786         0.002211         0.147824        0.003236\n",
       "22           0.145572         0.002032         0.147864        0.003169\n",
       "23           0.145561         0.001984         0.148356        0.002918\n",
       "24           0.145377         0.001979         0.147865        0.002910\n",
       "25           0.145643         0.001896         0.148151        0.002694\n",
       "26           0.145827         0.001588         0.148151        0.002807\n",
       "27           0.145715         0.001944         0.147742        0.003068\n",
       "28           0.145960         0.001227         0.147783        0.002833\n",
       "29           0.145367         0.001860         0.147742        0.002744\n",
       "..                ...              ...              ...             ...\n",
       "694          0.125578         0.000615         0.131690        0.003486\n",
       "695          0.125527         0.000605         0.131649        0.003382\n",
       "696          0.125496         0.000574         0.131567        0.003367\n",
       "697          0.125466         0.000587         0.131608        0.003280\n",
       "698          0.125435         0.000542         0.131649        0.003207\n",
       "699          0.125435         0.000520         0.131608        0.003249\n",
       "700          0.125455         0.000490         0.131649        0.003248\n",
       "701          0.125415         0.000515         0.131649        0.003312\n",
       "702          0.125404         0.000534         0.131649        0.003355\n",
       "703          0.125292         0.000535         0.131649        0.003312\n",
       "704          0.125271         0.000578         0.131608        0.003356\n",
       "705          0.125261         0.000558         0.131690        0.003278\n",
       "706          0.125200         0.000535         0.131731        0.003237\n",
       "707          0.125210         0.000516         0.131690        0.003306\n",
       "708          0.125179         0.000543         0.131731        0.003237\n",
       "709          0.125138         0.000555         0.131813        0.003260\n",
       "710          0.125138         0.000557         0.131772        0.003235\n",
       "711          0.125138         0.000529         0.131690        0.003334\n",
       "712          0.125128         0.000561         0.131690        0.003312\n",
       "713          0.125107         0.000605         0.131813        0.003382\n",
       "714          0.125128         0.000560         0.131690        0.003253\n",
       "715          0.125118         0.000606         0.131649        0.003305\n",
       "716          0.125056         0.000617         0.131649        0.003290\n",
       "717          0.125005         0.000652         0.131567        0.003263\n",
       "718          0.125036         0.000644         0.131526        0.003302\n",
       "719          0.125015         0.000653         0.131485        0.003299\n",
       "720          0.125015         0.000655         0.131485        0.003240\n",
       "721          0.124995         0.000673         0.131526        0.003304\n",
       "722          0.124974         0.000705         0.131485        0.003240\n",
       "723          0.124933         0.000668         0.131403        0.003270\n",
       "\n",
       "[724 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.1,\n",
    "    subsample=0.95,\n",
    "    colsample_bytree=0.6,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=8,\n",
    "    reg_alpha=1e-6,\n",
    "    scale_pos_weight=1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'n_estimators': 1423}\n",
      "Run 0 best score:  0.8707669628598338\n",
      "Run 1 best param:  {'n_estimators': 1223}\n",
      "Run 1 best score:  0.8701117890340281\n",
      "Run 2 best param:  {'n_estimators': 1223}\n",
      "Run 2 best score:  0.8706441177674952\n",
      "Run 3 best param:  {'n_estimators': 1423}\n",
      "Run 3 best score:  0.8716268785062037\n",
      "Run 4 best param:  {'n_estimators': 1423}\n",
      "Run 4 best score:  0.8702346341263667\n",
      "Run 5 best param:  {'n_estimators': 1423}\n",
      "Run 5 best score:  0.8701117890340281\n",
      "Run 6 best param:  {'n_estimators': 1423}\n",
      "Run 6 best score:  0.870398427582818\n",
      "Run 7 best param:  {'n_estimators': 1423}\n",
      "Run 7 best score:  0.8705622210392695\n",
      "Run 8 best param:  {'n_estimators': 1423}\n",
      "Run 8 best score:  0.8706441177674952\n",
      "Best params:  params               {'n_estimators': 1423}\n",
      "mean_test_score_0                  0.870767\n",
      "mean_test_score_1                  0.870112\n",
      "mean_test_score_2                  0.870562\n",
      "mean_test_score_3                  0.871627\n",
      "mean_test_score_4                  0.870235\n",
      "mean_test_score_5                  0.870112\n",
      "mean_test_score_6                  0.870398\n",
      "mean_test_score_7                  0.870562\n",
      "mean_test_score_8                  0.870644\n",
      "avg                                0.870558\n",
      "Name: 7, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(723, 1500, 100)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=195,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.1,\n",
    "        subsample=0.95,\n",
    "        colsample_bytree=0.6,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'n_estimators': 1480}\n",
      "Run 0 best score:  0.8708488595880595\n",
      "Run 1 best param:  {'n_estimators': 1480}\n",
      "Run 1 best score:  0.8702346341263667\n",
      "Run 2 best param:  {'n_estimators': 1440}\n",
      "Run 2 best score:  0.8706441177674952\n",
      "Run 3 best param:  {'n_estimators': 1400}\n",
      "Run 3 best score:  0.8717497235985422\n",
      "Run 4 best param:  {'n_estimators': 1400}\n",
      "Run 4 best score:  0.8703165308545924\n",
      "Run 5 best param:  {'n_estimators': 1480}\n",
      "Run 5 best score:  0.8705622210392695\n",
      "Run 6 best param:  {'n_estimators': 1480}\n",
      "Run 6 best score:  0.870685066131608\n",
      "Run 7 best param:  {'n_estimators': 1440}\n",
      "Run 7 best score:  0.8707260144957208\n",
      "Run 8 best param:  {'n_estimators': 1400}\n",
      "Run 8 best score:  0.870685066131608\n",
      "Best params:  params               {'n_estimators': 1480}\n",
      "mean_test_score_0                  0.870849\n",
      "mean_test_score_1                  0.870235\n",
      "mean_test_score_2                   0.87048\n",
      "mean_test_score_3                  0.871627\n",
      "mean_test_score_4                  0.870153\n",
      "mean_test_score_5                  0.870562\n",
      "mean_test_score_6                  0.870685\n",
      "mean_test_score_7                  0.870562\n",
      "mean_test_score_8                  0.870685\n",
      "avg                                0.870649\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(1400, 1500, 20)]+[1423]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=195,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.1,\n",
    "        subsample=0.95,\n",
    "        colsample_bytree=0.6,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0: 87.56%\n",
      "Accuracy 1: 87.54%\n",
      "Accuracy 2: 87.57%\n",
      "Accuracy 3: 87.50%\n",
      "Accuracy 4: 87.55%\n",
      "Accuracy 5: 87.58%\n",
      "Accuracy 6: 87.55%\n",
      "Accuracy 7: 87.52%\n",
      "Accuracy 8: 87.47%\n",
      "Average accuracy is: 87.54%\n"
     ]
    }
   ],
   "source": [
    "accuracy_array = []\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=1480,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.1,\n",
    "        subsample=0.95,\n",
    "        colsample_bytree=0.6,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        reg_alpha=1e-6,\n",
    "        scale_pos_weight=1,\n",
    "        seed=i\n",
    "    )\n",
    "    model = xgb.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_array.append(accuracy)\n",
    "    print('Accuracy {}: %.2f%%'.format(i) % (accuracy * 100.0))\n",
    "mean_accuracy_score = sum(accuracy_array) / NUM_TRIALS\n",
    "print('Average accuracy is: %.2f%%' % (mean_accuracy_score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
