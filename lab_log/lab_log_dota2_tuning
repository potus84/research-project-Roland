Find optimal n_estimators
Parameters of the xgboost: 
    {learning_rate =0.1,
    n_estimators=5000,
    max_depth=5,
    min_child_weight=1,
    gamma=0,
    subsample=0.8,
    colsample_bytree=0.8,
    objective= 'binary:logistic',
    n_jobs=-1}
metrics: 'error'
early_stopping_rounds=50
Best round: 267
Tuning max_depth and min_child_weight

Grid = {
 'max_depth':range(1,10,2),
 'min_child_weight':range(1,500,50)
}
Best params:  params               {'max_depth': 7, 'min_child_weight': 51}
mean_test_score_0                                    0.596849
mean_test_score_1                                    0.596285
mean_test_score_2                                    0.594537
mean_test_score_3                                    0.596518
avg                                                  0.596047

Grid = {
 'max_depth':range(1,10,2),
 'min_child_weight':range(25,75,5)
Best params:  params               {'max_depth': 7, 'min_child_weight': 65}
mean_test_score_0                                    0.598053
mean_test_score_1                                    0.597257
mean_test_score_2                                    0.597062
mean_test_score_3                                    0.595722
avg                                                  0.597024


Grid = {
 'max_depth':[6, 7, 8],
 'min_child_weight':range(65, 71)
}
Best params:  params               {'max_depth': 7, 'min_child_weight': 67}
mean_test_score_0                                    0.597373
mean_test_score_1                                    0.598811
mean_test_score_2                                    0.597121
mean_test_score_3                                    0.595819
avg                                                  0.597281


Tuning gamma
Grid = {
 'gamma':[i/10.0 for i in range(0,5)]
}
Best params:  params               {'gamma': 0.0}
mean_test_score_0          0.597373
mean_test_score_1          0.598811
mean_test_score_2          0.597121
mean_test_score_3          0.595819
avg                        0.597281

Recablirating the n_estimators and 1st tune n_estimators
Parameters of the xgboost: 
    {learning_rate =0.1,
    n_estimators=267,
    max_depth=7,
    min_child_weight=67,
    gamma=0,
    subsample=0.8,
    colsample_bytree=0.8,
    objective= 'binary:logistic',
    n_jobs=-1,
    seed=0}
metrics: 'error'
early_stopping_rounds=50
Best round: 271

Grid = {
 'n_estimators':[i for i in range(100, 1500, 100)]+[271]
}
Best params:  params               {'n_estimators': 400}
mean_test_score_0                 0.598131
mean_test_score_1                 0.597257
mean_test_score_2                 0.596907
mean_test_score_3                 0.596829
avg                               0.597281

Grid = {
 'n_estimators':[i for i in range(350, 451, 10)]
}
Best params:  params               {'n_estimators': 410}
mean_test_score_0                 0.599083
mean_test_score_1                 0.597509
mean_test_score_2                 0.596985
mean_test_score_3                 0.596616
avg                               0.597548

Tuning the subsample and colsample_bytree
Grid = {
 'subsample':[i/10.0 for i in range(6,10)],
 'colsample_bytree':[i/10.0 for i in range(6,10)]
}
Best params:  params               {'colsample_bytree': 0.7, 'subsample': 0.8}
mean_test_score_0                                       0.598714
mean_test_score_1                                       0.598753
mean_test_score_2                                       0.596363
mean_test_score_3                                       0.596732
avg                                                      0.59764

param_test5 = {
 'subsample':[i/100.0 for i in range(75,86,5)],
 'colsample_bytree':[i/100.0 for i in range(65,76,5)]
}
Best params:  params               {'colsample_bytree': 0.7, 'subsample': 0.8}
mean_test_score_0                                       0.598714
mean_test_score_1                                       0.598753
mean_test_score_2                                       0.596363
mean_test_score_3                                       0.596732
avg                                                      0.59764
Tuning Regularization Parameters
Grid = {
 'reg_alpha':[0, 1e-5, 1e-2, 0.1, 1, 100]
}
Best params:  params               {'reg_alpha': 0}
mean_test_score_0            0.598714
mean_test_score_1            0.598753
mean_test_score_2            0.596363
mean_test_score_3            0.596732
avg                           0.59764

Grid = {
 'reg_alpha':[0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4]
}
Best params:  params               {'reg_alpha': 0}
mean_test_score_0            0.598714
mean_test_score_1            0.598753
mean_test_score_2            0.596363
mean_test_score_3            0.596732
avg                           0.59764


Reduce the learning rate and tune the n_estimators
{
    earning_rate =0.01,
    n_estimators=400,
    max_depth=9,
    min_child_weight=45,
    gamma=0.4,
    reg_alpha=3,
    subsample=0.9,
    colsample_bytree=0.65,
    objective= 'binary:logistic',
    n_jobs=-1,
    seed=0
}
n_estimators: 1139

Grid = {
 'n_estimators':[i for i in range(1000, 1900, 100)]+[1139]
}
Best params:  params               {'n_estimators': 1900}
mean_test_score_0                  0.597198
mean_test_score_1                    0.5958
mean_test_score_2                    0.5958
mean_test_score_3                  0.596363
avg                                 0.59629

--Stop the pipeline here bacause of the decrease in the accuracy.



Test on test set
Accuracy 0: 59.55%
Accuracy 1: 59.47%
Accuracy 2: 59.37%
Accuracy 3: 59.40%
Average accuracy is: 59.45%

