{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adult dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xgboost\n",
    "import copy\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/tai/Projects/research-project-Roland/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/adult/adult.0.train.csv\", encoding='latin1', \n",
    "                 names=['age','workclass','fnlwgt','education',\n",
    "                         'education_num','marital_status','occupation',\n",
    "                         'relationship','race','sex','capital_gain','capital_loss',\n",
    "                         'hours_per_week','native_country','income'],\n",
    "                 na_values='?',\n",
    "                 low_memory=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/adult/adult.0.test.csv\", encoding='latin1', \n",
    "                 names=['age','workclass','fnlwgt','education',\n",
    "                         'education_num','marital_status','occupation',\n",
    "                         'relationship','race','sex','capital_gain','capital_loss',\n",
    "                         'hours_per_week','native_country','income'],\n",
    "                 na_values='?',\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covert the output as binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['over_50k'] = np.where(train.income == '>50K', 1, 0)\n",
    "train=train.drop(['income'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['over_50k'] = np.where(test.income == '>50K', 1, 0)\n",
    "test=test.drop(['income'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the numeric number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,'age'] = pd.to_numeric(train['age'], downcast='integer', errors='coerce')\n",
    "train.loc[:,'fnlwgt'] = pd.to_numeric(train['fnlwgt'], downcast='float', errors='coerce')\n",
    "train.loc[:,'age'] = pd.to_numeric(train['age'], downcast='integer', errors='coerce')\n",
    "train.loc[:,'capital_gain'] = pd.to_numeric(train['capital_gain'], downcast='float', errors='coerce')\n",
    "train.loc[:,'capital_loss'] = pd.to_numeric(train['capital_loss'], downcast='float', errors='coerce')\n",
    "train.loc[:,'hours_per_week'] = pd.to_numeric(train['hours_per_week'], downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:,'age'] = pd.to_numeric(test['age'], downcast='integer', errors='coerce')\n",
    "test.loc[:,'fnlwgt'] = pd.to_numeric(test['fnlwgt'], downcast='float', errors='coerce')\n",
    "test.loc[:,'age'] = pd.to_numeric(test['age'], downcast='integer', errors='coerce')\n",
    "test.loc[:,'capital_gain'] = pd.to_numeric(test['capital_gain'], downcast='float', errors='coerce')\n",
    "test.loc[:,'capital_loss'] = pd.to_numeric(test['capital_loss'], downcast='float', errors='coerce')\n",
    "test.loc[:,'hours_per_week'] = pd.to_numeric(test['hours_per_week'], downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         1379\n",
       "fnlwgt               0\n",
       "education            0\n",
       "education_num        0\n",
       "marital_status       0\n",
       "occupation        1384\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital_gain         0\n",
       "capital_loss         0\n",
       "hours_per_week       0\n",
       "native_country     431\n",
       "over_50k             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "workclass         1420\n",
       "fnlwgt               0\n",
       "education            0\n",
       "education_num        0\n",
       "marital_status       0\n",
       "occupation        1425\n",
       "relationship         0\n",
       "race                 0\n",
       "sex                  0\n",
       "capital_gain         0\n",
       "capital_loss         0\n",
       "hours_per_week       0\n",
       "native_country     426\n",
       "over_50k             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3291: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "train['education'] = train['education'].astype('category',\n",
    "                                               categories=['Bachelors', 'Some-college', '11th', 'HS-grad', \n",
    "                                                           'Prof-school', \n",
    "                                                           'Assoc-acdm', 'Assoc-voc', '9th', '7th-8th',\n",
    "                                                           '12th', 'Masters', '1st-4th', '10th', \n",
    "                                                           'Doctorate', '5th-6th', 'Preschool'])\n",
    "train['marital_status'] = train['marital_status'].astype('category',\n",
    "                                                         categories=['Married-civ-spouse', 'Divorced', \n",
    "                                                                     'Never-married', 'Separated', \n",
    "                                                                     'Widowed', 'Married-spouse-absent', \n",
    "                                                                     'Married-AF-spouse'])\n",
    "train['relationship'] = train['relationship'].astype('category',\n",
    "                                                     categories=['Wife', 'Own-child', 'Husband', \n",
    "                                                                 'Not-in-family', 'Other-relative', 'Unmarried'])\n",
    "train['race'] = train['race'].astype('category',\n",
    "                                     categories=['White', 'Asian-Pac-Islander', \n",
    "                                                 'Amer-Indian-Eskimo', 'Other', 'Black'])\n",
    "train['sex'] = train['sex'].astype('category', \n",
    "                                   categories=['Female', 'Male'])\n",
    "\n",
    "\n",
    "train['workclass'] = train['workclass'].astype('category',\n",
    "                                               categories=['Private', 'Self-emp-not-inc', \n",
    "                                                           'Self-emp-inc', 'Federal-gov', \n",
    "                                                           'Local-gov', 'State-gov', \n",
    "                                                           'Without-pay', 'Never-worked'])\n",
    "train['occupation'] = train['occupation'].astype('category',\n",
    "                                                 categories=['Tech-support', 'Craft-repair', \n",
    "                                                             'Other-service', 'Sales', 'Exec-managerial',\n",
    "                                                             'Prof-specialty', 'Handlers-cleaners', \n",
    "                                                             'Machine-op-inspct', 'Adm-clerical',\n",
    "                                                             'Farming-fishing', 'Transport-moving', \n",
    "                                                             'Priv-house-serv',\n",
    "                                                             'Protective-serv', 'Armed-Forces'])\n",
    "train['native_country'] = train['native_country'].astype('category',\n",
    "                                                         categories=['United-States',\n",
    "                                                                                 'Cambodia',\n",
    "                                                                                 'England',\n",
    "                                                                                 'Puerto-Rico',\n",
    "                                                                                 'Canada',\n",
    "                                                                                 'Germany',\n",
    "                                                                                 'Outlying-US(Guam-USVI-etc)',\n",
    "                                                                                 'India',\n",
    "                                                                                 'Japan',\n",
    "                                                                                 'Greece',\n",
    "                                                                                 'South',\n",
    "                                                                                 'China',\n",
    "                                                                                 'Cuba',\n",
    "                                                                                 'Iran',\n",
    "                                                                                 'Honduras',\n",
    "                                                                                 'Philippines',\n",
    "                                                                                 'Italy',\n",
    "                                                                                 'Poland',\n",
    "                                                                                 'Jamaica',\n",
    "                                                                                 'Vietnam',\n",
    "                                                                                 'Mexico',\n",
    "                                                                                 'Portugal',\n",
    "                                                                                 'Ireland',\n",
    "                                                                                 'France',\n",
    "                                                                                 'Dominican-Republic',\n",
    "                                                                                 'Laos',\n",
    "                                                                                 'Ecuador',\n",
    "                                                                                 'Taiwan',\n",
    "                                                                                 'Haiti',\n",
    "                                                                                 'Columbia',\n",
    "                                                                                 'Hungary',\n",
    "                                                                                 'Guatemala',\n",
    "                                                                                 'Nicaragua',\n",
    "                                                                                 'Scotland',\n",
    "                                                                                 'Thailand',\n",
    "                                                                                 'Yugoslavia',\n",
    "                                                                                 'El-Salvador',\n",
    "                                                                                 'Trinadad&Tobago',\n",
    "                                                                                 'Peru',\n",
    "                                                                                 'Hong',\n",
    "                                                                                 'Holand-Netherlands'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=['education','marital_status','relationship','race','sex'])\n",
    "train = pd.get_dummies(train, columns=['workclass','occupation','native_country'], dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['education'] = test['education'].astype('category',\n",
    "                                               categories=['Bachelors', 'Some-college', '11th', 'HS-grad', \n",
    "                                                           'Prof-school', \n",
    "                                                           'Assoc-acdm', 'Assoc-voc', '9th', '7th-8th',\n",
    "                                                           '12th', 'Masters', '1st-4th', '10th', \n",
    "                                                           'Doctorate', '5th-6th', 'Preschool'])\n",
    "test['marital_status'] = test['marital_status'].astype('category',\n",
    "                                                         categories=['Married-civ-spouse', 'Divorced', \n",
    "                                                                     'Never-married', 'Separated', \n",
    "                                                                     'Widowed', 'Married-spouse-absent', \n",
    "                                                                     'Married-AF-spouse'])\n",
    "test['relationship'] = test['relationship'].astype('category',\n",
    "                                                     categories=['Wife', 'Own-child', 'Husband', \n",
    "                                                                 'Not-in-family', 'Other-relative', 'Unmarried'])\n",
    "test['race'] = test['race'].astype('category',\n",
    "                                    categories=['White', 'Asian-Pac-Islander', \n",
    "                                                 'Amer-Indian-Eskimo', 'Other', 'Black'])\n",
    "test['sex'] = test['sex'].astype('category',\n",
    "                                   categories=['Female', 'Male'])\n",
    "\n",
    "\n",
    "test['workclass'] = test['workclass'].astype('category',\n",
    "                                               categories=['Private', 'Self-emp-not-inc', \n",
    "                                                           'Self-emp-inc', 'Federal-gov', \n",
    "                                                           'Local-gov', 'State-gov', \n",
    "                                                           'Without-pay', 'Never-worked'])\n",
    "test['occupation'] = test['occupation'].astype('category',\n",
    "                                                categories=['Tech-support', 'Craft-repair', \n",
    "                                                             'Other-service', 'Sales', 'Exec-managerial',\n",
    "                                                             'Prof-specialty', 'Handlers-cleaners', \n",
    "                                                             'Machine-op-inspct', 'Adm-clerical',\n",
    "                                                             'Farming-fishing', 'Transport-moving', \n",
    "                                                             'Priv-house-serv',\n",
    "                                                             'Protective-serv', 'Armed-Forces'])\n",
    "test['native_country'] = test['native_country'].astype('category',                                                         \n",
    "                                                         categories=['United-States',\n",
    "                                                                                 'Cambodia',\n",
    "                                                                                 'England',\n",
    "                                                                                 'Puerto-Rico',\n",
    "                                                                                 'Canada',\n",
    "                                                                                 'Germany',\n",
    "                                                                                 'Outlying-US(Guam-USVI-etc)',\n",
    "                                                                                 'India',\n",
    "                                                                                 'Japan',\n",
    "                                                                                 'Greece',\n",
    "                                                                                 'South',\n",
    "                                                                                 'China',\n",
    "                                                                                 'Cuba',\n",
    "                                                                                 'Iran',\n",
    "                                                                                 'Honduras',\n",
    "                                                                                 'Philippines',\n",
    "                                                                                 'Italy',\n",
    "                                                                                 'Poland',\n",
    "                                                                                 'Jamaica',\n",
    "                                                                                 'Vietnam',\n",
    "                                                                                 'Mexico',\n",
    "                                                                                 'Portugal',\n",
    "                                                                                 'Ireland',\n",
    "                                                                                 'France',\n",
    "                                                                                 'Dominican-Republic',\n",
    "                                                                                 'Laos',\n",
    "                                                                                 'Ecuador',\n",
    "                                                                                 'Taiwan',\n",
    "                                                                                 'Haiti',\n",
    "                                                                                 'Columbia',\n",
    "                                                                                 'Hungary',\n",
    "                                                                                 'Guatemala',\n",
    "                                                                                 'Nicaragua',\n",
    "                                                                                 'Scotland',\n",
    "                                                                                 'Thailand',\n",
    "                                                                                 'Yugoslavia',\n",
    "                                                                                 'El-Salvador',\n",
    "                                                                                 'Trinadad&Tobago',\n",
    "                                                                                 'Peru',\n",
    "                                                                                 'Hong',\n",
    "                                                                                 'Holand-Netherlands'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.get_dummies(test, columns=['education','marital_status','relationship','race','sex'])\n",
    "test = pd.get_dummies(test, columns=['workclass','occupation','native_country'], dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['over_50k'], axis=1)\n",
    "y_train = train.over_50k\n",
    "\n",
    "X_test = test.drop(['over_50k'], axis=1)\n",
    "y_test = test.over_50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = X_train.align(X_test, join='outer', fill_value=0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24421, 108)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24421, 108)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss',\n",
       "       'hours_per_week', 'education_Bachelors', 'education_Some-college',\n",
       "       'education_11th', 'education_HS-grad',\n",
       "       ...\n",
       "       'native_country_Nicaragua', 'native_country_Scotland',\n",
       "       'native_country_Thailand', 'native_country_Yugoslavia',\n",
       "       'native_country_El-Salvador', 'native_country_Trinadad&Tobago',\n",
       "       'native_country_Peru', 'native_country_Hong',\n",
       "       'native_country_Holand-Netherlands', 'native_country_nan'],\n",
       "      dtype='object', length=108)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss',\n",
       "       'hours_per_week', 'education_Bachelors', 'education_Some-college',\n",
       "       'education_11th', 'education_HS-grad',\n",
       "       ...\n",
       "       'native_country_Nicaragua', 'native_country_Scotland',\n",
       "       'native_country_Thailand', 'native_country_Yugoslavia',\n",
       "       'native_country_El-Salvador', 'native_country_Trinadad&Tobago',\n",
       "       'native_country_Peru', 'native_country_Hong',\n",
       "       'native_country_Holand-Netherlands', 'native_country_nan'],\n",
       "      dtype='object', length=108)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tuning on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.167069</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.172639</td>\n",
       "      <td>0.012472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144885</td>\n",
       "      <td>0.002585</td>\n",
       "      <td>0.148725</td>\n",
       "      <td>0.004196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147342</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.151468</td>\n",
       "      <td>0.005173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.145203</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.148315</td>\n",
       "      <td>0.005444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.144445</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.147742</td>\n",
       "      <td>0.004791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.144251</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.147455</td>\n",
       "      <td>0.005365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.143708</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.147046</td>\n",
       "      <td>0.005066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.143545</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.004553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.143033</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.146268</td>\n",
       "      <td>0.004691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.143381</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.003970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.142214</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.144958</td>\n",
       "      <td>0.004884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.142132</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.145408</td>\n",
       "      <td>0.004074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.141784</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.145285</td>\n",
       "      <td>0.004178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.141487</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.143975</td>\n",
       "      <td>0.004840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.141098</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.143442</td>\n",
       "      <td>0.005158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.140791</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.143197</td>\n",
       "      <td>0.004545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.140484</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.143278</td>\n",
       "      <td>0.004398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.140371</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.143033</td>\n",
       "      <td>0.004303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.140207</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.142542</td>\n",
       "      <td>0.004103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.140033</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.142419</td>\n",
       "      <td>0.004352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.140013</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.142337</td>\n",
       "      <td>0.003690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.139777</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.141927</td>\n",
       "      <td>0.004264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.139440</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.141968</td>\n",
       "      <td>0.004179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.138887</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.141886</td>\n",
       "      <td>0.004283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.138805</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.141804</td>\n",
       "      <td>0.004335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.138262</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.141354</td>\n",
       "      <td>0.004248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.138037</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.141231</td>\n",
       "      <td>0.003817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.137965</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.140986</td>\n",
       "      <td>0.003712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.137628</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.140781</td>\n",
       "      <td>0.003557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.137269</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.140535</td>\n",
       "      <td>0.003664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.115935</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.115822</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.129520</td>\n",
       "      <td>0.001518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.115577</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.129479</td>\n",
       "      <td>0.001450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.115434</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.129602</td>\n",
       "      <td>0.001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.115484</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.129479</td>\n",
       "      <td>0.001211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.115198</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.129724</td>\n",
       "      <td>0.001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.115003</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.114891</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.114819</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.129397</td>\n",
       "      <td>0.001303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.114666</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>0.001362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.114594</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.129315</td>\n",
       "      <td>0.001394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.114543</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.114430</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>0.001446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.114154</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.129274</td>\n",
       "      <td>0.001338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.113929</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.129643</td>\n",
       "      <td>0.001563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.113765</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.113580</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.129602</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.113468</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.129520</td>\n",
       "      <td>0.001404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.113345</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.129479</td>\n",
       "      <td>0.001452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.113222</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.113099</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.129315</td>\n",
       "      <td>0.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.113089</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.129192</td>\n",
       "      <td>0.001432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.112936</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.129192</td>\n",
       "      <td>0.001578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.112915</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.129233</td>\n",
       "      <td>0.001446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.112772</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.129274</td>\n",
       "      <td>0.001452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.112649</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.129397</td>\n",
       "      <td>0.001377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.112731</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>0.001361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.112690</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.129397</td>\n",
       "      <td>0.001489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.112516</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.129438</td>\n",
       "      <td>0.001548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.112321</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.129110</td>\n",
       "      <td>0.001411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.167069         0.008100         0.172639        0.012472\n",
       "1            0.144885         0.002585         0.148725        0.004196\n",
       "2            0.147342         0.002714         0.151468        0.005173\n",
       "3            0.145203         0.000719         0.148315        0.005444\n",
       "4            0.144445         0.001555         0.147742        0.004791\n",
       "5            0.144251         0.001563         0.147455        0.005365\n",
       "6            0.143708         0.001650         0.147046        0.005066\n",
       "7            0.143545         0.001366         0.146800        0.004553\n",
       "8            0.143033         0.001004         0.146268        0.004691\n",
       "9            0.143381         0.001173         0.146186        0.003970\n",
       "10           0.142214         0.001088         0.144958        0.004884\n",
       "11           0.142132         0.001469         0.145408        0.004074\n",
       "12           0.141784         0.001858         0.145285        0.004178\n",
       "13           0.141487         0.001324         0.143975        0.004840\n",
       "14           0.141098         0.001480         0.143442        0.005158\n",
       "15           0.140791         0.000961         0.143197        0.004545\n",
       "16           0.140484         0.000897         0.143278        0.004398\n",
       "17           0.140371         0.000767         0.143033        0.004303\n",
       "18           0.140207         0.000854         0.142542        0.004103\n",
       "19           0.140033         0.000987         0.142419        0.004352\n",
       "20           0.140013         0.001066         0.142337        0.003690\n",
       "21           0.139777         0.001213         0.141927        0.004264\n",
       "22           0.139440         0.001378         0.141968        0.004179\n",
       "23           0.138887         0.001354         0.141886        0.004283\n",
       "24           0.138805         0.001348         0.141804        0.004335\n",
       "25           0.138262         0.001232         0.141354        0.004248\n",
       "26           0.138037         0.001152         0.141231        0.003817\n",
       "27           0.137965         0.001109         0.140986        0.003712\n",
       "28           0.137628         0.001342         0.140781        0.003557\n",
       "29           0.137269         0.001207         0.140535        0.003664\n",
       "..                ...              ...              ...             ...\n",
       "130          0.115935         0.000840         0.129438        0.001421\n",
       "131          0.115822         0.000807         0.129520        0.001518\n",
       "132          0.115577         0.000781         0.129479        0.001450\n",
       "133          0.115434         0.000759         0.129602        0.001214\n",
       "134          0.115484         0.000826         0.129479        0.001211\n",
       "135          0.115198         0.000847         0.129724        0.001025\n",
       "136          0.115003         0.000749         0.129438        0.001217\n",
       "137          0.114891         0.000757         0.129438        0.001374\n",
       "138          0.114819         0.000721         0.129397        0.001303\n",
       "139          0.114666         0.000690         0.129356        0.001362\n",
       "140          0.114594         0.000758         0.129315        0.001394\n",
       "141          0.114543         0.000809         0.129438        0.001343\n",
       "142          0.114430         0.000747         0.129356        0.001446\n",
       "143          0.114154         0.000844         0.129274        0.001338\n",
       "144          0.113929         0.000677         0.129643        0.001563\n",
       "145          0.113765         0.000744         0.129438        0.001600\n",
       "146          0.113580         0.000533         0.129602        0.001609\n",
       "147          0.113468         0.000482         0.129520        0.001404\n",
       "148          0.113345         0.000533         0.129479        0.001452\n",
       "149          0.113222         0.000533         0.129356        0.001462\n",
       "150          0.113099         0.000515         0.129315        0.001465\n",
       "151          0.113089         0.000596         0.129192        0.001432\n",
       "152          0.112936         0.000629         0.129192        0.001578\n",
       "153          0.112915         0.000539         0.129233        0.001446\n",
       "154          0.112772         0.000663         0.129274        0.001452\n",
       "155          0.112649         0.000653         0.129397        0.001377\n",
       "156          0.112731         0.000799         0.129151        0.001361\n",
       "157          0.112690         0.000705         0.129397        0.001489\n",
       "158          0.112516         0.000569         0.129438        0.001548\n",
       "159          0.112321         0.000551         0.129110        0.001411\n",
       "\n",
       "[160 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 0 best score:  0.8694566152082225\n",
      "Run 1 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 1 best score:  0.8706441177674952\n",
      "Run 2 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 2 best score:  0.8706031694033823\n",
      "Run 3 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 3 best score:  0.8713811883215266\n",
      "Run 4 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 4 best score:  0.8698251504852381\n",
      "Run 5 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 5 best score:  0.8696204086646738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 6 best score:  0.8694975635723353\n",
      "Run 7 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 7 best score:  0.8697842021211253\n",
      "Run 8 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 8 best score:  0.8706031694033823\n",
      "Best params:  params               {'max_depth': 5, 'min_child_weight': 1}\n",
      "mean_test_score_0                                   0.869457\n",
      "mean_test_score_1                                   0.870644\n",
      "mean_test_score_2                                   0.870603\n",
      "mean_test_score_3                                   0.871381\n",
      "mean_test_score_4                                   0.869825\n",
      "mean_test_score_5                                    0.86962\n",
      "mean_test_score_6                                   0.869498\n",
      "mean_test_score_7                                   0.869784\n",
      "mean_test_score_8                                   0.870603\n",
      "avg                                                 0.870157\n",
      "Name: 12, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1 = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1,300,50)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=159,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1.fit(X_train,y_train)    \n",
    "    if grid_score1.empty:\n",
    "        grid_score1 = pd.DataFrame(gsearch1.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1.best_score_)\n",
    "\n",
    "grid_score1['avg'] = grid_score1.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1.loc[grid_score1.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 0 best score:  0.8699889439416896\n",
      "Run 1 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 1 best score:  0.870685066131608\n",
      "Run 2 best param:  {'max_depth': 5, 'min_child_weight': 5}\n",
      "Run 2 best score:  0.8714221366856394\n",
      "Run 3 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 3 best score:  0.8713811883215266\n",
      "Run 4 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 4 best score:  0.8704393759469309\n",
      "Run 5 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 5 best score:  0.8696204086646738\n",
      "Run 6 best param:  {'max_depth': 5, 'min_child_weight': 5}\n",
      "Run 6 best score:  0.869866098849351\n",
      "Run 7 best param:  {'max_depth': 5, 'min_child_weight': 5}\n",
      "Run 7 best score:  0.8698251504852381\n",
      "Run 8 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 8 best score:  0.8706031694033823\n",
      "Best params:  params               {'max_depth': 5, 'min_child_weight': 3}\n",
      "mean_test_score_0                                   0.869989\n",
      "mean_test_score_1                                   0.870685\n",
      "mean_test_score_2                                   0.871299\n",
      "mean_test_score_3                                   0.871054\n",
      "mean_test_score_4                                   0.870439\n",
      "mean_test_score_5                                   0.869375\n",
      "mean_test_score_6                                   0.869702\n",
      "mean_test_score_7                                    0.86962\n",
      "mean_test_score_8                                   0.869661\n",
      "avg                                                 0.870203\n",
      "Name: 11, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1b = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1, 10, 2)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1b = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=159,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1b = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1b,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1b.fit(X_train,y_train)    \n",
    "    if grid_score1b.empty:\n",
    "        grid_score1b = pd.DataFrame(gsearch1b.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1b.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1b['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1b.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1b.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1b.best_score_)\n",
    "\n",
    "grid_score1b['avg'] = grid_score1b.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1b.loc[grid_score1b.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 0 best score:  0.8699889439416896\n",
      "Run 1 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 1 best score:  0.870685066131608\n",
      "Run 2 best param:  {'max_depth': 5, 'min_child_weight': 4}\n",
      "Run 2 best score:  0.8717087752344294\n",
      "Run 3 best param:  {'max_depth': 6, 'min_child_weight': 3}\n",
      "Run 3 best score:  0.8713811883215266\n",
      "Run 4 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 4 best score:  0.8704393759469309\n",
      "Run 5 best param:  {'max_depth': 6, 'min_child_weight': 3}\n",
      "Run 5 best score:  0.8704393759469309\n",
      "Run 6 best param:  {'max_depth': 6, 'min_child_weight': 3}\n",
      "Run 6 best score:  0.8706031694033823\n",
      "Run 7 best param:  {'max_depth': 6, 'min_child_weight': 2}\n",
      "Run 7 best score:  0.8708079112239466\n",
      "Run 8 best param:  {'max_depth': 5, 'min_child_weight': 4}\n",
      "Run 8 best score:  0.8702346341263667\n",
      "Best params:  params               {'max_depth': 5, 'min_child_weight': 3}\n",
      "mean_test_score_0                                   0.869989\n",
      "mean_test_score_1                                   0.870685\n",
      "mean_test_score_2                                   0.871299\n",
      "mean_test_score_3                                   0.871054\n",
      "mean_test_score_4                                   0.870439\n",
      "mean_test_score_5                                   0.869375\n",
      "mean_test_score_6                                   0.869702\n",
      "mean_test_score_7                                    0.86962\n",
      "mean_test_score_8                                   0.869661\n",
      "avg                                                 0.870203\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look carefully again the neigbor values\n",
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test2 = {\n",
    " 'max_depth':[4, 5, 6],\n",
    " 'min_child_weight':[2, 3 ,4]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score2 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=159,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,        \n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test2,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch2.fit(X_train,y_train)    \n",
    "    if grid_score2.empty:\n",
    "        grid_score2 = pd.DataFrame(gsearch2.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score2.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score2['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch2.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch2.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch2.best_score_)\n",
    "\n",
    "grid_score2['avg'] = grid_score2.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score2.loc[grid_score2.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'gamma': 0.0}\n",
      "Run 0 best score:  0.8699889439416896\n",
      "Run 1 best param:  {'gamma': 0.3}\n",
      "Run 1 best score:  0.8709307563162851\n",
      "Run 2 best param:  {'gamma': 0.1}\n",
      "Run 2 best score:  0.8722001556037836\n",
      "Run 3 best param:  {'gamma': 0.0}\n",
      "Run 3 best score:  0.8710536014086238\n",
      "Run 4 best param:  {'gamma': 0.2}\n",
      "Run 4 best score:  0.8705622210392695\n",
      "Run 5 best param:  {'gamma': 0.4}\n",
      "Run 5 best score:  0.8701936857622538\n",
      "Run 6 best param:  {'gamma': 0.4}\n",
      "Run 6 best score:  0.8708898079521723\n",
      "Run 7 best param:  {'gamma': 0.1}\n",
      "Run 7 best score:  0.8697023053928995\n",
      "Run 8 best param:  {'gamma': 0.1}\n",
      "Run 8 best score:  0.8709307563162851\n",
      "Best params:  params               {'gamma': 0.0}\n",
      "mean_test_score_0          0.869989\n",
      "mean_test_score_1          0.870685\n",
      "mean_test_score_2          0.871299\n",
      "mean_test_score_3          0.871054\n",
      "mean_test_score_4          0.870439\n",
      "mean_test_score_5          0.869375\n",
      "mean_test_score_6          0.869702\n",
      "mean_test_score_7           0.86962\n",
      "mean_test_score_8          0.869661\n",
      "avg                        0.870203\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score3 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=159,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch3 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test3,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch3.fit(X_train,y_train)    \n",
    "    if grid_score3.empty:\n",
    "        grid_score3 = pd.DataFrame(gsearch3.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score3.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score3['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch3.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch3.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch3.best_score_)\n",
    "\n",
    "grid_score3['avg'] = grid_score3.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score3.loc[grid_score3.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recablirating the n_estimators and 1st tune the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.167499</td>\n",
       "      <td>0.008291</td>\n",
       "      <td>0.172721</td>\n",
       "      <td>0.012682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.145449</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.148929</td>\n",
       "      <td>0.004688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147639</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.151509</td>\n",
       "      <td>0.005178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.145571</td>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.148438</td>\n",
       "      <td>0.005468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.144855</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.148069</td>\n",
       "      <td>0.004862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.144793</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.147619</td>\n",
       "      <td>0.005214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.144046</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.147292</td>\n",
       "      <td>0.004879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.143708</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.004268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.143278</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.146063</td>\n",
       "      <td>0.004509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.143585</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.146186</td>\n",
       "      <td>0.004032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.142582</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.145367</td>\n",
       "      <td>0.004617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.142377</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.145326</td>\n",
       "      <td>0.004282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.141784</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>0.144753</td>\n",
       "      <td>0.004046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.141937</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.144016</td>\n",
       "      <td>0.004876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.141538</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.143688</td>\n",
       "      <td>0.004837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.141180</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.143197</td>\n",
       "      <td>0.004384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.140801</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.143033</td>\n",
       "      <td>0.004173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.140873</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.142951</td>\n",
       "      <td>0.004216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.140658</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.142542</td>\n",
       "      <td>0.004072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.140392</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.142501</td>\n",
       "      <td>0.004276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.140299</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.142583</td>\n",
       "      <td>0.004138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.140033</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.142296</td>\n",
       "      <td>0.004183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.139593</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.142337</td>\n",
       "      <td>0.004361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.139327</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.142091</td>\n",
       "      <td>0.004362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.139163</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.142296</td>\n",
       "      <td>0.004461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.138672</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.141968</td>\n",
       "      <td>0.004211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.138405</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.141845</td>\n",
       "      <td>0.004204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.138405</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.141600</td>\n",
       "      <td>0.003864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.137894</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.141559</td>\n",
       "      <td>0.003927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.137740</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.141436</td>\n",
       "      <td>0.003924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>0.109803</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.129602</td>\n",
       "      <td>0.000724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.109742</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.109639</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.129356</td>\n",
       "      <td>0.000705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>0.109352</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.129520</td>\n",
       "      <td>0.000619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>0.109373</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.129643</td>\n",
       "      <td>0.000603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0.109260</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.129684</td>\n",
       "      <td>0.000889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.109045</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.129643</td>\n",
       "      <td>0.000970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>0.108913</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.129806</td>\n",
       "      <td>0.001305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.108974</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.129806</td>\n",
       "      <td>0.001095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>0.108800</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.129725</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>0.108789</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.129929</td>\n",
       "      <td>0.001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.108667</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.129847</td>\n",
       "      <td>0.000811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.108544</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.130134</td>\n",
       "      <td>0.000549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.108534</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.130298</td>\n",
       "      <td>0.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.108380</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.130052</td>\n",
       "      <td>0.000798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.108308</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.130011</td>\n",
       "      <td>0.000921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.108237</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.129888</td>\n",
       "      <td>0.000988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.108165</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.129847</td>\n",
       "      <td>0.000968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.108278</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.129643</td>\n",
       "      <td>0.000558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.108083</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>0.129233</td>\n",
       "      <td>0.000756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.108217</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.129192</td>\n",
       "      <td>0.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.108104</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.129192</td>\n",
       "      <td>0.000897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.107899</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>0.000758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.107643</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>0.000734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.107633</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.129315</td>\n",
       "      <td>0.000824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.107663</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>0.129315</td>\n",
       "      <td>0.000663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.107592</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.129274</td>\n",
       "      <td>0.000596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.107397</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.129192</td>\n",
       "      <td>0.000574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.107305</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.129233</td>\n",
       "      <td>0.000634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.107315</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>0.000616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.167499         0.008291         0.172721        0.012682\n",
       "1            0.145449         0.002578         0.148929        0.004688\n",
       "2            0.147639         0.002788         0.151509        0.005178\n",
       "3            0.145571         0.000577         0.148438        0.005468\n",
       "4            0.144855         0.001569         0.148069        0.004862\n",
       "5            0.144793         0.001276         0.147619        0.005214\n",
       "6            0.144046         0.001369         0.147292        0.004879\n",
       "7            0.143708         0.001086         0.146800        0.004268\n",
       "8            0.143278         0.000960         0.146063        0.004509\n",
       "9            0.143585         0.001202         0.146186        0.004032\n",
       "10           0.142582         0.001037         0.145367        0.004617\n",
       "11           0.142377         0.001051         0.145326        0.004282\n",
       "12           0.141784         0.001511         0.144753        0.004046\n",
       "13           0.141937         0.001302         0.144016        0.004876\n",
       "14           0.141538         0.001334         0.143688        0.004837\n",
       "15           0.141180         0.000851         0.143197        0.004384\n",
       "16           0.140801         0.001047         0.143033        0.004173\n",
       "17           0.140873         0.000786         0.142951        0.004216\n",
       "18           0.140658         0.000684         0.142542        0.004072\n",
       "19           0.140392         0.000920         0.142501        0.004276\n",
       "20           0.140299         0.000911         0.142583        0.004138\n",
       "21           0.140033         0.001005         0.142296        0.004183\n",
       "22           0.139593         0.001189         0.142337        0.004361\n",
       "23           0.139327         0.001213         0.142091        0.004362\n",
       "24           0.139163         0.001318         0.142296        0.004461\n",
       "25           0.138672         0.001210         0.141968        0.004211\n",
       "26           0.138405         0.000971         0.141845        0.004204\n",
       "27           0.138405         0.001192         0.141600        0.003864\n",
       "28           0.137894         0.001167         0.141559        0.003927\n",
       "29           0.137740         0.001109         0.141436        0.003924\n",
       "..                ...              ...              ...             ...\n",
       "212          0.109803         0.001026         0.129602        0.000724\n",
       "213          0.109742         0.000883         0.129356        0.000749\n",
       "214          0.109639         0.000740         0.129356        0.000705\n",
       "215          0.109352         0.000873         0.129520        0.000619\n",
       "216          0.109373         0.000724         0.129643        0.000603\n",
       "217          0.109260         0.000666         0.129684        0.000889\n",
       "218          0.109045         0.000746         0.129643        0.000970\n",
       "219          0.108913         0.000858         0.129806        0.001305\n",
       "220          0.108974         0.001029         0.129806        0.001095\n",
       "221          0.108800         0.000874         0.129725        0.001054\n",
       "222          0.108789         0.000946         0.129929        0.001062\n",
       "223          0.108667         0.000820         0.129847        0.000811\n",
       "224          0.108544         0.000788         0.130134        0.000549\n",
       "225          0.108534         0.000941         0.130298        0.000563\n",
       "226          0.108380         0.001172         0.130052        0.000798\n",
       "227          0.108308         0.001235         0.130011        0.000921\n",
       "228          0.108237         0.001185         0.129888        0.000988\n",
       "229          0.108165         0.001115         0.129847        0.000968\n",
       "230          0.108278         0.001114         0.129643        0.000558\n",
       "231          0.108083         0.001202         0.129233        0.000756\n",
       "232          0.108217         0.001176         0.129192        0.000785\n",
       "233          0.108104         0.001217         0.129192        0.000897\n",
       "234          0.107899         0.001240         0.129151        0.000758\n",
       "235          0.107643         0.001327         0.129151        0.000734\n",
       "236          0.107633         0.001315         0.129315        0.000824\n",
       "237          0.107663         0.001255         0.129315        0.000663\n",
       "238          0.107592         0.001173         0.129274        0.000596\n",
       "239          0.107397         0.001278         0.129192        0.000574\n",
       "240          0.107305         0.001373         0.129233        0.000634\n",
       "241          0.107315         0.001315         0.129151        0.000616\n",
       "\n",
       "[242 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=1\n",
    "    )\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'n_estimators': 300}\n",
      "Run 0 best score:  0.8694156668441095\n",
      "Run 1 best param:  {'n_estimators': 200}\n",
      "Run 1 best score:  0.8706031694033823\n",
      "Run 2 best param:  {'n_estimators': 200}\n",
      "Run 2 best score:  0.8705622210392695\n",
      "Run 3 best param:  {'n_estimators': 200}\n",
      "Run 3 best score:  0.8707260144957208\n",
      "Run 4 best param:  {'n_estimators': 200}\n",
      "Run 4 best score:  0.8707260144957208\n",
      "Run 5 best param:  {'n_estimators': 300}\n",
      "Run 5 best score:  0.869579460300561\n",
      "Run 6 best param:  {'n_estimators': 241}\n",
      "Run 6 best score:  0.8701117890340281\n",
      "Run 7 best param:  {'n_estimators': 200}\n",
      "Run 7 best score:  0.8704803243110437\n",
      "Run 8 best param:  {'n_estimators': 100}\n",
      "Run 8 best score:  0.8699070472134638\n",
      "Best params:  params               {'n_estimators': 200}\n",
      "mean_test_score_0                 0.869252\n",
      "mean_test_score_1                 0.870603\n",
      "mean_test_score_2                 0.870562\n",
      "mean_test_score_3                 0.870726\n",
      "mean_test_score_4                 0.870726\n",
      "mean_test_score_5                 0.869252\n",
      "mean_test_score_6                 0.869784\n",
      "mean_test_score_7                  0.87048\n",
      "mean_test_score_8                 0.869866\n",
      "avg                               0.870139\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(100, 1000, 100)]+[241]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=241,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'n_estimators': 150}\n",
      "Run 0 best score:  0.8699889439416896\n",
      "Run 1 best param:  {'n_estimators': 210}\n",
      "Run 1 best score:  0.8708898079521723\n",
      "Run 2 best param:  {'n_estimators': 190}\n",
      "Run 2 best score:  0.8713811883215266\n",
      "Run 3 best param:  {'n_estimators': 170}\n",
      "Run 3 best score:  0.8717087752344294\n",
      "Run 4 best param:  {'n_estimators': 190}\n",
      "Run 4 best score:  0.8708488595880595\n",
      "Run 5 best param:  {'n_estimators': 150}\n",
      "Run 5 best score:  0.869866098849351\n",
      "Run 6 best param:  {'n_estimators': 170}\n",
      "Run 6 best score:  0.8705622210392695\n",
      "Run 7 best param:  {'n_estimators': 190}\n",
      "Run 7 best score:  0.8706441177674952\n",
      "Run 8 best param:  {'n_estimators': 210}\n",
      "Run 8 best score:  0.8697432537570124\n",
      "Best params:  params               {'n_estimators': 170}\n",
      "mean_test_score_0                 0.869989\n",
      "mean_test_score_1                 0.870276\n",
      "mean_test_score_2                 0.871217\n",
      "mean_test_score_3                 0.871709\n",
      "mean_test_score_4                 0.870562\n",
      "mean_test_score_5                 0.868924\n",
      "mean_test_score_6                 0.870562\n",
      "mean_test_score_7                 0.869661\n",
      "mean_test_score_8                 0.869498\n",
      "avg                               0.870266\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(150, 250, 20)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=241,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'colsample_bytree': 0.6, 'subsample': 0.8}\n",
      "Run 0 best score:  0.8710945497727366\n",
      "Run 1 best param:  {'colsample_bytree': 0.7, 'subsample': 0.6}\n",
      "Run 1 best score:  0.8713811883215266\n",
      "Run 2 best param:  {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "Run 2 best score:  0.8718725686908808\n",
      "Run 3 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 3 best score:  0.8717087752344294\n",
      "Run 4 best param:  {'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "Run 4 best score:  0.870971704680398\n",
      "Run 5 best param:  {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "Run 5 best score:  0.8708079112239466\n",
      "Run 6 best param:  {'colsample_bytree': 0.7, 'subsample': 0.8}\n",
      "Run 6 best score:  0.8713811883215266\n",
      "Run 7 best param:  {'colsample_bytree': 0.7, 'subsample': 0.6}\n",
      "Run 7 best score:  0.8705212726751567\n",
      "Run 8 best param:  {'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "Run 8 best score:  0.8700708406699152\n",
      "Best params:  params               {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "mean_test_score_0                                       0.870562\n",
      "mean_test_score_1                                        0.87089\n",
      "mean_test_score_2                                       0.871873\n",
      "mean_test_score_3                                       0.870726\n",
      "mean_test_score_4                                       0.870276\n",
      "mean_test_score_5                                       0.870808\n",
      "mean_test_score_6                                       0.869784\n",
      "mean_test_score_7                                       0.870235\n",
      "mean_test_score_8                                        0.86962\n",
      "avg                                                      0.87053\n",
      "Name: 7, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score4 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=170,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test4,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch4.fit(X_train,y_train)    \n",
    "    if grid_score4.empty:\n",
    "        grid_score4 = pd.DataFrame(gsearch4.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score4.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score4['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch4.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch4.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch4.best_score_)\n",
    "\n",
    "grid_score4['avg'] = grid_score4.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score4.loc[grid_score4.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "Run 0 best score:  0.8705622210392695\n",
      "Run 1 best param:  {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "Run 1 best score:  0.8708898079521723\n",
      "Run 2 best param:  {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "Run 2 best score:  0.8718725686908808\n",
      "Run 3 best param:  {'colsample_bytree': 0.75, 'subsample': 0.9}\n",
      "Run 3 best score:  0.8713811883215266\n",
      "Run 4 best param:  {'colsample_bytree': 0.75, 'subsample': 0.95}\n",
      "Run 4 best score:  0.8708488595880595\n",
      "Run 5 best param:  {'colsample_bytree': 0.75, 'subsample': 0.95}\n",
      "Run 5 best score:  0.8713402399574137\n",
      "Run 6 best param:  {'colsample_bytree': 0.75, 'subsample': 0.85}\n",
      "Run 6 best score:  0.8713402399574137\n",
      "Run 7 best param:  {'colsample_bytree': 0.7, 'subsample': 0.95}\n",
      "Run 7 best score:  0.8710126530445109\n",
      "Run 8 best param:  {'colsample_bytree': 0.75, 'subsample': 0.9}\n",
      "Run 8 best score:  0.8701117890340281\n",
      "Best params:  params               {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "mean_test_score_0                                       0.870562\n",
      "mean_test_score_1                                        0.87089\n",
      "mean_test_score_2                                       0.871873\n",
      "mean_test_score_3                                       0.870726\n",
      "mean_test_score_4                                       0.870276\n",
      "mean_test_score_5                                       0.870808\n",
      "mean_test_score_6                                       0.869784\n",
      "mean_test_score_7                                       0.870235\n",
      "mean_test_score_8                                        0.86962\n",
      "avg                                                      0.87053\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Carefully search for each neighboring 0.05\n",
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(85,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(65,80,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score5 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=170,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch5 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test5,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch5.fit(X_train,y_train)    \n",
    "    if grid_score5.empty:\n",
    "        grid_score5 = pd.DataFrame(gsearch5.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score5.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score5['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch5.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch5.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch5.best_score_)\n",
    "\n",
    "grid_score5['avg'] = grid_score5.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score5.loc[grid_score5.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'reg_alpha': 0}\n",
      "Run 0 best score:  0.8705622210392695\n",
      "Run 1 best param:  {'reg_alpha': 0}\n",
      "Run 1 best score:  0.8708898079521723\n",
      "Run 2 best param:  {'reg_alpha': 0.01}\n",
      "Run 2 best score:  0.8722001556037836\n",
      "Run 3 best param:  {'reg_alpha': 0.1}\n",
      "Run 3 best score:  0.871258343229188\n",
      "Run 4 best param:  {'reg_alpha': 1}\n",
      "Run 4 best score:  0.8706441177674952\n",
      "Run 5 best param:  {'reg_alpha': 0}\n",
      "Run 5 best score:  0.8708079112239466\n",
      "Run 6 best param:  {'reg_alpha': 0.01}\n",
      "Run 6 best score:  0.8707669628598338\n",
      "Run 7 best param:  {'reg_alpha': 1}\n",
      "Run 7 best score:  0.8708079112239466\n",
      "Run 8 best param:  {'reg_alpha': 0.01}\n",
      "Run 8 best score:  0.8697432537570124\n",
      "Best params:  params               {'reg_alpha': 0.01}\n",
      "mean_test_score_0               0.870521\n",
      "mean_test_score_1               0.869907\n",
      "mean_test_score_2                 0.8722\n",
      "mean_test_score_3               0.871135\n",
      "mean_test_score_4               0.870521\n",
      "mean_test_score_5               0.870317\n",
      "mean_test_score_6               0.870767\n",
      "mean_test_score_7               0.870153\n",
      "mean_test_score_8               0.869743\n",
      "avg                             0.870585\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[0, 1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score6 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=170,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch6 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test6,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch6.fit(X_train,y_train)    \n",
    "    if grid_score6.empty:\n",
    "        grid_score6 = pd.DataFrame(gsearch6.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score6.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score6['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch6.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch6.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch6.best_score_)\n",
    "\n",
    "grid_score6['avg'] = grid_score6.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score6.loc[grid_score6.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'reg_alpha': 0.0005}\n",
      "Run 0 best score:  0.8707260144957208\n",
      "Run 1 best param:  {'reg_alpha': 0.0001}\n",
      "Run 1 best score:  0.8708898079521723\n",
      "Run 2 best param:  {'reg_alpha': 0.001}\n",
      "Run 2 best score:  0.8722001556037836\n",
      "Run 3 best param:  {'reg_alpha': 0.005}\n",
      "Run 3 best score:  0.8715040334138651\n",
      "Run 4 best param:  {'reg_alpha': 0.01}\n",
      "Run 4 best score:  0.8705212726751567\n",
      "Run 5 best param:  {'reg_alpha': 0.0001}\n",
      "Run 5 best score:  0.8708079112239466\n",
      "Run 6 best param:  {'reg_alpha': 0.005}\n",
      "Run 6 best score:  0.8708488595880595\n",
      "Run 7 best param:  {'reg_alpha': 0.005}\n",
      "Run 7 best score:  0.8710945497727366\n",
      "Run 8 best param:  {'reg_alpha': 0.005}\n",
      "Run 8 best score:  0.869866098849351\n",
      "Best params:  params               {'reg_alpha': 0.01}\n",
      "mean_test_score_0               0.870521\n",
      "mean_test_score_1               0.869907\n",
      "mean_test_score_2                 0.8722\n",
      "mean_test_score_3               0.871135\n",
      "mean_test_score_4               0.870521\n",
      "mean_test_score_5               0.870317\n",
      "mean_test_score_6               0.870767\n",
      "mean_test_score_7               0.870153\n",
      "mean_test_score_8               0.869743\n",
      "avg                             0.870585\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[1e-4, 5e-4, 1e-3, 5e-3, 1e-2]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score7 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=170,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch7 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test7,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch7.fit(X_train,y_train)    \n",
    "    if grid_score7.empty:\n",
    "        grid_score7 = pd.DataFrame(gsearch7.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score7.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score7['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch7.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch7.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch7.best_score_)\n",
    "\n",
    "grid_score7['avg'] = grid_score7.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score7.loc[grid_score7.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the learning rate and tune n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.167683</td>\n",
       "      <td>0.008088</td>\n",
       "      <td>0.171861</td>\n",
       "      <td>0.012468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146257</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>0.149503</td>\n",
       "      <td>0.003553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150465</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.153065</td>\n",
       "      <td>0.007127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.146913</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.150772</td>\n",
       "      <td>0.004352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.147998</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>0.151468</td>\n",
       "      <td>0.007145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.147957</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>0.151550</td>\n",
       "      <td>0.007860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.147905</td>\n",
       "      <td>0.003402</td>\n",
       "      <td>0.150649</td>\n",
       "      <td>0.005940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.147660</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.150281</td>\n",
       "      <td>0.003787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.148090</td>\n",
       "      <td>0.003136</td>\n",
       "      <td>0.150608</td>\n",
       "      <td>0.004905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.147557</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.149912</td>\n",
       "      <td>0.003202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.147025</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.149339</td>\n",
       "      <td>0.004094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.147506</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.149216</td>\n",
       "      <td>0.005094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.146851</td>\n",
       "      <td>0.001449</td>\n",
       "      <td>0.148561</td>\n",
       "      <td>0.003717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.146851</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.148602</td>\n",
       "      <td>0.003750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.146800</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.148315</td>\n",
       "      <td>0.003649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.146861</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.148807</td>\n",
       "      <td>0.003439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.146790</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.148684</td>\n",
       "      <td>0.003353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.146913</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.148725</td>\n",
       "      <td>0.003265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.146810</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.148397</td>\n",
       "      <td>0.003349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.146575</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.148479</td>\n",
       "      <td>0.003463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.146523</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.148602</td>\n",
       "      <td>0.003329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.146022</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.148274</td>\n",
       "      <td>0.002978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.146431</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.148356</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.145991</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.148192</td>\n",
       "      <td>0.002969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.145920</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.148151</td>\n",
       "      <td>0.002575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.146298</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.148233</td>\n",
       "      <td>0.003079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.146298</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.148069</td>\n",
       "      <td>0.002736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.146329</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.148029</td>\n",
       "      <td>0.002704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.146298</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.147824</td>\n",
       "      <td>0.002798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.145827</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.147783</td>\n",
       "      <td>0.003006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>0.141344</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.143893</td>\n",
       "      <td>0.004044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.141282</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.143892</td>\n",
       "      <td>0.004046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.141323</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.143893</td>\n",
       "      <td>0.004128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.141333</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.143852</td>\n",
       "      <td>0.004038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.141313</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.143811</td>\n",
       "      <td>0.003896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>0.141292</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.143729</td>\n",
       "      <td>0.003982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>0.141210</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.143770</td>\n",
       "      <td>0.003817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>0.141180</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.143852</td>\n",
       "      <td>0.003816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.141139</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.143688</td>\n",
       "      <td>0.003896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>0.141231</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.143565</td>\n",
       "      <td>0.003756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.141170</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.143606</td>\n",
       "      <td>0.003776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.141108</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.143770</td>\n",
       "      <td>0.003883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.140985</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.143483</td>\n",
       "      <td>0.003813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.140985</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.143483</td>\n",
       "      <td>0.003738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.140955</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.143606</td>\n",
       "      <td>0.003759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.140852</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.143360</td>\n",
       "      <td>0.003612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.140832</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.143156</td>\n",
       "      <td>0.003612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.140862</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.143320</td>\n",
       "      <td>0.003637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0.140770</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.143238</td>\n",
       "      <td>0.003610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>0.140770</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.143196</td>\n",
       "      <td>0.003518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>0.140729</td>\n",
       "      <td>0.000920</td>\n",
       "      <td>0.142951</td>\n",
       "      <td>0.003631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>0.140750</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.143033</td>\n",
       "      <td>0.003779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.140739</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.143033</td>\n",
       "      <td>0.003853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>0.140688</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.142828</td>\n",
       "      <td>0.003776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>0.140760</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.142705</td>\n",
       "      <td>0.003710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.140709</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.142746</td>\n",
       "      <td>0.003784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>0.140627</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.142705</td>\n",
       "      <td>0.003481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>0.140514</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.142460</td>\n",
       "      <td>0.003516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>0.140525</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>0.142583</td>\n",
       "      <td>0.003537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>0.140412</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.142296</td>\n",
       "      <td>0.003503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.167683         0.008088         0.171861        0.012468\n",
       "1            0.146257         0.002849         0.149503        0.003553\n",
       "2            0.150465         0.003592         0.153065        0.007127\n",
       "3            0.146913         0.000984         0.150772        0.004352\n",
       "4            0.147998         0.003822         0.151468        0.007145\n",
       "5            0.147957         0.003849         0.151550        0.007860\n",
       "6            0.147905         0.003402         0.150649        0.005940\n",
       "7            0.147660         0.003596         0.150281        0.003787\n",
       "8            0.148090         0.003136         0.150608        0.004905\n",
       "9            0.147557         0.002717         0.149912        0.003202\n",
       "10           0.147025         0.001762         0.149339        0.004094\n",
       "11           0.147506         0.001567         0.149216        0.005094\n",
       "12           0.146851         0.001449         0.148561        0.003717\n",
       "13           0.146851         0.001256         0.148602        0.003750\n",
       "14           0.146800         0.001401         0.148315        0.003649\n",
       "15           0.146861         0.001483         0.148807        0.003439\n",
       "16           0.146790         0.001478         0.148684        0.003353\n",
       "17           0.146913         0.001299         0.148725        0.003265\n",
       "18           0.146810         0.001439         0.148397        0.003349\n",
       "19           0.146575         0.001321         0.148479        0.003463\n",
       "20           0.146523         0.001362         0.148602        0.003329\n",
       "21           0.146022         0.002070         0.148274        0.002978\n",
       "22           0.146431         0.001455         0.148356        0.003100\n",
       "23           0.145991         0.002120         0.148192        0.002969\n",
       "24           0.145920         0.002073         0.148151        0.002575\n",
       "25           0.146298         0.001576         0.148233        0.003079\n",
       "26           0.146298         0.001523         0.148069        0.002736\n",
       "27           0.146329         0.001502         0.148029        0.002704\n",
       "28           0.146298         0.001439         0.147824        0.002798\n",
       "29           0.145827         0.001951         0.147783        0.003006\n",
       "..                ...              ...              ...             ...\n",
       "135          0.141344         0.000716         0.143893        0.004044\n",
       "136          0.141282         0.000754         0.143892        0.004046\n",
       "137          0.141323         0.000777         0.143893        0.004128\n",
       "138          0.141333         0.000766         0.143852        0.004038\n",
       "139          0.141313         0.000737         0.143811        0.003896\n",
       "140          0.141292         0.000770         0.143729        0.003982\n",
       "141          0.141210         0.000789         0.143770        0.003817\n",
       "142          0.141180         0.000717         0.143852        0.003816\n",
       "143          0.141139         0.000717         0.143688        0.003896\n",
       "144          0.141231         0.000847         0.143565        0.003756\n",
       "145          0.141170         0.000906         0.143606        0.003776\n",
       "146          0.141108         0.000886         0.143770        0.003883\n",
       "147          0.140985         0.000924         0.143483        0.003813\n",
       "148          0.140985         0.000958         0.143483        0.003738\n",
       "149          0.140955         0.000855         0.143606        0.003759\n",
       "150          0.140852         0.000805         0.143360        0.003612\n",
       "151          0.140832         0.000813         0.143156        0.003612\n",
       "152          0.140862         0.000864         0.143320        0.003637\n",
       "153          0.140770         0.000922         0.143238        0.003610\n",
       "154          0.140770         0.000879         0.143196        0.003518\n",
       "155          0.140729         0.000920         0.142951        0.003631\n",
       "156          0.140750         0.000931         0.143033        0.003779\n",
       "157          0.140739         0.000899         0.143033        0.003853\n",
       "158          0.140688         0.000977         0.142828        0.003776\n",
       "159          0.140760         0.000822         0.142705        0.003710\n",
       "160          0.140709         0.000873         0.142746        0.003784\n",
       "161          0.140627         0.000928         0.142705        0.003481\n",
       "162          0.140514         0.000939         0.142460        0.003516\n",
       "163          0.140525         0.000965         0.142583        0.003537\n",
       "164          0.140412         0.000967         0.142296        0.003503\n",
       "\n",
       "[165 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.0,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.7,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    reg_alpha=0.01,\n",
    "    scale_pos_weight=1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'n_estimators': 1400}\n",
      "Run 0 best score:  0.8702755824904795\n",
      "Run 1 best param:  {'n_estimators': 1500}\n",
      "Run 1 best score:  0.8700708406699152\n",
      "Run 2 best param:  {'n_estimators': 1500}\n",
      "Run 2 best score:  0.870398427582818\n",
      "Run 3 best param:  {'n_estimators': 1500}\n",
      "Run 3 best score:  0.8717497235985422\n",
      "Run 4 best param:  {'n_estimators': 1500}\n",
      "Run 4 best score:  0.869866098849351\n",
      "Run 5 best param:  {'n_estimators': 1500}\n",
      "Run 5 best score:  0.8704393759469309\n",
      "Run 6 best param:  {'n_estimators': 1400}\n",
      "Run 6 best score:  0.8701527373981409\n",
      "Run 7 best param:  {'n_estimators': 1500}\n",
      "Run 7 best score:  0.8708079112239466\n",
      "Run 8 best param:  {'n_estimators': 1300}\n",
      "Run 8 best score:  0.8704393759469309\n",
      "Best params:  params               {'n_estimators': 1500}\n",
      "mean_test_score_0                  0.870071\n",
      "mean_test_score_1                  0.870071\n",
      "mean_test_score_2                  0.870398\n",
      "mean_test_score_3                   0.87175\n",
      "mean_test_score_4                  0.869866\n",
      "mean_test_score_5                  0.870439\n",
      "mean_test_score_6                  0.869989\n",
      "mean_test_score_7                  0.870808\n",
      "mean_test_score_8                  0.870071\n",
      "avg                                0.870385\n",
      "Name: 14, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(100, 1501, 100)]+[164]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=5000,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        reg_alpha=0.01,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 9 times\n",
      "Run 0 best param:  {'n_estimators': 2400}\n",
      "Run 0 best score:  0.8705212726751567\n",
      "Run 1 best param:  {'n_estimators': 2400}\n",
      "Run 1 best score:  0.8705622210392695\n",
      "Run 2 best param:  {'n_estimators': 2200}\n",
      "Run 2 best score:  0.8713811883215266\n",
      "Run 3 best param:  {'n_estimators': 1500}\n",
      "Run 3 best score:  0.8717497235985422\n",
      "Run 4 best param:  {'n_estimators': 2100}\n",
      "Run 4 best score:  0.8704803243110437\n",
      "Run 5 best param:  {'n_estimators': 2300}\n",
      "Run 5 best score:  0.870971704680398\n",
      "Run 6 best param:  {'n_estimators': 2000}\n",
      "Run 6 best score:  0.8715040334138651\n",
      "Run 7 best param:  {'n_estimators': 2200}\n",
      "Run 7 best score:  0.8715040334138651\n",
      "Run 8 best param:  {'n_estimators': 1900}\n",
      "Run 8 best score:  0.8705622210392695\n",
      "Best params:  params               {'n_estimators': 2200}\n",
      "mean_test_score_0                  0.870194\n",
      "mean_test_score_1                  0.870194\n",
      "mean_test_score_2                  0.871381\n",
      "mean_test_score_3                  0.871095\n",
      "mean_test_score_4                  0.870276\n",
      "mean_test_score_5                  0.870931\n",
      "mean_test_score_6                  0.871299\n",
      "mean_test_score_7                  0.871504\n",
      "mean_test_score_8                  0.870521\n",
      "avg                                0.870822\n",
      "Name: 7, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(1500, 2501, 100)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=5000,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        reg_alpha=0.01,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0: 87.58%\n",
      "Accuracy 1: 87.54%\n",
      "Accuracy 2: 87.56%\n",
      "Accuracy 3: 87.54%\n",
      "Accuracy 4: 87.56%\n",
      "Accuracy 5: 87.61%\n",
      "Accuracy 6: 87.59%\n",
      "Accuracy 7: 87.54%\n",
      "Accuracy 8: 87.55%\n",
      "Average accuracy is: 87.56%\n"
     ]
    }
   ],
   "source": [
    "accuracy_array = []\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=2200,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        reg_alpha=0.01,\n",
    "        scale_pos_weight=1,\n",
    "        seed=i\n",
    "    )\n",
    "    model = xgb.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_array.append(accuracy)\n",
    "    print('Accuracy {}: %.2f%%'.format(i) % (accuracy * 100.0))\n",
    "mean_accuracy_score = sum(accuracy_array) / NUM_TRIALS\n",
    "print('Average accuracy is: %.2f%%' % (mean_accuracy_score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
