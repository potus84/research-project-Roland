{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniBooNE dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xgboost\n",
    "import copy\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/tai/Projects/research-project-Roland')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/MiniBooNE/MiniBooNE.0.train\", encoding='latin1',\n",
    "                 na_values='?',\n",
    "                 header=None,\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>5.67570</td>\n",
       "      <td>1.001840</td>\n",
       "      <td>131.3430</td>\n",
       "      <td>0.298118</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.228417</td>\n",
       "      <td>1.094780</td>\n",
       "      <td>0.820118</td>\n",
       "      <td>3.49877</td>\n",
       "      <td>...</td>\n",
       "      <td>157.612</td>\n",
       "      <td>-42.93680</td>\n",
       "      <td>0.478071</td>\n",
       "      <td>12.17450</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.192053</td>\n",
       "      <td>-0.483775</td>\n",
       "      <td>2.675020</td>\n",
       "      <td>0.487935</td>\n",
       "      <td>0.203891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>4.21253</td>\n",
       "      <td>0.603285</td>\n",
       "      <td>173.0420</td>\n",
       "      <td>0.328132</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>0.150331</td>\n",
       "      <td>0.805325</td>\n",
       "      <td>0.857529</td>\n",
       "      <td>3.06962</td>\n",
       "      <td>...</td>\n",
       "      <td>188.546</td>\n",
       "      <td>15.01370</td>\n",
       "      <td>-0.253118</td>\n",
       "      <td>2.26856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046123</td>\n",
       "      <td>0.317755</td>\n",
       "      <td>3.910130</td>\n",
       "      <td>0.421626</td>\n",
       "      <td>0.319543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B</td>\n",
       "      <td>4.19362</td>\n",
       "      <td>1.040140</td>\n",
       "      <td>53.9026</td>\n",
       "      <td>0.254287</td>\n",
       "      <td>0.004630</td>\n",
       "      <td>0.243071</td>\n",
       "      <td>1.041770</td>\n",
       "      <td>0.878942</td>\n",
       "      <td>3.20468</td>\n",
       "      <td>...</td>\n",
       "      <td>141.582</td>\n",
       "      <td>-2.06235</td>\n",
       "      <td>0.755769</td>\n",
       "      <td>3.43672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.147755</td>\n",
       "      <td>0.270756</td>\n",
       "      <td>1.860970</td>\n",
       "      <td>0.930870</td>\n",
       "      <td>0.247875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>4.12466</td>\n",
       "      <td>1.340520</td>\n",
       "      <td>126.2690</td>\n",
       "      <td>0.288492</td>\n",
       "      <td>0.018083</td>\n",
       "      <td>0.148613</td>\n",
       "      <td>0.941860</td>\n",
       "      <td>0.859737</td>\n",
       "      <td>3.25207</td>\n",
       "      <td>...</td>\n",
       "      <td>169.137</td>\n",
       "      <td>-30.83770</td>\n",
       "      <td>-0.042714</td>\n",
       "      <td>3.45540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179401</td>\n",
       "      <td>-0.047517</td>\n",
       "      <td>2.142570</td>\n",
       "      <td>1.411120</td>\n",
       "      <td>0.250612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>3.82513</td>\n",
       "      <td>1.153430</td>\n",
       "      <td>299.1060</td>\n",
       "      <td>0.252850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.541220</td>\n",
       "      <td>0.903657</td>\n",
       "      <td>3.09860</td>\n",
       "      <td>...</td>\n",
       "      <td>115.348</td>\n",
       "      <td>-0.50963</td>\n",
       "      <td>0.716968</td>\n",
       "      <td>3.72687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151389</td>\n",
       "      <td>0.299851</td>\n",
       "      <td>1.215630</td>\n",
       "      <td>-0.192377</td>\n",
       "      <td>0.226930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B</td>\n",
       "      <td>6.18249</td>\n",
       "      <td>2.147570</td>\n",
       "      <td>99.4795</td>\n",
       "      <td>0.267395</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.106472</td>\n",
       "      <td>0.961092</td>\n",
       "      <td>0.802230</td>\n",
       "      <td>3.50252</td>\n",
       "      <td>...</td>\n",
       "      <td>139.243</td>\n",
       "      <td>-54.52520</td>\n",
       "      <td>0.541931</td>\n",
       "      <td>6.96016</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.153358</td>\n",
       "      <td>-0.065449</td>\n",
       "      <td>1.951540</td>\n",
       "      <td>0.604315</td>\n",
       "      <td>0.230527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B</td>\n",
       "      <td>5.20087</td>\n",
       "      <td>1.883870</td>\n",
       "      <td>112.7230</td>\n",
       "      <td>0.383446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139630</td>\n",
       "      <td>0.791168</td>\n",
       "      <td>0.843921</td>\n",
       "      <td>3.28384</td>\n",
       "      <td>...</td>\n",
       "      <td>138.243</td>\n",
       "      <td>-52.29130</td>\n",
       "      <td>1.752540</td>\n",
       "      <td>8.63231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259622</td>\n",
       "      <td>-0.389033</td>\n",
       "      <td>2.993190</td>\n",
       "      <td>2.140160</td>\n",
       "      <td>0.207261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B</td>\n",
       "      <td>4.56052</td>\n",
       "      <td>1.619860</td>\n",
       "      <td>100.7810</td>\n",
       "      <td>0.282722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128536</td>\n",
       "      <td>0.809710</td>\n",
       "      <td>0.768455</td>\n",
       "      <td>3.36314</td>\n",
       "      <td>...</td>\n",
       "      <td>153.667</td>\n",
       "      <td>-13.72970</td>\n",
       "      <td>0.696742</td>\n",
       "      <td>4.76317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116484</td>\n",
       "      <td>-0.085670</td>\n",
       "      <td>1.949260</td>\n",
       "      <td>-0.130875</td>\n",
       "      <td>0.260908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B</td>\n",
       "      <td>6.39708</td>\n",
       "      <td>1.293730</td>\n",
       "      <td>108.5990</td>\n",
       "      <td>0.454244</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.111512</td>\n",
       "      <td>0.918662</td>\n",
       "      <td>0.782574</td>\n",
       "      <td>3.38881</td>\n",
       "      <td>...</td>\n",
       "      <td>163.252</td>\n",
       "      <td>-5.44261</td>\n",
       "      <td>0.551840</td>\n",
       "      <td>3.61821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326823</td>\n",
       "      <td>0.639289</td>\n",
       "      <td>5.640570</td>\n",
       "      <td>5.937600</td>\n",
       "      <td>0.198393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B</td>\n",
       "      <td>3.20906</td>\n",
       "      <td>0.638871</td>\n",
       "      <td>124.2980</td>\n",
       "      <td>0.233698</td>\n",
       "      <td>0.013139</td>\n",
       "      <td>0.383318</td>\n",
       "      <td>1.124330</td>\n",
       "      <td>0.911206</td>\n",
       "      <td>3.36329</td>\n",
       "      <td>...</td>\n",
       "      <td>136.395</td>\n",
       "      <td>4.27472</td>\n",
       "      <td>0.396741</td>\n",
       "      <td>5.69763</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.109576</td>\n",
       "      <td>-0.121126</td>\n",
       "      <td>0.773704</td>\n",
       "      <td>0.043680</td>\n",
       "      <td>0.265910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  0        1         2         3         4         5         6         7   \\\n",
       "0  B  5.67570  1.001840  131.3430  0.298118  0.012048  0.228417  1.094780   \n",
       "1  B  4.21253  0.603285  173.0420  0.328132  0.005568  0.150331  0.805325   \n",
       "2  B  4.19362  1.040140   53.9026  0.254287  0.004630  0.243071  1.041770   \n",
       "3  B  4.12466  1.340520  126.2690  0.288492  0.018083  0.148613  0.941860   \n",
       "4  S  3.82513  1.153430  299.1060  0.252850  0.000000  0.000000  1.541220   \n",
       "5  B  6.18249  2.147570   99.4795  0.267395  0.007042  0.106472  0.961092   \n",
       "6  B  5.20087  1.883870  112.7230  0.383446  0.000000  0.139630  0.791168   \n",
       "7  B  4.56052  1.619860  100.7810  0.282722  0.000000  0.128536  0.809710   \n",
       "8  B  6.39708  1.293730  108.5990  0.454244  0.004348  0.111512  0.918662   \n",
       "9  B  3.20906  0.638871  124.2980  0.233698  0.013139  0.383318  1.124330   \n",
       "\n",
       "         8        9   ...       41        42        43        44        45  \\\n",
       "0  0.820118  3.49877  ...  157.612 -42.93680  0.478071  12.17450  0.024096   \n",
       "1  0.857529  3.06962  ...  188.546  15.01370 -0.253118   2.26856  0.000000   \n",
       "2  0.878942  3.20468  ...  141.582  -2.06235  0.755769   3.43672  0.000000   \n",
       "3  0.859737  3.25207  ...  169.137 -30.83770 -0.042714   3.45540  0.000000   \n",
       "4  0.903657  3.09860  ...  115.348  -0.50963  0.716968   3.72687  0.000000   \n",
       "5  0.802230  3.50252  ...  139.243 -54.52520  0.541931   6.96016  0.002347   \n",
       "6  0.843921  3.28384  ...  138.243 -52.29130  1.752540   8.63231  0.000000   \n",
       "7  0.768455  3.36314  ...  153.667 -13.72970  0.696742   4.76317  0.000000   \n",
       "8  0.782574  3.38881  ...  163.252  -5.44261  0.551840   3.61821  0.000000   \n",
       "9  0.911206  3.36329  ...  136.395   4.27472  0.396741   5.69763  0.002920   \n",
       "\n",
       "         46        47        48        49        50  \n",
       "0  0.192053 -0.483775  2.675020  0.487935  0.203891  \n",
       "1  0.046123  0.317755  3.910130  0.421626  0.319543  \n",
       "2  0.147755  0.270756  1.860970  0.930870  0.247875  \n",
       "3  0.179401 -0.047517  2.142570  1.411120  0.250612  \n",
       "4  0.151389  0.299851  1.215630 -0.192377  0.226930  \n",
       "5  0.153358 -0.065449  1.951540  0.604315  0.230527  \n",
       "6  0.259622 -0.389033  2.993190  2.140160  0.207261  \n",
       "7  0.116484 -0.085670  1.949260 -0.130875  0.260908  \n",
       "8  0.326823  0.639289  5.640570  5.937600  0.198393  \n",
       "9  0.109576 -0.121126  0.773704  0.043680  0.265910  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/MiniBooNE/MiniBooNE.0.test\", encoding='latin1', \n",
    "                 header=None,\n",
    "                 na_values='?',\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "16    0\n",
       "17    0\n",
       "18    0\n",
       "19    0\n",
       "20    0\n",
       "21    0\n",
       "22    0\n",
       "23    0\n",
       "24    0\n",
       "25    0\n",
       "26    0\n",
       "27    0\n",
       "28    0\n",
       "29    0\n",
       "30    0\n",
       "31    0\n",
       "32    0\n",
       "33    0\n",
       "34    0\n",
       "35    0\n",
       "36    0\n",
       "37    0\n",
       "38    0\n",
       "39    0\n",
       "40    0\n",
       "41    0\n",
       "42    0\n",
       "43    0\n",
       "44    0\n",
       "45    0\n",
       "46    0\n",
       "47    0\n",
       "48    0\n",
       "49    0\n",
       "50    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "14    0\n",
       "15    0\n",
       "16    0\n",
       "17    0\n",
       "18    0\n",
       "19    0\n",
       "20    0\n",
       "21    0\n",
       "22    0\n",
       "23    0\n",
       "24    0\n",
       "25    0\n",
       "26    0\n",
       "27    0\n",
       "28    0\n",
       "29    0\n",
       "30    0\n",
       "31    0\n",
       "32    0\n",
       "33    0\n",
       "34    0\n",
       "35    0\n",
       "36    0\n",
       "37    0\n",
       "38    0\n",
       "39    0\n",
       "40    0\n",
       "41    0\n",
       "42    0\n",
       "43    0\n",
       "44    0\n",
       "45    0\n",
       "46    0\n",
       "47    0\n",
       "48    0\n",
       "49    0\n",
       "50    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary convertion of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop([0], axis=1)\n",
    "y_train = np.where(train[0] == 'S', 0, 1)\n",
    "\n",
    "X_test = test.drop([0], axis=1)\n",
    "y_test = np.where(test[0] == 'S', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.align(X_test, join='outer', fill_value=0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65032, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65032, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "            18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "            35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tuning on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.107655</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>0.112898</td>\n",
       "      <td>0.001686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.101089</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.106686</td>\n",
       "      <td>0.002531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.098048</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>0.103518</td>\n",
       "      <td>0.004369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.094899</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.100304</td>\n",
       "      <td>0.003360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093231</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.098029</td>\n",
       "      <td>0.002940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.090648</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.095599</td>\n",
       "      <td>0.002710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.088760</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.094108</td>\n",
       "      <td>0.002223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.087169</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.092385</td>\n",
       "      <td>0.002378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.086084</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.091909</td>\n",
       "      <td>0.001861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.085227</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.090817</td>\n",
       "      <td>0.001966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.084339</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.089971</td>\n",
       "      <td>0.001780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.083044</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.088572</td>\n",
       "      <td>0.001974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.082225</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.087895</td>\n",
       "      <td>0.001118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.081664</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.087603</td>\n",
       "      <td>0.001654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.080925</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.087357</td>\n",
       "      <td>0.001571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.080095</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.086927</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.078930</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.086096</td>\n",
       "      <td>0.001883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.078223</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.085220</td>\n",
       "      <td>0.002196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.077266</td>\n",
       "      <td>0.000554</td>\n",
       "      <td>0.084620</td>\n",
       "      <td>0.001937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.076301</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.083482</td>\n",
       "      <td>0.001511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.075424</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.082729</td>\n",
       "      <td>0.001669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.074748</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.081883</td>\n",
       "      <td>0.001847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.073748</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.081114</td>\n",
       "      <td>0.001808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.073145</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.081052</td>\n",
       "      <td>0.001851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.072703</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.080499</td>\n",
       "      <td>0.001998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.071626</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.080222</td>\n",
       "      <td>0.001772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.071188</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.079069</td>\n",
       "      <td>0.001447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.070704</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.078853</td>\n",
       "      <td>0.001374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.070188</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.077977</td>\n",
       "      <td>0.001490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.069454</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.077577</td>\n",
       "      <td>0.001440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.016065</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.056296</td>\n",
       "      <td>0.001287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.015988</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.056342</td>\n",
       "      <td>0.001478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.015938</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.056249</td>\n",
       "      <td>0.001507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.015800</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.056280</td>\n",
       "      <td>0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.015788</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.056219</td>\n",
       "      <td>0.001343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.015758</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.056249</td>\n",
       "      <td>0.001385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.015639</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.056219</td>\n",
       "      <td>0.001272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.015538</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.056065</td>\n",
       "      <td>0.001177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.056188</td>\n",
       "      <td>0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.015427</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.056219</td>\n",
       "      <td>0.001160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.015388</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.056219</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.015339</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.056203</td>\n",
       "      <td>0.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.015254</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.056157</td>\n",
       "      <td>0.001408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.015189</td>\n",
       "      <td>0.000203</td>\n",
       "      <td>0.056126</td>\n",
       "      <td>0.001459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.015116</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.056003</td>\n",
       "      <td>0.001535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.015100</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.056126</td>\n",
       "      <td>0.001606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.015043</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.056065</td>\n",
       "      <td>0.001529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.014962</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.056065</td>\n",
       "      <td>0.001614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.014908</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>0.056172</td>\n",
       "      <td>0.001540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.014812</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.056172</td>\n",
       "      <td>0.001579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.014739</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.001473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0.014654</td>\n",
       "      <td>0.000264</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>0.001528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0.014600</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.055880</td>\n",
       "      <td>0.001418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.014555</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.055865</td>\n",
       "      <td>0.001427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.014508</td>\n",
       "      <td>0.000239</td>\n",
       "      <td>0.055803</td>\n",
       "      <td>0.001371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.014447</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.055788</td>\n",
       "      <td>0.001372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.014354</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.055880</td>\n",
       "      <td>0.001337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0.014285</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.055942</td>\n",
       "      <td>0.001311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.014247</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.055896</td>\n",
       "      <td>0.001293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.014185</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.055773</td>\n",
       "      <td>0.001295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>428 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.107655         0.003351         0.112898        0.001686\n",
       "1            0.101089         0.000679         0.106686        0.002531\n",
       "2            0.098048         0.002493         0.103518        0.004369\n",
       "3            0.094899         0.000974         0.100304        0.003360\n",
       "4            0.093231         0.001553         0.098029        0.002940\n",
       "5            0.090648         0.001253         0.095599        0.002710\n",
       "6            0.088760         0.001119         0.094108        0.002223\n",
       "7            0.087169         0.001039         0.092385        0.002378\n",
       "8            0.086084         0.000675         0.091909        0.001861\n",
       "9            0.085227         0.000842         0.090817        0.001966\n",
       "10           0.084339         0.000704         0.089971        0.001780\n",
       "11           0.083044         0.001017         0.088572        0.001974\n",
       "12           0.082225         0.000566         0.087895        0.001118\n",
       "13           0.081664         0.000616         0.087603        0.001654\n",
       "14           0.080925         0.000482         0.087357        0.001571\n",
       "15           0.080095         0.000361         0.086927        0.001600\n",
       "16           0.078930         0.000439         0.086096        0.001883\n",
       "17           0.078223         0.000390         0.085220        0.002196\n",
       "18           0.077266         0.000554         0.084620        0.001937\n",
       "19           0.076301         0.000332         0.083482        0.001511\n",
       "20           0.075424         0.000540         0.082729        0.001669\n",
       "21           0.074748         0.000510         0.081883        0.001847\n",
       "22           0.073748         0.000496         0.081114        0.001808\n",
       "23           0.073145         0.000495         0.081052        0.001851\n",
       "24           0.072703         0.000359         0.080499        0.001998\n",
       "25           0.071626         0.000526         0.080222        0.001772\n",
       "26           0.071188         0.000561         0.079069        0.001447\n",
       "27           0.070704         0.000531         0.078853        0.001374\n",
       "28           0.070188         0.000719         0.077977        0.001490\n",
       "29           0.069454         0.000658         0.077577        0.001440\n",
       "..                ...              ...              ...             ...\n",
       "398          0.016065         0.000267         0.056296        0.001287\n",
       "399          0.015988         0.000262         0.056342        0.001478\n",
       "400          0.015938         0.000241         0.056249        0.001507\n",
       "401          0.015800         0.000276         0.056280        0.001421\n",
       "402          0.015788         0.000238         0.056219        0.001343\n",
       "403          0.015758         0.000228         0.056249        0.001385\n",
       "404          0.015639         0.000204         0.056219        0.001272\n",
       "405          0.015538         0.000221         0.056065        0.001177\n",
       "406          0.015504         0.000203         0.056188        0.001288\n",
       "407          0.015427         0.000200         0.056219        0.001160\n",
       "408          0.015388         0.000228         0.056219        0.001378\n",
       "409          0.015339         0.000222         0.056203        0.001465\n",
       "410          0.015254         0.000240         0.056157        0.001408\n",
       "411          0.015189         0.000203         0.056126        0.001459\n",
       "412          0.015116         0.000238         0.056003        0.001535\n",
       "413          0.015100         0.000210         0.056126        0.001606\n",
       "414          0.015043         0.000226         0.056065        0.001529\n",
       "415          0.014962         0.000188         0.056065        0.001614\n",
       "416          0.014908         0.000173         0.056172        0.001540\n",
       "417          0.014812         0.000221         0.056172        0.001579\n",
       "418          0.014739         0.000255         0.056019        0.001473\n",
       "419          0.014654         0.000264         0.056034        0.001528\n",
       "420          0.014600         0.000276         0.055880        0.001418\n",
       "421          0.014555         0.000260         0.055865        0.001427\n",
       "422          0.014508         0.000239         0.055803        0.001371\n",
       "423          0.014447         0.000222         0.055788        0.001372\n",
       "424          0.014354         0.000188         0.055880        0.001337\n",
       "425          0.014285         0.000165         0.055942        0.001311\n",
       "426          0.014247         0.000205         0.055896        0.001293\n",
       "427          0.014185         0.000207         0.055773        0.001295\n",
       "\n",
       "[428 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'max_depth': 9, 'min_child_weight': 1}\n",
      "Run 0 best score:  0.9454883749538688\n",
      "Run 1 best param:  {'max_depth': 9, 'min_child_weight': 1}\n",
      "Run 1 best score:  0.9455191290441629\n",
      "Run 2 best param:  {'max_depth': 7, 'min_child_weight': 1}\n",
      "Run 2 best score:  0.9455191290441629\n",
      "Run 3 best param:  {'max_depth': 7, 'min_child_weight': 1}\n",
      "Run 3 best score:  0.9456421454053389\n",
      "Best params:  params               {'max_depth': 9, 'min_child_weight': 1}\n",
      "mean_test_score_0                                   0.945488\n",
      "mean_test_score_1                                   0.945519\n",
      "mean_test_score_2                                   0.945442\n",
      "mean_test_score_3                                   0.945504\n",
      "avg                                                 0.945488\n",
      "Name: 28, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1 = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1,650,100)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=427,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1.fit(X_train,y_train)    \n",
    "    if grid_score1.empty:\n",
    "        grid_score1 = pd.DataFrame(gsearch1.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1.best_score_)\n",
    "\n",
    "grid_score1['avg'] = grid_score1.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1.loc[grid_score1.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'max_depth': 9, 'min_child_weight': 1}\n",
      "Run 0 best score:  0.9454883749538688\n",
      "Run 1 best param:  {'max_depth': 9, 'min_child_weight': 7}\n",
      "Run 1 best score:  0.945872801082544\n",
      "Run 2 best param:  {'max_depth': 7, 'min_child_weight': 1}\n",
      "Run 2 best score:  0.9455191290441629\n",
      "Run 3 best param:  {'max_depth': 9, 'min_child_weight': 3}\n",
      "Run 3 best score:  0.946057325624308\n",
      "Best params:  params               {'max_depth': 9, 'min_child_weight': 1}\n",
      "mean_test_score_0                                   0.945488\n",
      "mean_test_score_1                                   0.945519\n",
      "mean_test_score_2                                   0.945442\n",
      "mean_test_score_3                                   0.945504\n",
      "avg                                                 0.945488\n",
      "Name: 20, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1b = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1,10,2)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1b = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=427,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1b = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1b,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1b.fit(X_train,y_train)    \n",
    "    if grid_score1b.empty:\n",
    "        grid_score1b = pd.DataFrame(gsearch1b.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1b.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1b['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1b.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1b.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1b.best_score_)\n",
    "\n",
    "grid_score1b['avg'] = grid_score1b.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1b.loc[grid_score1b.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'max_depth': 10, 'min_child_weight': 2}\n",
      "Run 0 best score:  0.9458574240373969\n",
      "Run 1 best param:  {'max_depth': 9, 'min_child_weight': 1}\n",
      "Run 1 best score:  0.9455191290441629\n",
      "Run 2 best param:  {'max_depth': 10, 'min_child_weight': 1}\n",
      "Run 2 best score:  0.945934309263132\n",
      "Run 3 best param:  {'max_depth': 10, 'min_child_weight': 1}\n",
      "Run 3 best score:  0.946057325624308\n",
      "Best params:  params               {'max_depth': 10, 'min_child_weight': 1}\n",
      "mean_test_score_0                                    0.945119\n",
      "mean_test_score_1                                    0.945458\n",
      "mean_test_score_2                                    0.945934\n",
      "mean_test_score_3                                    0.946057\n",
      "avg                                                  0.945642\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look carefully again the neigbor values\n",
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test2 = {\n",
    " 'max_depth':[9, 10],\n",
    " 'min_child_weight':[1, 2]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score2 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=427,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,        \n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test2,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch2.fit(X_train,y_train)    \n",
    "    if grid_score2.empty:\n",
    "        grid_score2 = pd.DataFrame(gsearch2.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score2.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score2['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch2.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch2.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch2.best_score_)\n",
    "\n",
    "grid_score2['avg'] = grid_score2.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score2.loc[grid_score2.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'max_depth': 20}\n",
      "Run 0 best score:  0.9453653585926928\n",
      "Run 1 best param:  {'max_depth': 10}\n",
      "Run 1 best score:  0.9454576208635749\n",
      "Run 2 best param:  {'max_depth': 10}\n",
      "Run 2 best score:  0.945934309263132\n",
      "Run 3 best param:  {'max_depth': 10}\n",
      "Run 3 best score:  0.946057325624308\n",
      "Best params:  params               {'max_depth': 10}\n",
      "mean_test_score_0             0.945119\n",
      "mean_test_score_1             0.945458\n",
      "mean_test_score_2             0.945934\n",
      "mean_test_score_3             0.946057\n",
      "avg                           0.945642\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look carefully again the neigbor values\n",
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test2 = {\n",
    " 'max_depth':[i for i in range (10, 30, 5)],\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score2 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=427,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,        \n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test2,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch2.fit(X_train,y_train)    \n",
    "    if grid_score2.empty:\n",
    "        grid_score2 = pd.DataFrame(gsearch2.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score2.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score2['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch2.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch2.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch2.best_score_)\n",
    "\n",
    "grid_score2['avg'] = grid_score2.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score2.loc[grid_score2.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'gamma': 0.1}\n",
      "Run 0 best score:  0.945903555172838\n",
      "Run 1 best param:  {'gamma': 0.0}\n",
      "Run 1 best score:  0.9454576208635749\n",
      "Run 2 best param:  {'gamma': 0.0}\n",
      "Run 2 best score:  0.945934309263132\n",
      "Run 3 best param:  {'gamma': 0.0}\n",
      "Run 3 best score:  0.946057325624308\n",
      "Best params:  params               {'gamma': 0.1}\n",
      "mean_test_score_0          0.945904\n",
      "mean_test_score_1          0.945104\n",
      "mean_test_score_2          0.945734\n",
      "mean_test_score_3          0.945904\n",
      "avg                        0.945661\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score3 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=427,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch3 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test3,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch3.fit(X_train,y_train)    \n",
    "    if grid_score3.empty:\n",
    "        grid_score3 = pd.DataFrame(gsearch3.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score3.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score3['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch3.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch3.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch3.best_score_)\n",
    "\n",
    "grid_score3['avg'] = grid_score3.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score3.loc[grid_score3.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recablirating the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065860</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.100550</td>\n",
       "      <td>0.002674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.055580</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.086942</td>\n",
       "      <td>0.000852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.048642</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.082974</td>\n",
       "      <td>0.000521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045481</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.080191</td>\n",
       "      <td>0.000978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042406</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.077423</td>\n",
       "      <td>0.000548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.040227</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.075793</td>\n",
       "      <td>0.000590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.038320</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.074948</td>\n",
       "      <td>0.001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.036789</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.073579</td>\n",
       "      <td>0.001280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.035460</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.072534</td>\n",
       "      <td>0.001455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.033980</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.071303</td>\n",
       "      <td>0.001170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.032503</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.070304</td>\n",
       "      <td>0.000748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.031269</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.070242</td>\n",
       "      <td>0.000632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.030147</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.069766</td>\n",
       "      <td>0.000669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.029005</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.069073</td>\n",
       "      <td>0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.028236</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>0.068720</td>\n",
       "      <td>0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.027375</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>0.068136</td>\n",
       "      <td>0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.026529</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.067628</td>\n",
       "      <td>0.000510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.025834</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.067290</td>\n",
       "      <td>0.000511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.024895</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.067059</td>\n",
       "      <td>0.000564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.024277</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.066506</td>\n",
       "      <td>0.000268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.023761</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.066106</td>\n",
       "      <td>0.000879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.023223</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.065552</td>\n",
       "      <td>0.000658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.022604</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.065322</td>\n",
       "      <td>0.000946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.022005</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.065506</td>\n",
       "      <td>0.000704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.021359</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.065030</td>\n",
       "      <td>0.000857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.020763</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.064799</td>\n",
       "      <td>0.000608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.020156</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.064707</td>\n",
       "      <td>0.000513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.019587</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.064368</td>\n",
       "      <td>0.000644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.019141</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.064291</td>\n",
       "      <td>0.000690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.018664</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.063968</td>\n",
       "      <td>0.000653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055219</td>\n",
       "      <td>0.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055296</td>\n",
       "      <td>0.001497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.001502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055173</td>\n",
       "      <td>0.001458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055234</td>\n",
       "      <td>0.001402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055250</td>\n",
       "      <td>0.001384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055219</td>\n",
       "      <td>0.001424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055204</td>\n",
       "      <td>0.001409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055281</td>\n",
       "      <td>0.001458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055219</td>\n",
       "      <td>0.001464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055234</td>\n",
       "      <td>0.001544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055235</td>\n",
       "      <td>0.001506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055204</td>\n",
       "      <td>0.001490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.001440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055311</td>\n",
       "      <td>0.001476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055234</td>\n",
       "      <td>0.001426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055373</td>\n",
       "      <td>0.001424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055250</td>\n",
       "      <td>0.001386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055281</td>\n",
       "      <td>0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055219</td>\n",
       "      <td>0.001329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055342</td>\n",
       "      <td>0.001465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055388</td>\n",
       "      <td>0.001497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055296</td>\n",
       "      <td>0.001514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055419</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055388</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055311</td>\n",
       "      <td>0.001480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055311</td>\n",
       "      <td>0.001440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055265</td>\n",
       "      <td>0.001486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055219</td>\n",
       "      <td>0.001477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.055034</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>327 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.065860         0.003643         0.100550        0.002674\n",
       "1            0.055580         0.000902         0.086942        0.000852\n",
       "2            0.048642         0.000916         0.082974        0.000521\n",
       "3            0.045481         0.001208         0.080191        0.000978\n",
       "4            0.042406         0.001442         0.077423        0.000548\n",
       "5            0.040227         0.000864         0.075793        0.000590\n",
       "6            0.038320         0.000581         0.074948        0.001025\n",
       "7            0.036789         0.000685         0.073579        0.001280\n",
       "8            0.035460         0.000517         0.072534        0.001455\n",
       "9            0.033980         0.000525         0.071303        0.001170\n",
       "10           0.032503         0.000516         0.070304        0.000748\n",
       "11           0.031269         0.000455         0.070242        0.000632\n",
       "12           0.030147         0.000418         0.069766        0.000669\n",
       "13           0.029005         0.000484         0.069073        0.000945\n",
       "14           0.028236         0.000453         0.068720        0.000749\n",
       "15           0.027375         0.000306         0.068136        0.000774\n",
       "16           0.026529         0.000399         0.067628        0.000510\n",
       "17           0.025834         0.000487         0.067290        0.000511\n",
       "18           0.024895         0.000487         0.067059        0.000564\n",
       "19           0.024277         0.000462         0.066506        0.000268\n",
       "20           0.023761         0.000311         0.066106        0.000879\n",
       "21           0.023223         0.000275         0.065552        0.000658\n",
       "22           0.022604         0.000405         0.065322        0.000946\n",
       "23           0.022005         0.000474         0.065506        0.000704\n",
       "24           0.021359         0.000391         0.065030        0.000857\n",
       "25           0.020763         0.000482         0.064799        0.000608\n",
       "26           0.020156         0.000437         0.064707        0.000513\n",
       "27           0.019587         0.000397         0.064368        0.000644\n",
       "28           0.019141         0.000350         0.064291        0.000690\n",
       "29           0.018664         0.000374         0.063968        0.000653\n",
       "..                ...              ...              ...             ...\n",
       "297          0.000092         0.000014         0.055219        0.001462\n",
       "298          0.000092         0.000014         0.055296        0.001497\n",
       "299          0.000092         0.000014         0.055142        0.001502\n",
       "300          0.000092         0.000014         0.055173        0.001458\n",
       "301          0.000092         0.000014         0.055234        0.001402\n",
       "302          0.000092         0.000014         0.055250        0.001384\n",
       "303          0.000092         0.000014         0.055219        0.001424\n",
       "304          0.000092         0.000014         0.055204        0.001409\n",
       "305          0.000092         0.000014         0.055281        0.001458\n",
       "306          0.000092         0.000014         0.055219        0.001464\n",
       "307          0.000092         0.000014         0.055234        0.001544\n",
       "308          0.000092         0.000014         0.055235        0.001506\n",
       "309          0.000092         0.000014         0.055204        0.001490\n",
       "310          0.000092         0.000014         0.055142        0.001440\n",
       "311          0.000092         0.000014         0.055311        0.001476\n",
       "312          0.000092         0.000014         0.055234        0.001426\n",
       "313          0.000092         0.000014         0.055373        0.001424\n",
       "314          0.000092         0.000014         0.055250        0.001386\n",
       "315          0.000092         0.000014         0.055281        0.001316\n",
       "316          0.000092         0.000014         0.055219        0.001329\n",
       "317          0.000092         0.000014         0.055342        0.001465\n",
       "318          0.000092         0.000014         0.055388        0.001497\n",
       "319          0.000092         0.000014         0.055296        0.001514\n",
       "320          0.000092         0.000014         0.055419        0.001500\n",
       "321          0.000092         0.000014         0.055388        0.001383\n",
       "322          0.000092         0.000014         0.055311        0.001480\n",
       "323          0.000092         0.000014         0.055311        0.001440\n",
       "324          0.000092         0.000014         0.055265        0.001486\n",
       "325          0.000092         0.000014         0.055219        0.001477\n",
       "326          0.000092         0.000014         0.055034        0.001383\n",
       "\n",
       "[327 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=10,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=1\n",
    "    )\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'n_estimators': 800}\n",
      "Run 0 best score:  0.9465955222044532\n",
      "Run 1 best param:  {'n_estimators': 900}\n",
      "Run 1 best score:  0.9456728994956329\n",
      "Run 2 best param:  {'n_estimators': 600}\n",
      "Run 2 best score:  0.946195719030631\n",
      "Run 3 best param:  {'n_estimators': 900}\n",
      "Run 3 best score:  0.9462879813015131\n",
      "Best params:  params               {'n_estimators': 800}\n",
      "mean_test_score_0                 0.946596\n",
      "mean_test_score_1                 0.945565\n",
      "mean_test_score_2                  0.94618\n",
      "mean_test_score_3                 0.946211\n",
      "avg                               0.946138\n",
      "Name: 7, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test_recalibrate = {\n",
    " 'n_estimators':[i for i in range(100, 1000, 100)]+[326]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score_recalibrate = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=326,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch_recalibrate = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test_recalibrate,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch_recalibrate.fit(X_train,y_train)    \n",
    "    if grid_score_recalibrate.empty:\n",
    "        grid_score_recalibrate = pd.DataFrame(gsearch_recalibrate.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score_recalibrate.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score_recalibrate['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch_recalibrate.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch_recalibrate.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch_recalibrate.best_score_)\n",
    "\n",
    "grid_score_recalibrate['avg'] = grid_score_recalibrate.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score_recalibrate.loc[grid_score_recalibrate.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'n_estimators': 880}\n",
      "Run 0 best score:  0.9466416533398943\n",
      "Run 1 best param:  {'n_estimators': 840}\n",
      "Run 1 best score:  0.9456882765407799\n",
      "Run 2 best param:  {'n_estimators': 800}\n",
      "Run 2 best score:  0.9461803419854841\n",
      "Run 3 best param:  {'n_estimators': 880}\n",
      "Run 3 best score:  0.9463033583466601\n",
      "Best params:  params               {'n_estimators': 880}\n",
      "mean_test_score_0                 0.946642\n",
      "mean_test_score_1                 0.945673\n",
      "mean_test_score_2                 0.946042\n",
      "mean_test_score_3                 0.946303\n",
      "avg                               0.946165\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test_recalibrateb = {\n",
    " 'n_estimators':[i for i in range(800, 900, 20)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score_recalibrateb = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=326,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch_recalibrateb = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test_recalibrateb,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch_recalibrateb.fit(X_train,y_train)    \n",
    "    if grid_score_recalibrateb.empty:\n",
    "        grid_score_recalibrateb = pd.DataFrame(gsearch_recalibrateb.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score_recalibrateb.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score_recalibrateb['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch_recalibrateb.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch_recalibrateb.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch_recalibrateb.best_score_)\n",
    "\n",
    "grid_score_recalibrateb['avg'] = grid_score_recalibrateb.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score_recalibrateb.loc[grid_score_recalibrateb.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 0 best score:  0.9466416533398943\n",
      "Run 1 best param:  {'colsample_bytree': 0.8, 'subsample': 0.6}\n",
      "Run 1 best score:  0.9461649649403371\n",
      "Run 2 best param:  {'colsample_bytree': 0.6, 'subsample': 0.7}\n",
      "Run 2 best score:  0.946195719030631\n",
      "Run 3 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 3 best score:  0.9463033583466601\n",
      "Best params:  params               {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "mean_test_score_0                                       0.946642\n",
      "mean_test_score_1                                       0.945673\n",
      "mean_test_score_2                                       0.946042\n",
      "mean_test_score_3                                       0.946303\n",
      "avg                                                     0.946165\n",
      "Name: 10, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score4 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=880,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test4,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch4.fit(X_train,y_train)    \n",
    "    if grid_score4.empty:\n",
    "        grid_score4 = pd.DataFrame(gsearch4.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score4.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score4['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch4.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch4.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch4.best_score_)\n",
    "\n",
    "grid_score4['avg'] = grid_score4.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score4.loc[grid_score4.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 0 best score:  0.9466416533398943\n",
      "Run 1 best param:  {'colsample_bytree': 0.75, 'subsample': 0.8}\n",
      "Run 1 best score:  0.946026571534014\n",
      "Run 2 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 2 best score:  0.946041948579161\n",
      "Run 3 best param:  {'colsample_bytree': 0.75, 'subsample': 0.75}\n",
      "Run 3 best score:  0.9464417517529832\n",
      "Best params:  params               {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "mean_test_score_0                                       0.946642\n",
      "mean_test_score_1                                       0.945673\n",
      "mean_test_score_2                                       0.946042\n",
      "mean_test_score_3                                       0.946303\n",
      "avg                                                     0.946165\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Carefully search for each neighboring 0.05\n",
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(75,90,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score5 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=880,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch5 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test5,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch5.fit(X_train,y_train)    \n",
    "    if grid_score5.empty:\n",
    "        grid_score5 = pd.DataFrame(gsearch5.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score5.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score5['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch5.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch5.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch5.best_score_)\n",
    "\n",
    "grid_score5['avg'] = grid_score5.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score5.loc[grid_score5.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'reg_alpha': 0}\n",
      "Run 0 best score:  0.9466416533398943\n",
      "Run 1 best param:  {'reg_alpha': 0.1}\n",
      "Run 1 best score:  0.945888178127691\n",
      "Run 2 best param:  {'reg_alpha': 1e-05}\n",
      "Run 2 best score:  0.9465186369787182\n",
      "Run 3 best param:  {'reg_alpha': 0}\n",
      "Run 3 best score:  0.9463033583466601\n",
      "Best params:  params               {'reg_alpha': 0}\n",
      "mean_test_score_0            0.946642\n",
      "mean_test_score_1            0.945673\n",
      "mean_test_score_2            0.946042\n",
      "mean_test_score_3            0.946303\n",
      "avg                          0.946165\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[0, 1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score6 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=880,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch6 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test6,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch6.fit(X_train,y_train)    \n",
    "    if grid_score6.empty:\n",
    "        grid_score6 = pd.DataFrame(gsearch6.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score6.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score6['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch6.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch6.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch6.best_score_)\n",
    "\n",
    "grid_score6['avg'] = grid_score6.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score6.loc[grid_score6.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'reg_alpha': 0}\n",
      "Run 0 best score:  0.9466416533398943\n",
      "Run 1 best param:  {'reg_alpha': 0.0005}\n",
      "Run 1 best score:  0.946057325624308\n",
      "Run 2 best param:  {'reg_alpha': 1e-05}\n",
      "Run 2 best score:  0.9465186369787182\n",
      "Run 3 best param:  {'reg_alpha': 0.0005}\n",
      "Run 3 best score:  0.9465186369787182\n",
      "Best params:  params               {'reg_alpha': 0}\n",
      "mean_test_score_0            0.946642\n",
      "mean_test_score_1            0.945673\n",
      "mean_test_score_2            0.946042\n",
      "mean_test_score_3            0.946303\n",
      "avg                          0.946165\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score7 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=880,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch7 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test7,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch7.fit(X_train,y_train)    \n",
    "    if grid_score7.empty:\n",
    "        grid_score7 = pd.DataFrame(gsearch7.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score7.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score7['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch7.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch7.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch7.best_score_)\n",
    "\n",
    "grid_score7['avg'] = grid_score7.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score7.loc[grid_score7.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the learning rate and tune n_estimators (remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065860</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.100550</td>\n",
       "      <td>0.002674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.056299</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.087787</td>\n",
       "      <td>0.000941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050225</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.083759</td>\n",
       "      <td>0.000791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047838</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.080837</td>\n",
       "      <td>0.001228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.046254</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.078823</td>\n",
       "      <td>0.000461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.044778</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.077454</td>\n",
       "      <td>0.001162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.044140</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.076731</td>\n",
       "      <td>0.001204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.043110</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.076224</td>\n",
       "      <td>0.001047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.042360</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.075317</td>\n",
       "      <td>0.001635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.041752</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.075132</td>\n",
       "      <td>0.001720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.041564</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.075055</td>\n",
       "      <td>0.001636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.041287</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.074625</td>\n",
       "      <td>0.001306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.040818</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.073610</td>\n",
       "      <td>0.001795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.040192</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.073610</td>\n",
       "      <td>0.001591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.040249</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.073056</td>\n",
       "      <td>0.001694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.040053</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>0.073364</td>\n",
       "      <td>0.001542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.039730</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.072764</td>\n",
       "      <td>0.002011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.039515</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.072749</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.039246</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.072211</td>\n",
       "      <td>0.001704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.039054</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.072072</td>\n",
       "      <td>0.001874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.072426</td>\n",
       "      <td>0.001627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.039127</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.072257</td>\n",
       "      <td>0.002096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.038666</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.072042</td>\n",
       "      <td>0.002164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.038696</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.071995</td>\n",
       "      <td>0.001975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.038423</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.071780</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.038235</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.071980</td>\n",
       "      <td>0.001641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.038027</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.071519</td>\n",
       "      <td>0.001741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.037974</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.071565</td>\n",
       "      <td>0.001833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.037735</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.071565</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.037505</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.071103</td>\n",
       "      <td>0.002139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.056111</td>\n",
       "      <td>0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.056003</td>\n",
       "      <td>0.000851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.056049</td>\n",
       "      <td>0.000841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.056065</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.055988</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.000759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>0.000755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.000091</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>0.000738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.056003</td>\n",
       "      <td>0.000780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.055973</td>\n",
       "      <td>0.000862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>0.000911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.056050</td>\n",
       "      <td>0.000829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.056050</td>\n",
       "      <td>0.000871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.056065</td>\n",
       "      <td>0.000829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>0.000841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.056065</td>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.056050</td>\n",
       "      <td>0.000898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.056096</td>\n",
       "      <td>0.000852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.000829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.056034</td>\n",
       "      <td>0.000824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.056019</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1073</th>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.055988</td>\n",
       "      <td>0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.056003</td>\n",
       "      <td>0.000796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.055988</td>\n",
       "      <td>0.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.055973</td>\n",
       "      <td>0.000832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.055926</td>\n",
       "      <td>0.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.055926</td>\n",
       "      <td>0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.055911</td>\n",
       "      <td>0.000795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.055865</td>\n",
       "      <td>0.000788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1081 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0             0.065860         0.003643         0.100550        0.002674\n",
       "1             0.056299         0.000789         0.087787        0.000941\n",
       "2             0.050225         0.001014         0.083759        0.000791\n",
       "3             0.047838         0.001047         0.080837        0.001228\n",
       "4             0.046254         0.001038         0.078823        0.000461\n",
       "5             0.044778         0.000658         0.077454        0.001162\n",
       "6             0.044140         0.000856         0.076731        0.001204\n",
       "7             0.043110         0.000704         0.076224        0.001047\n",
       "8             0.042360         0.000801         0.075317        0.001635\n",
       "9             0.041752         0.000963         0.075132        0.001720\n",
       "10            0.041564         0.000812         0.075055        0.001636\n",
       "11            0.041287         0.000621         0.074625        0.001306\n",
       "12            0.040818         0.000620         0.073610        0.001795\n",
       "13            0.040192         0.000434         0.073610        0.001591\n",
       "14            0.040249         0.000339         0.073056        0.001694\n",
       "15            0.040053         0.000344         0.073364        0.001542\n",
       "16            0.039730         0.000425         0.072764        0.002011\n",
       "17            0.039515         0.000210         0.072749        0.001640\n",
       "18            0.039246         0.000584         0.072211        0.001704\n",
       "19            0.039054         0.000502         0.072072        0.001874\n",
       "20            0.039100         0.000276         0.072426        0.001627\n",
       "21            0.039127         0.000314         0.072257        0.002096\n",
       "22            0.038666         0.000372         0.072042        0.002164\n",
       "23            0.038696         0.000381         0.071995        0.001975\n",
       "24            0.038423         0.000415         0.071780        0.001832\n",
       "25            0.038235         0.000338         0.071980        0.001641\n",
       "26            0.038027         0.000324         0.071519        0.001741\n",
       "27            0.037974         0.000289         0.071565        0.001833\n",
       "28            0.037735         0.000256         0.071565        0.001700\n",
       "29            0.037505         0.000312         0.071103        0.002139\n",
       "...                ...              ...              ...             ...\n",
       "1051          0.001253         0.000100         0.056111        0.000904\n",
       "1052          0.001253         0.000108         0.056003        0.000851\n",
       "1053          0.001249         0.000098         0.056049        0.000841\n",
       "1054          0.001242         0.000100         0.056065        0.000741\n",
       "1055          0.001238         0.000104         0.055988        0.000730\n",
       "1056          0.001230         0.000092         0.056019        0.000759\n",
       "1057          0.001219         0.000092         0.056034        0.000755\n",
       "1058          0.001219         0.000091         0.056034        0.000738\n",
       "1059          0.001200         0.000099         0.056003        0.000780\n",
       "1060          0.001196         0.000100         0.056019        0.000797\n",
       "1061          0.001200         0.000096         0.055973        0.000862\n",
       "1062          0.001192         0.000099         0.056034        0.000911\n",
       "1063          0.001184         0.000093         0.056050        0.000829\n",
       "1064          0.001165         0.000088         0.056050        0.000871\n",
       "1065          0.001165         0.000088         0.056065        0.000829\n",
       "1066          0.001161         0.000092         0.056034        0.000841\n",
       "1067          0.001165         0.000088         0.056065        0.000885\n",
       "1068          0.001153         0.000085         0.056050        0.000898\n",
       "1069          0.001149         0.000078         0.056096        0.000852\n",
       "1070          0.001149         0.000090         0.056019        0.000829\n",
       "1071          0.001138         0.000086         0.056034        0.000824\n",
       "1072          0.001134         0.000079         0.056019        0.000730\n",
       "1073          0.001134         0.000079         0.055988        0.000741\n",
       "1074          0.001134         0.000079         0.056003        0.000796\n",
       "1075          0.001130         0.000078         0.055988        0.000846\n",
       "1076          0.001122         0.000071         0.055973        0.000832\n",
       "1077          0.001134         0.000068         0.055926        0.000785\n",
       "1078          0.001115         0.000058         0.055926        0.000774\n",
       "1079          0.001115         0.000058         0.055911        0.000795\n",
       "1080          0.001111         0.000052         0.055865        0.000788\n",
       "\n",
       "[1081 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=880,\n",
    "    max_depth=10,\n",
    "    min_child_weight=1,\n",
    "    gamma=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'n_estimators': 2000}\n",
      "Run 0 best score:  0.9453653585926928\n",
      "Run 1 best param:  {'n_estimators': 2000}\n",
      "Run 1 best score:  0.9449348013285767\n",
      "Run 2 best param:  {'n_estimators': 1900}\n",
      "Run 2 best score:  0.9450578176897527\n",
      "Run 3 best param:  {'n_estimators': 2000}\n",
      "Run 3 best score:  0.9453653585926928\n",
      "Best params:  params               {'n_estimators': 2000}\n",
      "mean_test_score_0                  0.945365\n",
      "mean_test_score_1                  0.944935\n",
      "mean_test_score_2                  0.944996\n",
      "mean_test_score_3                  0.945365\n",
      "avg                                0.945165\n",
      "Name: 10, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(1000, 2100, 100)]+[1080]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=1080,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'n_estimators': 3000}\n",
      "Run 0 best score:  0.945949686308279\n",
      "Run 1 best param:  {'n_estimators': 2900}\n",
      "Run 1 best score:  0.9456728994956329\n",
      "Run 2 best param:  {'n_estimators': 2800}\n",
      "Run 2 best score:  0.945872801082544\n",
      "Run 3 best param:  {'n_estimators': 3000}\n",
      "Run 3 best score:  0.9462264731209251\n",
      "Best params:  params               {'n_estimators': 3000}\n",
      "mean_test_score_0                   0.94595\n",
      "mean_test_score_1                  0.945658\n",
      "mean_test_score_2                  0.945873\n",
      "mean_test_score_3                  0.946226\n",
      "avg                                0.945927\n",
      "Name: 10, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8b = {\n",
    " 'n_estimators':[i for i in range(2000, 3100, 100)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8b = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=1080,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8b = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8b,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8b.fit(X_train,y_train)    \n",
    "    if grid_score8b.empty:\n",
    "        grid_score8b = pd.DataFrame(gsearch8b.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8b.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8b['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8b.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8b.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8b.best_score_)\n",
    "\n",
    "grid_score8b['avg'] = grid_score8b.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8b.loc[grid_score8b.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0: 94.75%\n",
      "Accuracy 1: 94.66%\n",
      "Accuracy 2: 94.75%\n",
      "Accuracy 3: 94.69%\n",
      "Average accuracy is: 94.71%\n"
     ]
    }
   ],
   "source": [
    "accuracy_array = []\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=880,\n",
    "        max_depth=10,\n",
    "        min_child_weight=1,\n",
    "        gamma=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=i\n",
    "    )\n",
    "    model = xgb.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_array.append(accuracy)\n",
    "    print('Accuracy {}: %.2f%%'.format(i) % (accuracy * 100.0))\n",
    "mean_accuracy_score = sum(accuracy_array) / NUM_TRIALS\n",
    "print('Average accuracy is: %.2f%%' % (mean_accuracy_score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
