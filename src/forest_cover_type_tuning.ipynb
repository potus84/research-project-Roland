{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest cover type dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xgboost\n",
    "import copy\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/tai/Projects/research-project-Roland')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/forest_cover_type/forest_cover_type.0.train\", encoding='latin1', \n",
    "                 header=None,\n",
    "                 na_values='?',\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/forest_cover_type/forest_cover_type.0.test\", encoding='latin1', \n",
    "                 header=None,\n",
    "                 na_values='?',\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, :-1]\n",
    "y_train = train.iloc[:, -1:]\n",
    "\n",
    "X_test = test.iloc[:, :-1]\n",
    "y_test = test.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290506, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290506, 54)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290506, 54)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "            51, 52, 53],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "            17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "            34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "            51, 52, 53],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tuning on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1500,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'multi:softmax',\n",
    "    num_class = 7,\n",
    "    n_jobs=-1)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=1500, nfold=5, metrics=['merror'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fea613f66e20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                             \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfive_folds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                             return_train_score=False)\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrid_score1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mgrid_score1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    853\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    537\u001b[0m         \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 \u001b[0mqmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mcq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1 = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1,300,50)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1500,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1.fit(X_train,y_train)    \n",
    "    if grid_score1.empty:\n",
    "        grid_score1 = pd.DataFrame(gsearch1.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1.best_score_)\n",
    "\n",
    "grid_score1['avg'] = grid_score1.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1.loc[grid_score1.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'max_depth': 9, 'min_child_weight': 1}\n",
      "Run 0 best score:  0.9589543761574632\n",
      "Best params:  params               {'max_depth': 9, 'min_child_weight': 1}\n",
      "mean_test_score_0                                   0.958954\n",
      "avg                                                 0.958954\n",
      "Name: 20, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1b = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1, 10, 2)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1b = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1500,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1b = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1b,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1b.fit(X_train,y_train)    \n",
    "    if grid_score1b.empty:\n",
    "        grid_score1b = pd.DataFrame(gsearch1b.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1b.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1b['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1b.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1b.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1b.best_score_)\n",
    "\n",
    "grid_score1b['avg'] = grid_score1b.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1b.loc[grid_score1b.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'max_depth': 10, 'min_child_weight': 1}\n",
      "Run 0 best score:  0.9605791274534777\n",
      "Best params:  params               {'max_depth': 10, 'min_child_weight': 1}\n",
      "mean_test_score_0                                    0.960579\n",
      "avg                                                  0.960579\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look carefully again the neigbor values\n",
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test2 = {\n",
    " 'max_depth':[9,10],\n",
    " 'min_child_weight':[1,2]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score2 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1500,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=8,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test2,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch2.fit(X_train,y_train)    \n",
    "    if grid_score2.empty:\n",
    "        grid_score2 = pd.DataFrame(gsearch2.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score2.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score2['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch2.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch2.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch2.best_score_)\n",
    "\n",
    "grid_score2['avg'] = grid_score2.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score2.loc[grid_score2.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'max_depth': 25}\n",
      "Run 0 best score:  0.9632262328488912\n",
      "Best params:  params               {'max_depth': 25}\n",
      "mean_test_score_0             0.963226\n",
      "avg                           0.963226\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look carefully again the neigbor values\n",
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test2 = {\n",
    " 'max_depth':[i for i in range(10, 31, 5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score2 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1500,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test2,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch2.fit(X_train,y_train)    \n",
    "    if grid_score2.empty:\n",
    "        grid_score2 = pd.DataFrame(gsearch2.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score2.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score2['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch2.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch2.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch2.best_score_)\n",
    "\n",
    "grid_score2['avg'] = grid_score2.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score2.loc[grid_score2.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'max_depth': 21}\n",
      "Run 0 best score:  0.9633467122882143\n",
      "Best params:  params               {'max_depth': 21}\n",
      "mean_test_score_0             0.963347\n",
      "avg                           0.963347\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look carefully again the neigbor values\n",
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test2 = {\n",
    " 'max_depth':[i for i in range(21, 30, 2)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score2 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1500,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test2,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch2.fit(X_train,y_train)    \n",
    "    if grid_score2.empty:\n",
    "        grid_score2 = pd.DataFrame(gsearch2.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score2.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score2['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch2.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch2.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch2.best_score_)\n",
    "\n",
    "grid_score2['avg'] = grid_score2.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score2.loc[grid_score2.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'gamma': 0.0}\n",
      "Run 0 best score:  0.9633467122882143\n",
      "Best params:  params               {'gamma': 0.0}\n",
      "mean_test_score_0          0.963347\n",
      "avg                        0.963347\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score3 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1500,\n",
    "        max_depth=21,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch3 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test3,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch3.fit(X_train,y_train)    \n",
    "    if grid_score3.empty:\n",
    "        grid_score3 = pd.DataFrame(gsearch3.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score3.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score3['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch3.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch3.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch3.best_score_)\n",
    "\n",
    "grid_score3['avg'] = grid_score3.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score3.loc[grid_score3.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recablirating the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-merror-mean</th>\n",
       "      <th>train-merror-std</th>\n",
       "      <th>test-merror-mean</th>\n",
       "      <th>test-merror-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.111291</td>\n",
       "      <td>0.048555</td>\n",
       "      <td>0.150292</td>\n",
       "      <td>0.045942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062210</td>\n",
       "      <td>0.016586</td>\n",
       "      <td>0.103357</td>\n",
       "      <td>0.019812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046866</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.087021</td>\n",
       "      <td>0.007397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.041008</td>\n",
       "      <td>0.004079</td>\n",
       "      <td>0.082236</td>\n",
       "      <td>0.005999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037238</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>0.078780</td>\n",
       "      <td>0.005097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.034020</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.075362</td>\n",
       "      <td>0.003202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.031005</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.072731</td>\n",
       "      <td>0.002676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.028196</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.070560</td>\n",
       "      <td>0.003184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.026187</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.068907</td>\n",
       "      <td>0.003232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.024521</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.067517</td>\n",
       "      <td>0.003063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.066109</td>\n",
       "      <td>0.002627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.021026</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.064663</td>\n",
       "      <td>0.002506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.019652</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.063634</td>\n",
       "      <td>0.002269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.062481</td>\n",
       "      <td>0.002424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.016802</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.061493</td>\n",
       "      <td>0.002110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.015481</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.060130</td>\n",
       "      <td>0.001864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.014293</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.059169</td>\n",
       "      <td>0.002098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.013278</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.058243</td>\n",
       "      <td>0.002062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.012156</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.057379</td>\n",
       "      <td>0.001384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.011252</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.056660</td>\n",
       "      <td>0.001381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.010248</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.055644</td>\n",
       "      <td>0.001372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.009630</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.055142</td>\n",
       "      <td>0.001268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.008924</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.054553</td>\n",
       "      <td>0.001123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.008297</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.054099</td>\n",
       "      <td>0.000994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.007724</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.053579</td>\n",
       "      <td>0.000942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.007106</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.052952</td>\n",
       "      <td>0.000932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.006481</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.052230</td>\n",
       "      <td>0.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.005990</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.051751</td>\n",
       "      <td>0.000925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.005510</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.051190</td>\n",
       "      <td>0.000919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.005093</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.050839</td>\n",
       "      <td>0.000875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036808</td>\n",
       "      <td>0.000750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036733</td>\n",
       "      <td>0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036749</td>\n",
       "      <td>0.000760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036732</td>\n",
       "      <td>0.000733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036774</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036736</td>\n",
       "      <td>0.000775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036753</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036743</td>\n",
       "      <td>0.000736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036798</td>\n",
       "      <td>0.000768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036812</td>\n",
       "      <td>0.000781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036798</td>\n",
       "      <td>0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036774</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036753</td>\n",
       "      <td>0.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036722</td>\n",
       "      <td>0.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036750</td>\n",
       "      <td>0.000789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036740</td>\n",
       "      <td>0.000721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036781</td>\n",
       "      <td>0.000795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036750</td>\n",
       "      <td>0.000785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036746</td>\n",
       "      <td>0.000820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036770</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036733</td>\n",
       "      <td>0.000780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036732</td>\n",
       "      <td>0.000821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036736</td>\n",
       "      <td>0.000808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036719</td>\n",
       "      <td>0.000832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036750</td>\n",
       "      <td>0.000801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036750</td>\n",
       "      <td>0.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036753</td>\n",
       "      <td>0.000777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036715</td>\n",
       "      <td>0.000784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036712</td>\n",
       "      <td>0.000767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036702</td>\n",
       "      <td>0.000776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-merror-mean  train-merror-std  test-merror-mean  test-merror-std\n",
       "0             0.111291          0.048555          0.150292         0.045942\n",
       "1             0.062210          0.016586          0.103357         0.019812\n",
       "2             0.046866          0.005631          0.087021         0.007397\n",
       "3             0.041008          0.004079          0.082236         0.005999\n",
       "4             0.037238          0.003157          0.078780         0.005097\n",
       "5             0.034020          0.001986          0.075362         0.003202\n",
       "6             0.031005          0.000943          0.072731         0.002676\n",
       "7             0.028196          0.001126          0.070560         0.003184\n",
       "8             0.026187          0.001263          0.068907         0.003232\n",
       "9             0.024521          0.001185          0.067517         0.003063\n",
       "10            0.022920          0.000915          0.066109         0.002627\n",
       "11            0.021026          0.001010          0.064663         0.002506\n",
       "12            0.019652          0.001016          0.063634         0.002269\n",
       "13            0.018265          0.001044          0.062481         0.002424\n",
       "14            0.016802          0.000675          0.061493         0.002110\n",
       "15            0.015481          0.000847          0.060130         0.001864\n",
       "16            0.014293          0.000850          0.059169         0.002098\n",
       "17            0.013278          0.000619          0.058243         0.002062\n",
       "18            0.012156          0.000513          0.057379         0.001384\n",
       "19            0.011252          0.000418          0.056660         0.001381\n",
       "20            0.010248          0.000290          0.055644         0.001372\n",
       "21            0.009630          0.000371          0.055142         0.001268\n",
       "22            0.008924          0.000292          0.054553         0.001123\n",
       "23            0.008297          0.000286          0.054099         0.000994\n",
       "24            0.007724          0.000279          0.053579         0.000942\n",
       "25            0.007106          0.000391          0.052952         0.000932\n",
       "26            0.006481          0.000390          0.052230         0.000846\n",
       "27            0.005990          0.000339          0.051751         0.000925\n",
       "28            0.005510          0.000423          0.051190         0.000919\n",
       "29            0.005093          0.000451          0.050839         0.000875\n",
       "..                 ...               ...               ...              ...\n",
       "372           0.000000          0.000000          0.036808         0.000750\n",
       "373           0.000000          0.000000          0.036733         0.000764\n",
       "374           0.000000          0.000000          0.036749         0.000760\n",
       "375           0.000000          0.000000          0.036732         0.000733\n",
       "376           0.000000          0.000000          0.036774         0.000745\n",
       "377           0.000000          0.000000          0.036736         0.000775\n",
       "378           0.000000          0.000000          0.036753         0.000752\n",
       "379           0.000000          0.000000          0.036743         0.000736\n",
       "380           0.000000          0.000000          0.036798         0.000768\n",
       "381           0.000000          0.000000          0.036812         0.000781\n",
       "382           0.000000          0.000000          0.036798         0.000749\n",
       "383           0.000000          0.000000          0.036774         0.000762\n",
       "384           0.000000          0.000000          0.036753         0.000779\n",
       "385           0.000000          0.000000          0.036722         0.000779\n",
       "386           0.000000          0.000000          0.036750         0.000789\n",
       "387           0.000000          0.000000          0.036740         0.000721\n",
       "388           0.000000          0.000000          0.036781         0.000795\n",
       "389           0.000000          0.000000          0.036750         0.000785\n",
       "390           0.000000          0.000000          0.036746         0.000820\n",
       "391           0.000000          0.000000          0.036770         0.000800\n",
       "392           0.000000          0.000000          0.036733         0.000780\n",
       "393           0.000000          0.000000          0.036732         0.000821\n",
       "394           0.000000          0.000000          0.036736         0.000808\n",
       "395           0.000000          0.000000          0.036719         0.000832\n",
       "396           0.000000          0.000000          0.036750         0.000801\n",
       "397           0.000000          0.000000          0.036750         0.000793\n",
       "398           0.000000          0.000000          0.036753         0.000777\n",
       "399           0.000000          0.000000          0.036715         0.000784\n",
       "400           0.000000          0.000000          0.036712         0.000767\n",
       "401           0.000000          0.000000          0.036702         0.000776\n",
       "\n",
       "[402 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1500,\n",
    "    max_depth=21,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'multi:softmax',\n",
    "    num_class = 7,\n",
    "    n_jobs=-1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=1500, nfold=5, metrics=['merror'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'n_estimators': 700}\n",
      "Run 0 best score:  0.9633845772548587\n",
      "Best params:  params               {'n_estimators': 700}\n",
      "mean_test_score_0                 0.963385\n",
      "avg                               0.963385\n",
      "Name: 6, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'n_estimators':[i for i in range(100, 1500, 100)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score4 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=401,\n",
    "        max_depth=21,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test4,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch4.fit(X_train,y_train)    \n",
    "    if grid_score4.empty:\n",
    "        grid_score4 = pd.DataFrame(gsearch4.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score4.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score4['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch4.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch4.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch4.best_score_)\n",
    "\n",
    "grid_score4['avg'] = grid_score4.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score4.loc[grid_score4.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'n_estimators': 690}\n",
      "Run 0 best score:  0.9633949040639436\n",
      "Best params:  params               {'n_estimators': 690}\n",
      "mean_test_score_0                 0.963395\n",
      "avg                               0.963395\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'n_estimators':[i for i in range(650, 751, 10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score4 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=401,\n",
    "        max_depth=21,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test4,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch4.fit(X_train,y_train)    \n",
    "    if grid_score4.empty:\n",
    "        grid_score4 = pd.DataFrame(gsearch4.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score4.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score4['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch4.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch4.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch4.best_score_)\n",
    "\n",
    "grid_score4['avg'] = grid_score4.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score4.loc[grid_score4.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "Run 0 best score:  0.9644379117815123\n",
      "Best params:  params               {'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "mean_test_score_0                                       0.964438\n",
      "avg                                                     0.964438\n",
      "Name: 15, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score4 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=690,\n",
    "        max_depth=21,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test4,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch4.fit(X_train,y_train)    \n",
    "    if grid_score4.empty:\n",
    "        grid_score4 = pd.DataFrame(gsearch4.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score4.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score4['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch4.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch4.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch4.best_score_)\n",
    "\n",
    "grid_score4['avg'] = grid_score4.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score4.loc[grid_score4.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "Run 0 best score:  0.9644379117815123\n",
      "Best params:  params               {'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "mean_test_score_0                                       0.964438\n",
      "avg                                                     0.964438\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Carefully search for each neighboring 0.05\n",
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(85,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(85,100,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score5 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=690,\n",
    "        max_depth=21,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch5 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test5,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch5.fit(X_train,y_train)    \n",
    "    if grid_score5.empty:\n",
    "        grid_score5 = pd.DataFrame(gsearch5.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score5.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score5['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch5.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch5.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch5.best_score_)\n",
    "\n",
    "grid_score5['avg'] = grid_score5.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score5.loc[grid_score5.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'reg_alpha': 0.01}\n",
      "Run 0 best score:  0.9644654499390718\n",
      "Best params:  params               {'reg_alpha': 0.01}\n",
      "mean_test_score_0               0.964465\n",
      "avg                             0.964465\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[0, 1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score6 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=690,\n",
    "        max_depth=21,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch6 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test6,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch6.fit(X_train,y_train)    \n",
    "    if grid_score6.empty:\n",
    "        grid_score6 = pd.DataFrame(gsearch6.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score6.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score6['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch6.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch6.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch6.best_score_)\n",
    "\n",
    "grid_score6['avg'] = grid_score6.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score6.loc[grid_score6.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'reg_alpha': 0.01}\n",
      "Run 0 best score:  0.9644654499390718\n",
      "Best params:  params               {'reg_alpha': 0.01}\n",
      "mean_test_score_0               0.964465\n",
      "avg                             0.964465\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[1e-2, 1e-3, 1e-5]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score7 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=690,\n",
    "        max_depth=21,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch7 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test7,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch7.fit(X_train,y_train)    \n",
    "    if grid_score7.empty:\n",
    "        grid_score7 = pd.DataFrame(gsearch7.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score7.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score7['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch7.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch7.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch7.best_score_)\n",
    "\n",
    "grid_score7['avg'] = grid_score7.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score7.loc[grid_score7.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the learning rate and tune n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-merror-mean</th>\n",
       "      <th>train-merror-std</th>\n",
       "      <th>test-merror-mean</th>\n",
       "      <th>test-merror-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061344</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>0.101605</td>\n",
       "      <td>0.004574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049829</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>0.088680</td>\n",
       "      <td>0.007908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.043347</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>0.081640</td>\n",
       "      <td>0.003507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.041095</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.079561</td>\n",
       "      <td>0.002911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.038873</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.077100</td>\n",
       "      <td>0.001977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.037361</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.075417</td>\n",
       "      <td>0.001740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.036806</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.075059</td>\n",
       "      <td>0.001576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.036010</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.074267</td>\n",
       "      <td>0.001167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.035320</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.073492</td>\n",
       "      <td>0.001376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.034793</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.073141</td>\n",
       "      <td>0.001457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.034072</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.072177</td>\n",
       "      <td>0.001406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.033355</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.071596</td>\n",
       "      <td>0.001224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.032881</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.070880</td>\n",
       "      <td>0.001129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.032406</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.070522</td>\n",
       "      <td>0.000793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.031995</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.070329</td>\n",
       "      <td>0.001138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.031565</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.069796</td>\n",
       "      <td>0.001030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.031211</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.069710</td>\n",
       "      <td>0.000954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.030927</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.069448</td>\n",
       "      <td>0.001053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.030454</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.069031</td>\n",
       "      <td>0.000883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>0.068883</td>\n",
       "      <td>0.001002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.029767</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.068722</td>\n",
       "      <td>0.001304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.029571</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.068508</td>\n",
       "      <td>0.001220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.029320</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.068264</td>\n",
       "      <td>0.001088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.068026</td>\n",
       "      <td>0.000874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.028876</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.067930</td>\n",
       "      <td>0.000846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.028627</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.067654</td>\n",
       "      <td>0.000942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.028294</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.067386</td>\n",
       "      <td>0.000813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.027999</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>0.000795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.027667</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.067093</td>\n",
       "      <td>0.000809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.027490</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.067028</td>\n",
       "      <td>0.000960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036116</td>\n",
       "      <td>0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036140</td>\n",
       "      <td>0.000775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036140</td>\n",
       "      <td>0.000769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036137</td>\n",
       "      <td>0.000766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036140</td>\n",
       "      <td>0.000768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.000756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036137</td>\n",
       "      <td>0.000766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036140</td>\n",
       "      <td>0.000759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036130</td>\n",
       "      <td>0.000765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036116</td>\n",
       "      <td>0.000759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036123</td>\n",
       "      <td>0.000761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036130</td>\n",
       "      <td>0.000765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036123</td>\n",
       "      <td>0.000761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036120</td>\n",
       "      <td>0.000762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036116</td>\n",
       "      <td>0.000777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036106</td>\n",
       "      <td>0.000771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036092</td>\n",
       "      <td>0.000771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036092</td>\n",
       "      <td>0.000757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>0.000766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036092</td>\n",
       "      <td>0.000755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>0.000763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036092</td>\n",
       "      <td>0.000770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036089</td>\n",
       "      <td>0.000751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036092</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>0.000751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036085</td>\n",
       "      <td>0.000748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036089</td>\n",
       "      <td>0.000758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036085</td>\n",
       "      <td>0.000765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036075</td>\n",
       "      <td>0.000760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      train-merror-mean  train-merror-std  test-merror-mean  test-merror-std\n",
       "0              0.061344          0.004885          0.101605         0.004574\n",
       "1              0.049829          0.006886          0.088680         0.007908\n",
       "2              0.043347          0.003636          0.081640         0.003507\n",
       "3              0.041095          0.002911          0.079561         0.002911\n",
       "4              0.038873          0.001870          0.077100         0.001977\n",
       "5              0.037361          0.001417          0.075417         0.001740\n",
       "6              0.036806          0.000994          0.075059         0.001576\n",
       "7              0.036010          0.000858          0.074267         0.001167\n",
       "8              0.035320          0.000703          0.073492         0.001376\n",
       "9              0.034793          0.000716          0.073141         0.001457\n",
       "10             0.034072          0.000814          0.072177         0.001406\n",
       "11             0.033355          0.000812          0.071596         0.001224\n",
       "12             0.032881          0.000989          0.070880         0.001129\n",
       "13             0.032406          0.000682          0.070522         0.000793\n",
       "14             0.031995          0.000637          0.070329         0.001138\n",
       "15             0.031565          0.000700          0.069796         0.001030\n",
       "16             0.031211          0.000648          0.069710         0.000954\n",
       "17             0.030927          0.000681          0.069448         0.001053\n",
       "18             0.030454          0.000671          0.069031         0.000883\n",
       "19             0.030119          0.000716          0.068883         0.001002\n",
       "20             0.029767          0.000736          0.068722         0.001304\n",
       "21             0.029571          0.000824          0.068508         0.001220\n",
       "22             0.029320          0.000812          0.068264         0.001088\n",
       "23             0.029100          0.000693          0.068026         0.000874\n",
       "24             0.028876          0.000722          0.067930         0.000846\n",
       "25             0.028627          0.000666          0.067654         0.000942\n",
       "26             0.028294          0.000593          0.067386         0.000813\n",
       "27             0.027999          0.000616          0.067159         0.000795\n",
       "28             0.027667          0.000592          0.067093         0.000809\n",
       "29             0.027490          0.000631          0.067028         0.000960\n",
       "...                 ...               ...               ...              ...\n",
       "2070           0.000000          0.000000          0.036116         0.000774\n",
       "2071           0.000000          0.000000          0.036140         0.000775\n",
       "2072           0.000000          0.000000          0.036140         0.000769\n",
       "2073           0.000000          0.000000          0.036137         0.000766\n",
       "2074           0.000000          0.000000          0.036140         0.000768\n",
       "2075           0.000000          0.000000          0.036133         0.000757\n",
       "2076           0.000000          0.000000          0.036133         0.000756\n",
       "2077           0.000000          0.000000          0.036137         0.000766\n",
       "2078           0.000000          0.000000          0.036140         0.000759\n",
       "2079           0.000000          0.000000          0.036130         0.000765\n",
       "2080           0.000000          0.000000          0.036116         0.000759\n",
       "2081           0.000000          0.000000          0.036123         0.000761\n",
       "2082           0.000000          0.000000          0.036130         0.000765\n",
       "2083           0.000000          0.000000          0.036123         0.000761\n",
       "2084           0.000000          0.000000          0.036120         0.000762\n",
       "2085           0.000000          0.000000          0.036116         0.000777\n",
       "2086           0.000000          0.000000          0.036106         0.000771\n",
       "2087           0.000000          0.000000          0.036092         0.000771\n",
       "2088           0.000000          0.000000          0.036092         0.000757\n",
       "2089           0.000000          0.000000          0.036096         0.000766\n",
       "2090           0.000000          0.000000          0.036092         0.000755\n",
       "2091           0.000000          0.000000          0.036096         0.000763\n",
       "2092           0.000000          0.000000          0.036092         0.000770\n",
       "2093           0.000000          0.000000          0.036089         0.000751\n",
       "2094           0.000000          0.000000          0.036092         0.000753\n",
       "2095           0.000000          0.000000          0.036096         0.000751\n",
       "2096           0.000000          0.000000          0.036085         0.000748\n",
       "2097           0.000000          0.000000          0.036089         0.000758\n",
       "2098           0.000000          0.000000          0.036085         0.000765\n",
       "2099           0.000000          0.000000          0.036075         0.000760\n",
       "\n",
       "[2100 rows x 4 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=690,\n",
    "    max_depth=21,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    objective= 'multi:softmax',\n",
    "    num_class = 7,\n",
    "    reg_alpha = 0.01,\n",
    "    n_jobs=-1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['merror'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0 best param:  {'n_estimators': 2400}\n",
      "Run 0 best score:  0.9640248394181188\n",
      "Best params:  params               {'n_estimators': 2400}\n",
      "mean_test_score_0                  0.964025\n",
      "avg                                0.964025\n",
      "Name: 9, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(1500, 2500, 100)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=690,\n",
    "        max_depth=21,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        reg_alpha = 0.01,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(1400, 1500, 20)]+[1423]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=690,\n",
    "        max_depth=21,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        reg_alpha = 0.01,\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0: 96.88%\n",
      "Average accuracy is: 96.88%\n"
     ]
    }
   ],
   "source": [
    "accuracy_array = []\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=690,\n",
    "        max_depth=21,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 7,\n",
    "        n_jobs=-1,\n",
    "        reg_alpha=0.01,\n",
    "        seed=i\n",
    "    )\n",
    "    model = xgb.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_array.append(accuracy)\n",
    "    print('Accuracy {}: %.2f%%'.format(i) % (accuracy * 100.0))\n",
    "mean_accuracy_score = sum(accuracy_array) / NUM_TRIALS\n",
    "print('Average accuracy is: %.2f%%' % (mean_accuracy_score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
