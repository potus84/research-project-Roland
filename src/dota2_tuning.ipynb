{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dota2 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xgboost\n",
    "import copy\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/tai/Projects/research-project-Roland')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/dota2/dota2.0.train\", encoding='latin1', \n",
    "                 header=None,\n",
    "                 na_values='?',\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/dota2/dota2.0.test\", encoding='latin1', \n",
    "                 header=None,\n",
    "                 na_values='?',\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "5      0\n",
       "6      0\n",
       "7      0\n",
       "8      0\n",
       "9      0\n",
       "10     0\n",
       "11     0\n",
       "12     0\n",
       "13     0\n",
       "14     0\n",
       "15     0\n",
       "16     0\n",
       "17     0\n",
       "18     0\n",
       "19     0\n",
       "20     0\n",
       "21     0\n",
       "22     0\n",
       "23     0\n",
       "24     0\n",
       "25     0\n",
       "26     0\n",
       "27     0\n",
       "28     0\n",
       "29     0\n",
       "      ..\n",
       "87     0\n",
       "88     0\n",
       "89     0\n",
       "90     0\n",
       "91     0\n",
       "92     0\n",
       "93     0\n",
       "94     0\n",
       "95     0\n",
       "96     0\n",
       "97     0\n",
       "98     0\n",
       "99     0\n",
       "100    0\n",
       "101    0\n",
       "102    0\n",
       "103    0\n",
       "104    0\n",
       "105    0\n",
       "106    0\n",
       "107    0\n",
       "108    0\n",
       "109    0\n",
       "110    0\n",
       "111    0\n",
       "112    0\n",
       "113    0\n",
       "114    0\n",
       "115    0\n",
       "116    0\n",
       "Length: 117, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "5      0\n",
       "6      0\n",
       "7      0\n",
       "8      0\n",
       "9      0\n",
       "10     0\n",
       "11     0\n",
       "12     0\n",
       "13     0\n",
       "14     0\n",
       "15     0\n",
       "16     0\n",
       "17     0\n",
       "18     0\n",
       "19     0\n",
       "20     0\n",
       "21     0\n",
       "22     0\n",
       "23     0\n",
       "24     0\n",
       "25     0\n",
       "26     0\n",
       "27     0\n",
       "28     0\n",
       "29     0\n",
       "      ..\n",
       "87     0\n",
       "88     0\n",
       "89     0\n",
       "90     0\n",
       "91     0\n",
       "92     0\n",
       "93     0\n",
       "94     0\n",
       "95     0\n",
       "96     0\n",
       "97     0\n",
       "98     0\n",
       "99     0\n",
       "100    0\n",
       "101    0\n",
       "102    0\n",
       "103    0\n",
       "104    0\n",
       "105    0\n",
       "106    0\n",
       "107    0\n",
       "108    0\n",
       "109    0\n",
       "110    0\n",
       "111    0\n",
       "112    0\n",
       "113    0\n",
       "114    0\n",
       "115    0\n",
       "116    0\n",
       "Length: 117, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.iloc[:, 0]\n",
    "X_train = train.iloc[:, 2:]\n",
    "\n",
    "\n",
    "y_test = test.iloc[:, 0]\n",
    "X_test = test.iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.00000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.0</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "      <td>51472.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.316347</td>\n",
       "      <td>2.387162</td>\n",
       "      <td>-0.002409</td>\n",
       "      <td>-0.000291</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>-0.00033</td>\n",
       "      <td>-0.002681</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>-0.001088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001535</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>-0.001107</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.633338</td>\n",
       "      <td>0.487345</td>\n",
       "      <td>0.402043</td>\n",
       "      <td>0.465119</td>\n",
       "      <td>0.164272</td>\n",
       "      <td>0.35501</td>\n",
       "      <td>0.331891</td>\n",
       "      <td>0.483905</td>\n",
       "      <td>0.350355</td>\n",
       "      <td>0.502716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534865</td>\n",
       "      <td>0.205801</td>\n",
       "      <td>0.283297</td>\n",
       "      <td>0.154335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.220562</td>\n",
       "      <td>0.205422</td>\n",
       "      <td>0.170310</td>\n",
       "      <td>0.191674</td>\n",
       "      <td>0.138336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 115 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                2             3             4             5             6    \\\n",
       "count  51472.000000  51472.000000  51472.000000  51472.000000  51472.000000   \n",
       "mean       3.316347      2.387162     -0.002409     -0.000291      0.000952   \n",
       "std        2.633338      0.487345      0.402043      0.465119      0.164272   \n",
       "min        1.000000      1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%        2.000000      2.000000      0.000000      0.000000      0.000000   \n",
       "50%        2.000000      2.000000      0.000000      0.000000      0.000000   \n",
       "75%        2.000000      3.000000      0.000000      0.000000      0.000000   \n",
       "max        9.000000      3.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "               7             8             9             10            11   \\\n",
       "count  51472.00000  51472.000000  51472.000000  51472.000000  51472.000000   \n",
       "mean      -0.00033     -0.002681      0.002545     -0.000039     -0.001088   \n",
       "std        0.35501      0.331891      0.483905      0.350355      0.502716   \n",
       "min       -1.00000     -1.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.00000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.00000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...           107           108           109           110      111  \\\n",
       "count  ...  51472.000000  51472.000000  51472.000000  51472.000000  51472.0   \n",
       "mean   ...     -0.001535      0.000311      0.001146      0.000117      0.0   \n",
       "std    ...      0.534865      0.205801      0.283297      0.154335      0.0   \n",
       "min    ...     -1.000000     -1.000000     -1.000000     -1.000000      0.0   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000      0.0   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000      0.0   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000      0.0   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000      0.0   \n",
       "\n",
       "                112           113           114           115           116  \n",
       "count  51472.000000  51472.000000  51472.000000  51472.000000  51472.000000  \n",
       "mean       0.001010      0.000350     -0.001107      0.000214      0.000369  \n",
       "std        0.220562      0.205422      0.170310      0.191674      0.138336  \n",
       "min       -1.000000     -1.000000     -1.000000     -1.000000     -1.000000  \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000  \n",
       "\n",
       "[8 rows x 115 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[(y_train==-1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[(y_test==-1)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onehot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, columns=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.get_dummies(X_test, columns=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.iloc[:, 0:113] = X_train.iloc[:, 0:113].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[:, 0:113] = X_test.iloc[:, 0:113].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.get_dummies(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51472, 347)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tuning on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.456000</td>\n",
       "      <td>0.006321</td>\n",
       "      <td>0.466312</td>\n",
       "      <td>0.001944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.447083</td>\n",
       "      <td>0.004315</td>\n",
       "      <td>0.457803</td>\n",
       "      <td>0.002286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.444509</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>0.454791</td>\n",
       "      <td>0.003364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.439793</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>0.450459</td>\n",
       "      <td>0.003843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.437063</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>0.450905</td>\n",
       "      <td>0.002244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.434644</td>\n",
       "      <td>0.003702</td>\n",
       "      <td>0.448768</td>\n",
       "      <td>0.006003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.433017</td>\n",
       "      <td>0.003748</td>\n",
       "      <td>0.449137</td>\n",
       "      <td>0.004384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.429986</td>\n",
       "      <td>0.003041</td>\n",
       "      <td>0.446165</td>\n",
       "      <td>0.004118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.428160</td>\n",
       "      <td>0.002557</td>\n",
       "      <td>0.443076</td>\n",
       "      <td>0.003589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.425255</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>0.441716</td>\n",
       "      <td>0.004589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.423585</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.440706</td>\n",
       "      <td>0.003903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.421710</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.440881</td>\n",
       "      <td>0.004586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.419563</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.438841</td>\n",
       "      <td>0.005604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.418276</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.437228</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.417688</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.436587</td>\n",
       "      <td>0.004444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.416114</td>\n",
       "      <td>0.001804</td>\n",
       "      <td>0.436975</td>\n",
       "      <td>0.004775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.413904</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.436723</td>\n",
       "      <td>0.005258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.413302</td>\n",
       "      <td>0.001790</td>\n",
       "      <td>0.436082</td>\n",
       "      <td>0.005816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.413166</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.436043</td>\n",
       "      <td>0.004673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.411461</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.435421</td>\n",
       "      <td>0.005095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.409417</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.433867</td>\n",
       "      <td>0.004850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.408329</td>\n",
       "      <td>0.001111</td>\n",
       "      <td>0.434722</td>\n",
       "      <td>0.004308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.406745</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>0.433847</td>\n",
       "      <td>0.003951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.405361</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.432895</td>\n",
       "      <td>0.004309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.403943</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.431691</td>\n",
       "      <td>0.003807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.403054</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.431652</td>\n",
       "      <td>0.004577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.402039</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.432565</td>\n",
       "      <td>0.004452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.400587</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.431380</td>\n",
       "      <td>0.004370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.399669</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.430467</td>\n",
       "      <td>0.004021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.398241</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.429573</td>\n",
       "      <td>0.003351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.312961</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.409738</td>\n",
       "      <td>0.001375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.312898</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.409893</td>\n",
       "      <td>0.001608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.312743</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.409776</td>\n",
       "      <td>0.001619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.312534</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.409660</td>\n",
       "      <td>0.001317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.001582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.311961</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.409893</td>\n",
       "      <td>0.001655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.311873</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.409931</td>\n",
       "      <td>0.001865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.311378</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.409835</td>\n",
       "      <td>0.001762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.311349</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.410281</td>\n",
       "      <td>0.001578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.311135</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>0.409776</td>\n",
       "      <td>0.001023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.311019</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.409835</td>\n",
       "      <td>0.001318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.310688</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.410126</td>\n",
       "      <td>0.001317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.310305</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.410048</td>\n",
       "      <td>0.001186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.310023</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.410145</td>\n",
       "      <td>0.001369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.309858</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.410242</td>\n",
       "      <td>0.001048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.309513</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.410107</td>\n",
       "      <td>0.001192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.309304</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.409970</td>\n",
       "      <td>0.001157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.308955</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.001395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.308780</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.409426</td>\n",
       "      <td>0.001689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.308415</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.409932</td>\n",
       "      <td>0.001905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.308362</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.409932</td>\n",
       "      <td>0.002432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.308231</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.410145</td>\n",
       "      <td>0.002092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.307978</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.409795</td>\n",
       "      <td>0.002151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.307779</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.409601</td>\n",
       "      <td>0.002182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.307735</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.409193</td>\n",
       "      <td>0.002227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.307424</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.409349</td>\n",
       "      <td>0.002319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0.307210</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.409465</td>\n",
       "      <td>0.002504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.306798</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.409193</td>\n",
       "      <td>0.002639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.306351</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.409154</td>\n",
       "      <td>0.002469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.306200</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.408863</td>\n",
       "      <td>0.002615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.456000         0.006321         0.466312        0.001944\n",
       "1            0.447083         0.004315         0.457803        0.002286\n",
       "2            0.444509         0.005431         0.454791        0.003364\n",
       "3            0.439793         0.003425         0.450459        0.003843\n",
       "4            0.437063         0.003233         0.450905        0.002244\n",
       "5            0.434644         0.003702         0.448768        0.006003\n",
       "6            0.433017         0.003748         0.449137        0.004384\n",
       "7            0.429986         0.003041         0.446165        0.004118\n",
       "8            0.428160         0.002557         0.443076        0.003589\n",
       "9            0.425255         0.002746         0.441716        0.004589\n",
       "10           0.423585         0.002115         0.440706        0.003903\n",
       "11           0.421710         0.002220         0.440881        0.004586\n",
       "12           0.419563         0.002772         0.438841        0.005604\n",
       "13           0.418276         0.002316         0.437228        0.004682\n",
       "14           0.417688         0.002009         0.436587        0.004444\n",
       "15           0.416114         0.001804         0.436975        0.004775\n",
       "16           0.413904         0.002195         0.436723        0.005258\n",
       "17           0.413302         0.001790         0.436082        0.005816\n",
       "18           0.413166         0.000896         0.436043        0.004673\n",
       "19           0.411461         0.001101         0.435421        0.005095\n",
       "20           0.409417         0.001490         0.433867        0.004850\n",
       "21           0.408329         0.001111         0.434722        0.004308\n",
       "22           0.406745         0.001356         0.433847        0.003951\n",
       "23           0.405361         0.001212         0.432895        0.004309\n",
       "24           0.403943         0.001616         0.431691        0.003807\n",
       "25           0.403054         0.001688         0.431652        0.004577\n",
       "26           0.402039         0.002085         0.432565        0.004452\n",
       "27           0.400587         0.001593         0.431380        0.004370\n",
       "28           0.399669         0.001411         0.430467        0.004021\n",
       "29           0.398241         0.002030         0.429573        0.003351\n",
       "..                ...              ...              ...             ...\n",
       "238          0.312961         0.000904         0.409738        0.001375\n",
       "239          0.312898         0.000843         0.409893        0.001608\n",
       "240          0.312743         0.000939         0.409776        0.001619\n",
       "241          0.312534         0.001213         0.409660        0.001317\n",
       "242          0.312223         0.001121         0.409524        0.001582\n",
       "243          0.311961         0.000870         0.409893        0.001655\n",
       "244          0.311873         0.000584         0.409931        0.001865\n",
       "245          0.311378         0.000440         0.409835        0.001762\n",
       "246          0.311349         0.000782         0.410281        0.001578\n",
       "247          0.311135         0.000760         0.409776        0.001023\n",
       "248          0.311019         0.000711         0.409835        0.001318\n",
       "249          0.310688         0.000615         0.410126        0.001317\n",
       "250          0.310305         0.000687         0.410048        0.001186\n",
       "251          0.310023         0.000686         0.410145        0.001369\n",
       "252          0.309858         0.000627         0.410242        0.001048\n",
       "253          0.309513         0.000802         0.410107        0.001192\n",
       "254          0.309304         0.000792         0.409970        0.001157\n",
       "255          0.308955         0.000936         0.409524        0.001395\n",
       "256          0.308780         0.000809         0.409426        0.001689\n",
       "257          0.308415         0.000676         0.409932        0.001905\n",
       "258          0.308362         0.000653         0.409932        0.002432\n",
       "259          0.308231         0.000374         0.410145        0.002092\n",
       "260          0.307978         0.000719         0.409795        0.002151\n",
       "261          0.307779         0.000633         0.409601        0.002182\n",
       "262          0.307735         0.000657         0.409193        0.002227\n",
       "263          0.307424         0.000517         0.409349        0.002319\n",
       "264          0.307210         0.000549         0.409465        0.002504\n",
       "265          0.306798         0.000424         0.409193        0.002639\n",
       "266          0.306351         0.000412         0.409154        0.002469\n",
       "267          0.306200         0.000627         0.408863        0.002615\n",
       "\n",
       "[268 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=1500,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=1500, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'max_depth': 7, 'min_child_weight': 51}\n",
      "Run 0 best score:  0.596848772147964\n",
      "Run 1 best param:  {'max_depth': 5, 'min_child_weight': 51}\n",
      "Run 1 best score:  0.5970624805719614\n",
      "Run 2 best param:  {'max_depth': 7, 'min_child_weight': 51}\n",
      "Run 2 best score:  0.5945368355610817\n",
      "Run 3 best param:  {'max_depth': 7, 'min_child_weight': 51}\n",
      "Run 3 best score:  0.5965184954926951\n",
      "Best params:  params               {'max_depth': 7, 'min_child_weight': 51}\n",
      "mean_test_score_0                                    0.596849\n",
      "mean_test_score_1                                    0.596285\n",
      "mean_test_score_2                                    0.594537\n",
      "mean_test_score_3                                    0.596518\n",
      "avg                                                  0.596047\n",
      "Name: 31, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1 = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1,500,50)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=267,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1.fit(X_train,y_train)    \n",
    "    if grid_score1.empty:\n",
    "        grid_score1 = pd.DataFrame(gsearch1.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1.best_score_)\n",
    "\n",
    "grid_score1['avg'] = grid_score1.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1.loc[grid_score1.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'max_depth': 7, 'min_child_weight': 65}\n",
      "Run 0 best score:  0.5980533105377681\n",
      "Run 1 best param:  {'max_depth': 9, 'min_child_weight': 65}\n",
      "Run 1 best score:  0.5981893068075848\n",
      "Run 2 best param:  {'max_depth': 7, 'min_child_weight': 65}\n",
      "Run 2 best score:  0.5970624805719614\n",
      "Run 3 best param:  {'max_depth': 5, 'min_child_weight': 65}\n",
      "Run 3 best score:  0.5966739198010569\n",
      "Best params:  params               {'max_depth': 7, 'min_child_weight': 65}\n",
      "mean_test_score_0                                    0.598053\n",
      "mean_test_score_1                                    0.597257\n",
      "mean_test_score_2                                    0.597062\n",
      "mean_test_score_3                                    0.595722\n",
      "avg                                                  0.597024\n",
      "Name: 38, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1b = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(25,75,5)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1b = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=267,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1b = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1b,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1b.fit(X_train,y_train)    \n",
    "    if grid_score1b.empty:\n",
    "        grid_score1b = pd.DataFrame(gsearch1b.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1b.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1b['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1b.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1b.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1b.best_score_)\n",
    "\n",
    "grid_score1b['avg'] = grid_score1b.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1b.loc[grid_score1b.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'max_depth': 6, 'min_child_weight': 68}\n",
      "Run 0 best score:  0.5985390115013988\n",
      "Run 1 best param:  {'max_depth': 7, 'min_child_weight': 67}\n",
      "Run 1 best score:  0.598811004041032\n",
      "Run 2 best param:  {'max_depth': 7, 'min_child_weight': 67}\n",
      "Run 2 best score:  0.5971207646875971\n",
      "Run 3 best param:  {'max_depth': 8, 'min_child_weight': 69}\n",
      "Run 3 best score:  0.596771059993783\n",
      "Best params:  params               {'max_depth': 7, 'min_child_weight': 67}\n",
      "mean_test_score_0                                    0.597373\n",
      "mean_test_score_1                                    0.598811\n",
      "mean_test_score_2                                    0.597121\n",
      "mean_test_score_3                                    0.595819\n",
      "avg                                                  0.597281\n",
      "Name: 8, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look carefully again the neigbor values\n",
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test2 = {\n",
    " 'max_depth':[6, 7, 8],\n",
    " 'min_child_weight':range(65, 71)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score2 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=267,\n",
    "        max_depth=7,\n",
    "        min_child_weight=65,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test2,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch2.fit(X_train,y_train)    \n",
    "    if grid_score2.empty:\n",
    "        grid_score2 = pd.DataFrame(gsearch2.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score2.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score2['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch2.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch2.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch2.best_score_)\n",
    "\n",
    "grid_score2['avg'] = grid_score2.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score2.loc[grid_score2.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'gamma': 0.3}\n",
      "Run 0 best score:  0.5984807273857631\n",
      "Run 1 best param:  {'gamma': 0.0}\n",
      "Run 1 best score:  0.598811004041032\n",
      "Run 2 best param:  {'gamma': 0.0}\n",
      "Run 2 best score:  0.5971207646875971\n",
      "Run 3 best param:  {'gamma': 0.3}\n",
      "Run 3 best score:  0.5961299347217904\n",
      "Best params:  params               {'gamma': 0.0}\n",
      "mean_test_score_0          0.597373\n",
      "mean_test_score_1          0.598811\n",
      "mean_test_score_2          0.597121\n",
      "mean_test_score_3          0.595819\n",
      "avg                        0.597281\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score3 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=267,\n",
    "        max_depth=7,\n",
    "        min_child_weight=67,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch3 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test3,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch3.fit(X_train,y_train)    \n",
    "    if grid_score3.empty:\n",
    "        grid_score3 = pd.DataFrame(gsearch3.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score3.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score3['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch3.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch3.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch3.best_score_)\n",
    "\n",
    "grid_score3['avg'] = grid_score3.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score3.loc[grid_score3.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recablirating the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.451269</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.458094</td>\n",
       "      <td>0.003188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.443703</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>0.449682</td>\n",
       "      <td>0.006224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440356</td>\n",
       "      <td>0.008308</td>\n",
       "      <td>0.446185</td>\n",
       "      <td>0.007777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.435824</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.443620</td>\n",
       "      <td>0.006135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.433911</td>\n",
       "      <td>0.003296</td>\n",
       "      <td>0.442415</td>\n",
       "      <td>0.005501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.432522</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.440492</td>\n",
       "      <td>0.004914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.430793</td>\n",
       "      <td>0.002049</td>\n",
       "      <td>0.441094</td>\n",
       "      <td>0.006321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.429355</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>0.440200</td>\n",
       "      <td>0.004762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.428009</td>\n",
       "      <td>0.002957</td>\n",
       "      <td>0.438841</td>\n",
       "      <td>0.004180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.426742</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.437247</td>\n",
       "      <td>0.004350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.425537</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.434333</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.423730</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>0.432176</td>\n",
       "      <td>0.003764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.421574</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.430603</td>\n",
       "      <td>0.001575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.420661</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.430486</td>\n",
       "      <td>0.002120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.419752</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.430875</td>\n",
       "      <td>0.002746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.418378</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.429865</td>\n",
       "      <td>0.002530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.417470</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.429748</td>\n",
       "      <td>0.001738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.417086</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.429340</td>\n",
       "      <td>0.002319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.416445</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.428038</td>\n",
       "      <td>0.002161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.415697</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.427825</td>\n",
       "      <td>0.002905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.414313</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.428291</td>\n",
       "      <td>0.002758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.413555</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.427222</td>\n",
       "      <td>0.002526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.413181</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.426873</td>\n",
       "      <td>0.003504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.412171</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.427339</td>\n",
       "      <td>0.002706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.411355</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.426659</td>\n",
       "      <td>0.002982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.410568</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.425824</td>\n",
       "      <td>0.002839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.409859</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.425493</td>\n",
       "      <td>0.003236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.408960</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.425415</td>\n",
       "      <td>0.003498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.408251</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.426134</td>\n",
       "      <td>0.003537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.408110</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.424988</td>\n",
       "      <td>0.003495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.366660</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.403054</td>\n",
       "      <td>0.003532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.366617</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.403170</td>\n",
       "      <td>0.003642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.366539</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.403229</td>\n",
       "      <td>0.003678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.366330</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.403307</td>\n",
       "      <td>0.003595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.366189</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.402840</td>\n",
       "      <td>0.003585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.366185</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.003917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.366029</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.403268</td>\n",
       "      <td>0.003936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.365874</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.402937</td>\n",
       "      <td>0.003963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.365650</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.403384</td>\n",
       "      <td>0.003848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.365660</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.403287</td>\n",
       "      <td>0.004007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.365675</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.403054</td>\n",
       "      <td>0.004485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.365499</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.402879</td>\n",
       "      <td>0.004716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.365349</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.402996</td>\n",
       "      <td>0.004756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.365140</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>0.004723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.364752</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.403306</td>\n",
       "      <td>0.004789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.364659</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.403404</td>\n",
       "      <td>0.004863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.364465</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.403151</td>\n",
       "      <td>0.004648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.364475</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.403364</td>\n",
       "      <td>0.004461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.364392</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.403132</td>\n",
       "      <td>0.004489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.364475</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.403073</td>\n",
       "      <td>0.004928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.364407</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>0.403190</td>\n",
       "      <td>0.004788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.364276</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.402957</td>\n",
       "      <td>0.004436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0.364115</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.403229</td>\n",
       "      <td>0.004472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.363955</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.403112</td>\n",
       "      <td>0.004559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>0.364038</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.402763</td>\n",
       "      <td>0.004388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>0.363994</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.402957</td>\n",
       "      <td>0.003826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>0.363980</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.403093</td>\n",
       "      <td>0.003824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.363775</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.403093</td>\n",
       "      <td>0.003677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.402821</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.363387</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.402665</td>\n",
       "      <td>0.003568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.451269         0.006996         0.458094        0.003188\n",
       "1            0.443703         0.006866         0.449682        0.006224\n",
       "2            0.440356         0.008308         0.446185        0.007777\n",
       "3            0.435824         0.003161         0.443620        0.006135\n",
       "4            0.433911         0.003296         0.442415        0.005501\n",
       "5            0.432522         0.004231         0.440492        0.004914\n",
       "6            0.430793         0.002049         0.441094        0.006321\n",
       "7            0.429355         0.003424         0.440200        0.004762\n",
       "8            0.428009         0.002957         0.438841        0.004180\n",
       "9            0.426742         0.002118         0.437247        0.004350\n",
       "10           0.425537         0.002286         0.434333        0.004400\n",
       "11           0.423730         0.001708         0.432176        0.003764\n",
       "12           0.421574         0.001627         0.430603        0.001575\n",
       "13           0.420661         0.001024         0.430486        0.002120\n",
       "14           0.419752         0.001367         0.430875        0.002746\n",
       "15           0.418378         0.000761         0.429865        0.002530\n",
       "16           0.417470         0.001045         0.429748        0.001738\n",
       "17           0.417086         0.000741         0.429340        0.002319\n",
       "18           0.416445         0.001041         0.428038        0.002161\n",
       "19           0.415697         0.000680         0.427825        0.002905\n",
       "20           0.414313         0.000480         0.428291        0.002758\n",
       "21           0.413555         0.000898         0.427222        0.002526\n",
       "22           0.413181         0.000951         0.426873        0.003504\n",
       "23           0.412171         0.000922         0.427339        0.002706\n",
       "24           0.411355         0.000819         0.426659        0.002982\n",
       "25           0.410568         0.000580         0.425824        0.002839\n",
       "26           0.409859         0.000670         0.425493        0.003236\n",
       "27           0.408960         0.000895         0.425415        0.003498\n",
       "28           0.408251         0.000863         0.426134        0.003537\n",
       "29           0.408110         0.000759         0.424988        0.003495\n",
       "..                ...              ...              ...             ...\n",
       "242          0.366660         0.000981         0.403054        0.003532\n",
       "243          0.366617         0.000944         0.403170        0.003642\n",
       "244          0.366539         0.001141         0.403229        0.003678\n",
       "245          0.366330         0.001161         0.403307        0.003595\n",
       "246          0.366189         0.001070         0.402840        0.003585\n",
       "247          0.366185         0.001075         0.403034        0.003917\n",
       "248          0.366029         0.000942         0.403268        0.003936\n",
       "249          0.365874         0.000866         0.402937        0.003963\n",
       "250          0.365650         0.000900         0.403384        0.003848\n",
       "251          0.365660         0.000846         0.403287        0.004007\n",
       "252          0.365675         0.001080         0.403054        0.004485\n",
       "253          0.365499         0.000944         0.402879        0.004716\n",
       "254          0.365349         0.001085         0.402996        0.004756\n",
       "255          0.365140         0.001155         0.403034        0.004723\n",
       "256          0.364752         0.000941         0.403306        0.004789\n",
       "257          0.364659         0.000905         0.403404        0.004863\n",
       "258          0.364465         0.000735         0.403151        0.004648\n",
       "259          0.364475         0.000967         0.403364        0.004461\n",
       "260          0.364392         0.000780         0.403132        0.004489\n",
       "261          0.364475         0.000818         0.403073        0.004928\n",
       "262          0.364407         0.000884         0.403190        0.004788\n",
       "263          0.364276         0.000861         0.402957        0.004436\n",
       "264          0.364115         0.000896         0.403229        0.004472\n",
       "265          0.363955         0.000852         0.403112        0.004559\n",
       "266          0.364038         0.000891         0.402763        0.004388\n",
       "267          0.363994         0.001086         0.402957        0.003826\n",
       "268          0.363980         0.001106         0.403093        0.003824\n",
       "269          0.363775         0.001000         0.403093        0.003677\n",
       "270          0.363600         0.001090         0.402821        0.003510\n",
       "271          0.363387         0.001042         0.402665        0.003568\n",
       "\n",
       "[272 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=267,\n",
    "    max_depth=7,\n",
    "    min_child_weight=67,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=1500, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'n_estimators': 400}\n",
      "Run 0 best score:  0.5981310226919491\n",
      "Run 1 best param:  {'n_estimators': 271}\n",
      "Run 1 best score:  0.5989664283493938\n",
      "Run 2 best param:  {'n_estimators': 400}\n",
      "Run 2 best score:  0.5969070562635996\n",
      "Run 3 best param:  {'n_estimators': 400}\n",
      "Run 3 best score:  0.5968293441094187\n",
      "Best params:  params               {'n_estimators': 400}\n",
      "mean_test_score_0                 0.598131\n",
      "mean_test_score_1                 0.597257\n",
      "mean_test_score_2                 0.596907\n",
      "mean_test_score_3                 0.596829\n",
      "avg                               0.597281\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(100, 1500, 100)]+[271]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=271,\n",
    "        max_depth=7,\n",
    "        min_child_weight=67,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'n_estimators': 410}\n",
      "Run 0 best score:  0.5990829965806652\n",
      "Run 1 best param:  {'n_estimators': 350}\n",
      "Run 1 best score:  0.5978978862294063\n",
      "Run 2 best param:  {'n_estimators': 420}\n",
      "Run 2 best score:  0.5977230338824993\n",
      "Run 3 best param:  {'n_estimators': 370}\n",
      "Run 3 best score:  0.597023624494871\n",
      "Best params:  params               {'n_estimators': 410}\n",
      "mean_test_score_0                 0.599083\n",
      "mean_test_score_1                 0.597509\n",
      "mean_test_score_2                 0.596985\n",
      "mean_test_score_3                 0.596616\n",
      "avg                               0.597548\n",
      "Name: 6, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(350, 450, 10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=271,\n",
    "        max_depth=7,\n",
    "        min_child_weight=67,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 0 best score:  0.5990829965806652\n",
      "Run 1 best param:  {'colsample_bytree': 0.7, 'subsample': 0.8}\n",
      "Run 1 best score:  0.5987527199253964\n",
      "Run 2 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 2 best score:  0.5969847684177806\n",
      "Run 3 best param:  {'colsample_bytree': 0.7, 'subsample': 0.8}\n",
      "Run 3 best score:  0.5967322039166926\n",
      "Best params:  params               {'colsample_bytree': 0.7, 'subsample': 0.8}\n",
      "mean_test_score_0                                       0.598714\n",
      "mean_test_score_1                                       0.598753\n",
      "mean_test_score_2                                       0.596363\n",
      "mean_test_score_3                                       0.596732\n",
      "avg                                                      0.59764\n",
      "Name: 6, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score4 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=410,\n",
    "        max_depth=7,\n",
    "        min_child_weight=67,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test4,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch4.fit(X_train,y_train)    \n",
    "    if grid_score4.empty:\n",
    "        grid_score4 = pd.DataFrame(gsearch4.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score4.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score4['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch4.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch4.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch4.best_score_)\n",
    "\n",
    "grid_score4['avg'] = grid_score4.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score4.loc[grid_score4.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'colsample_bytree': 0.65, 'subsample': 0.75}\n",
      "Run 0 best score:  0.600423531240286\n",
      "Run 1 best param:  {'colsample_bytree': 0.75, 'subsample': 0.8}\n",
      "Run 1 best score:  0.5992967050046627\n",
      "Run 2 best param:  {'colsample_bytree': 0.75, 'subsample': 0.8}\n",
      "Run 2 best score:  0.59694591234069\n",
      "Run 3 best param:  {'colsample_bytree': 0.7, 'subsample': 0.85}\n",
      "Run 3 best score:  0.5975676095741373\n",
      "Best params:  params               {'colsample_bytree': 0.7, 'subsample': 0.8}\n",
      "mean_test_score_0                                       0.598714\n",
      "mean_test_score_1                                       0.598753\n",
      "mean_test_score_2                                       0.596363\n",
      "mean_test_score_3                                       0.596732\n",
      "avg                                                      0.59764\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Carefully search for each neighboring 0.05\n",
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(75,86,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(65,76,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score5 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=410,\n",
    "        max_depth=7,\n",
    "        min_child_weight=67,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch5 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test5,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch5.fit(X_train,y_train)    \n",
    "    if grid_score5.empty:\n",
    "        grid_score5 = pd.DataFrame(gsearch5.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score5.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score5['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch5.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch5.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch5.best_score_)\n",
    "\n",
    "grid_score5['avg'] = grid_score5.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score5.loc[grid_score5.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'reg_alpha': 0}\n",
      "Run 0 best score:  0.5987138638483058\n",
      "Run 1 best param:  {'reg_alpha': 0}\n",
      "Run 1 best score:  0.5987527199253964\n",
      "Run 2 best param:  {'reg_alpha': 0}\n",
      "Run 2 best score:  0.5963630711843332\n",
      "Run 3 best param:  {'reg_alpha': 1}\n",
      "Run 3 best score:  0.5972373329188685\n",
      "Best params:  params               {'reg_alpha': 0}\n",
      "mean_test_score_0            0.598714\n",
      "mean_test_score_1            0.598753\n",
      "mean_test_score_2            0.596363\n",
      "mean_test_score_3            0.596732\n",
      "avg                           0.59764\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[0, 1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score6 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=410,\n",
    "        max_depth=7,\n",
    "        min_child_weight=67,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch6 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test6,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch6.fit(X_train,y_train)    \n",
    "    if grid_score6.empty:\n",
    "        grid_score6 = pd.DataFrame(gsearch6.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score6.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score6['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch6.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch6.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch6.best_score_)\n",
    "\n",
    "grid_score6['avg'] = grid_score6.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score6.loc[grid_score6.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'reg_alpha': 0}\n",
      "Run 0 best score:  0.5987138638483058\n",
      "Run 1 best param:  {'reg_alpha': 0}\n",
      "Run 1 best score:  0.5987527199253964\n",
      "Run 2 best param:  {'reg_alpha': 0}\n",
      "Run 2 best score:  0.5963630711843332\n",
      "Run 3 best param:  {'reg_alpha': 0}\n",
      "Run 3 best score:  0.5967322039166926\n",
      "Best params:  params               {'reg_alpha': 0}\n",
      "mean_test_score_0            0.598714\n",
      "mean_test_score_1            0.598753\n",
      "mean_test_score_2            0.596363\n",
      "mean_test_score_3            0.596732\n",
      "avg                           0.59764\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test7 = {\n",
    " 'reg_alpha':[0, 1e-6, 5e-6, 1e-5, 5e-5, 1e-4, 5e-4]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score7 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=410,\n",
    "        max_depth=7,\n",
    "        min_child_weight=67,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch7 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test7,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch7.fit(X_train,y_train)    \n",
    "    if grid_score7.empty:\n",
    "        grid_score7 = pd.DataFrame(gsearch7.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score7.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score7['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch7.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch7.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch7.best_score_)\n",
    "\n",
    "grid_score7['avg'] = grid_score7.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score7.loc[grid_score7.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the learning rate and tune n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.449118</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>0.454577</td>\n",
       "      <td>0.004564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.442862</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.448904</td>\n",
       "      <td>0.004448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440686</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.446767</td>\n",
       "      <td>0.005154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440201</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>0.448127</td>\n",
       "      <td>0.004002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.439579</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.447253</td>\n",
       "      <td>0.005173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.439001</td>\n",
       "      <td>0.002369</td>\n",
       "      <td>0.447233</td>\n",
       "      <td>0.005898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.440409</td>\n",
       "      <td>0.003817</td>\n",
       "      <td>0.448652</td>\n",
       "      <td>0.005159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.438491</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.447661</td>\n",
       "      <td>0.005742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.437184</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>0.444669</td>\n",
       "      <td>0.006240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.437767</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.444572</td>\n",
       "      <td>0.005936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.437748</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>0.444883</td>\n",
       "      <td>0.005417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.437223</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.445660</td>\n",
       "      <td>0.006562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.436878</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.444980</td>\n",
       "      <td>0.005539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.436383</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>0.445660</td>\n",
       "      <td>0.005513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.435679</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.445699</td>\n",
       "      <td>0.006256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.435237</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>0.445524</td>\n",
       "      <td>0.005832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.435606</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.445796</td>\n",
       "      <td>0.006348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.435596</td>\n",
       "      <td>0.002156</td>\n",
       "      <td>0.445038</td>\n",
       "      <td>0.006079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.434979</td>\n",
       "      <td>0.001955</td>\n",
       "      <td>0.443523</td>\n",
       "      <td>0.006364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.434537</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.443212</td>\n",
       "      <td>0.006515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.434688</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.442979</td>\n",
       "      <td>0.006824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.434659</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>0.443678</td>\n",
       "      <td>0.006049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.434712</td>\n",
       "      <td>0.002590</td>\n",
       "      <td>0.443600</td>\n",
       "      <td>0.006824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.434392</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.443795</td>\n",
       "      <td>0.006128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.434003</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.444183</td>\n",
       "      <td>0.006074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.433896</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.442746</td>\n",
       "      <td>0.006615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.433561</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.442493</td>\n",
       "      <td>0.006216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.433026</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.442415</td>\n",
       "      <td>0.006478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.432842</td>\n",
       "      <td>0.001865</td>\n",
       "      <td>0.441658</td>\n",
       "      <td>0.006935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.432658</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.442201</td>\n",
       "      <td>0.007159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>0.382455</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.407639</td>\n",
       "      <td>0.004172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>0.382402</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.407581</td>\n",
       "      <td>0.004144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>0.382422</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.407503</td>\n",
       "      <td>0.004250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>0.382494</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.407445</td>\n",
       "      <td>0.004130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>0.382504</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.407425</td>\n",
       "      <td>0.004155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>0.382407</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.407270</td>\n",
       "      <td>0.004096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>0.382383</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.407231</td>\n",
       "      <td>0.004256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>0.382368</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.407386</td>\n",
       "      <td>0.004317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>0.382310</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.407445</td>\n",
       "      <td>0.004416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>0.382315</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.407270</td>\n",
       "      <td>0.004251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>0.382344</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.407212</td>\n",
       "      <td>0.004281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>0.382329</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.407173</td>\n",
       "      <td>0.004286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>0.382319</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.407114</td>\n",
       "      <td>0.004211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>0.382324</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.407212</td>\n",
       "      <td>0.004298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>0.382281</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.407250</td>\n",
       "      <td>0.004274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.382169</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.406979</td>\n",
       "      <td>0.004169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>0.382145</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.407037</td>\n",
       "      <td>0.004160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>0.382188</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.406823</td>\n",
       "      <td>0.004296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>0.382218</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.406940</td>\n",
       "      <td>0.004135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>0.382310</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.406959</td>\n",
       "      <td>0.004119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>0.382242</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.407076</td>\n",
       "      <td>0.004228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>0.382193</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.407075</td>\n",
       "      <td>0.004143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>0.382310</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.407095</td>\n",
       "      <td>0.004243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>0.382246</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.406959</td>\n",
       "      <td>0.004321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1033</th>\n",
       "      <td>0.382252</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.407056</td>\n",
       "      <td>0.004194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>0.382193</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.407017</td>\n",
       "      <td>0.004176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>0.382135</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.406901</td>\n",
       "      <td>0.004189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>0.381999</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.406862</td>\n",
       "      <td>0.004210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>0.382043</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.406745</td>\n",
       "      <td>0.004169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>0.382101</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.406745</td>\n",
       "      <td>0.004060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1039 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0             0.449118         0.004318         0.454577        0.004564\n",
       "1             0.442862         0.002303         0.448904        0.004448\n",
       "2             0.440686         0.001609         0.446767        0.005154\n",
       "3             0.440201         0.003466         0.448127        0.004002\n",
       "4             0.439579         0.003657         0.447253        0.005173\n",
       "5             0.439001         0.002369         0.447233        0.005898\n",
       "6             0.440409         0.003817         0.448652        0.005159\n",
       "7             0.438491         0.003978         0.447661        0.005742\n",
       "8             0.437184         0.003217         0.444669        0.006240\n",
       "9             0.437767         0.002869         0.444572        0.005936\n",
       "10            0.437748         0.003223         0.444883        0.005417\n",
       "11            0.437223         0.003075         0.445660        0.006562\n",
       "12            0.436878         0.002766         0.444980        0.005539\n",
       "13            0.436383         0.002689         0.445660        0.005513\n",
       "14            0.435679         0.002745         0.445699        0.006256\n",
       "15            0.435237         0.002547         0.445524        0.005832\n",
       "16            0.435606         0.002568         0.445796        0.006348\n",
       "17            0.435596         0.002156         0.445038        0.006079\n",
       "18            0.434979         0.001955         0.443523        0.006364\n",
       "19            0.434537         0.002277         0.443212        0.006515\n",
       "20            0.434688         0.002395         0.442979        0.006824\n",
       "21            0.434659         0.002856         0.443678        0.006049\n",
       "22            0.434712         0.002590         0.443600        0.006824\n",
       "23            0.434392         0.002838         0.443795        0.006128\n",
       "24            0.434003         0.002725         0.444183        0.006074\n",
       "25            0.433896         0.002470         0.442746        0.006615\n",
       "26            0.433561         0.001989         0.442493        0.006216\n",
       "27            0.433026         0.001761         0.442415        0.006478\n",
       "28            0.432842         0.001865         0.441658        0.006935\n",
       "29            0.432658         0.001606         0.442201        0.007159\n",
       "...                ...              ...              ...             ...\n",
       "1009          0.382455         0.001025         0.407639        0.004172\n",
       "1010          0.382402         0.001014         0.407581        0.004144\n",
       "1011          0.382422         0.001062         0.407503        0.004250\n",
       "1012          0.382494         0.001163         0.407445        0.004130\n",
       "1013          0.382504         0.001059         0.407425        0.004155\n",
       "1014          0.382407         0.001069         0.407270        0.004096\n",
       "1015          0.382383         0.001170         0.407231        0.004256\n",
       "1016          0.382368         0.001213         0.407386        0.004317\n",
       "1017          0.382310         0.001235         0.407445        0.004416\n",
       "1018          0.382315         0.001152         0.407270        0.004251\n",
       "1019          0.382344         0.001223         0.407212        0.004281\n",
       "1020          0.382329         0.001288         0.407173        0.004286\n",
       "1021          0.382319         0.001238         0.407114        0.004211\n",
       "1022          0.382324         0.001195         0.407212        0.004298\n",
       "1023          0.382281         0.001139         0.407250        0.004274\n",
       "1024          0.382169         0.001187         0.406979        0.004169\n",
       "1025          0.382145         0.001174         0.407037        0.004160\n",
       "1026          0.382188         0.001200         0.406823        0.004296\n",
       "1027          0.382218         0.001107         0.406940        0.004135\n",
       "1028          0.382310         0.001013         0.406959        0.004119\n",
       "1029          0.382242         0.000994         0.407076        0.004228\n",
       "1030          0.382193         0.001018         0.407075        0.004143\n",
       "1031          0.382310         0.000998         0.407095        0.004243\n",
       "1032          0.382246         0.000969         0.406959        0.004321\n",
       "1033          0.382252         0.001050         0.407056        0.004194\n",
       "1034          0.382193         0.001064         0.407017        0.004176\n",
       "1035          0.382135         0.001214         0.406901        0.004189\n",
       "1036          0.381999         0.001132         0.406862        0.004210\n",
       "1037          0.382043         0.001166         0.406745        0.004169\n",
       "1038          0.382101         0.001144         0.406745        0.004060\n",
       "\n",
       "[1039 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=410,\n",
    "    max_depth=7,\n",
    "    min_child_weight=67,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.7,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 4 times\n",
      "Run 0 best param:  {'n_estimators': 1900}\n",
      "Run 0 best score:  0.5971984768417781\n",
      "Run 1 best param:  {'n_estimators': 1800}\n",
      "Run 1 best score:  0.5957996580665216\n",
      "Run 2 best param:  {'n_estimators': 1900}\n",
      "Run 2 best score:  0.5957996580665216\n",
      "Run 3 best param:  {'n_estimators': 1700}\n",
      "Run 3 best score:  0.5963630711843332\n",
      "Best params:  params               {'n_estimators': 1900}\n",
      "mean_test_score_0                  0.597198\n",
      "mean_test_score_1                    0.5958\n",
      "mean_test_score_2                    0.5958\n",
      "mean_test_score_3                  0.596363\n",
      "avg                                 0.59629\n",
      "Name: 9, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(1000, 2000, 100)]+[1038]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=410,\n",
    "        max_depth=7,\n",
    "        min_child_weight=67,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(1400, 1500, 20)]+[1423]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=195,\n",
    "        max_depth=5,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.1,\n",
    "        subsample=0.95,\n",
    "        colsample_bytree=0.6,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0: 59.55%\n",
      "Accuracy 1: 59.47%\n",
      "Accuracy 2: 59.37%\n",
      "Accuracy 3: 59.40%\n",
      "Average accuracy is: 59.45%\n"
     ]
    }
   ],
   "source": [
    "accuracy_array = []\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=410,\n",
    "        max_depth=7,\n",
    "        min_child_weight=67,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.7,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=i\n",
    "    )\n",
    "    model = xgb.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_array.append(accuracy)\n",
    "    print('Accuracy {}: %.2f%%'.format(i) % (accuracy * 100.0))\n",
    "mean_accuracy_score = sum(accuracy_array) / NUM_TRIALS\n",
    "print('Average accuracy is: %.2f%%' % (mean_accuracy_score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
