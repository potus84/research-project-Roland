{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flavours of physics dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xgboost\n",
    "import copy\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/tai/Projects/research-project-Roland/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/flavours_of_physics/flavours_of_physics.0.train\", encoding='latin1', \n",
    "                 header=None,\n",
    "                 na_values='?',\n",
    "                 low_memory=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/flavours_of_physics/flavours_of_physics.0.test\", encoding='latin1', \n",
    "                 header=None,\n",
    "                 na_values='?',\n",
    "                 low_memory=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9082753</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.999952</td>\n",
       "      <td>4.066977</td>\n",
       "      <td>0.328034</td>\n",
       "      <td>0.039081</td>\n",
       "      <td>2.240830</td>\n",
       "      <td>6.968715</td>\n",
       "      <td>2934.679199</td>\n",
       "      <td>0.048262</td>\n",
       "      <td>...</td>\n",
       "      <td>17462.171875</td>\n",
       "      <td>10766.777344</td>\n",
       "      <td>3.868153</td>\n",
       "      <td>2.994936</td>\n",
       "      <td>2.947038</td>\n",
       "      <td>261</td>\n",
       "      <td>-99</td>\n",
       "      <td>0</td>\n",
       "      <td>1797.798950</td>\n",
       "      <td>0.238707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1124795</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.999466</td>\n",
       "      <td>8.500341</td>\n",
       "      <td>0.375011</td>\n",
       "      <td>0.237087</td>\n",
       "      <td>15.826837</td>\n",
       "      <td>11.026155</td>\n",
       "      <td>2551.849609</td>\n",
       "      <td>0.106709</td>\n",
       "      <td>...</td>\n",
       "      <td>7694.305664</td>\n",
       "      <td>38083.234375</td>\n",
       "      <td>3.437981</td>\n",
       "      <td>2.975132</td>\n",
       "      <td>3.747602</td>\n",
       "      <td>231</td>\n",
       "      <td>-99</td>\n",
       "      <td>0</td>\n",
       "      <td>1727.854980</td>\n",
       "      <td>0.662632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6620172</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>42.543549</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.264044</td>\n",
       "      <td>12.668289</td>\n",
       "      <td>2.365929</td>\n",
       "      <td>5646.794434</td>\n",
       "      <td>0.035337</td>\n",
       "      <td>...</td>\n",
       "      <td>80296.695312</td>\n",
       "      <td>18222.199219</td>\n",
       "      <td>3.821355</td>\n",
       "      <td>3.950199</td>\n",
       "      <td>3.454009</td>\n",
       "      <td>186</td>\n",
       "      <td>-99</td>\n",
       "      <td>0</td>\n",
       "      <td>1864.751953</td>\n",
       "      <td>0.209850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4502983</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>15.797983</td>\n",
       "      <td>0.507167</td>\n",
       "      <td>0.048741</td>\n",
       "      <td>2.586148</td>\n",
       "      <td>6.734583</td>\n",
       "      <td>2136.523438</td>\n",
       "      <td>0.027997</td>\n",
       "      <td>...</td>\n",
       "      <td>25081.158203</td>\n",
       "      <td>12336.913086</td>\n",
       "      <td>3.655804</td>\n",
       "      <td>3.555438</td>\n",
       "      <td>3.397589</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1762.937744</td>\n",
       "      <td>0.254817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6703161</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>11.153282</td>\n",
       "      <td>0.314592</td>\n",
       "      <td>0.042594</td>\n",
       "      <td>2.986686</td>\n",
       "      <td>13.142979</td>\n",
       "      <td>8152.657715</td>\n",
       "      <td>0.016565</td>\n",
       "      <td>...</td>\n",
       "      <td>55539.929688</td>\n",
       "      <td>20167.025391</td>\n",
       "      <td>2.847753</td>\n",
       "      <td>3.169065</td>\n",
       "      <td>2.935316</td>\n",
       "      <td>188</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1780.325562</td>\n",
       "      <td>0.883157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2          3         4         5          6   \\\n",
       "0  9082753  0.000643  0.999952   4.066977  0.328034  0.039081   2.240830   \n",
       "1  1124795  0.000978  0.999466   8.500341  0.375011  0.237087  15.826837   \n",
       "2  6620172  0.001970  0.999982  42.543549  0.711831  0.264044  12.668289   \n",
       "3  4502983  0.001992  0.999995  15.797983  0.507167  0.048741   2.586148   \n",
       "4  6703161  0.000750  0.999992  11.153282  0.314592  0.042594   2.986686   \n",
       "\n",
       "          7            8         9   ...            41            42  \\\n",
       "0   6.968715  2934.679199  0.048262  ...  17462.171875  10766.777344   \n",
       "1  11.026155  2551.849609  0.106709  ...   7694.305664  38083.234375   \n",
       "2   2.365929  5646.794434  0.035337  ...  80296.695312  18222.199219   \n",
       "3   6.734583  2136.523438  0.027997  ...  25081.158203  12336.913086   \n",
       "4  13.142979  8152.657715  0.016565  ...  55539.929688  20167.025391   \n",
       "\n",
       "         43        44        45   46  47  48           49        50  \n",
       "0  3.868153  2.994936  2.947038  261 -99   0  1797.798950  0.238707  \n",
       "1  3.437981  2.975132  3.747602  231 -99   0  1727.854980  0.662632  \n",
       "2  3.821355  3.950199  3.454009  186 -99   0  1864.751953  0.209850  \n",
       "3  3.655804  3.555438  3.397589  268   1   1  1762.937744  0.254817  \n",
       "4  2.847753  3.169065  2.935316  188   1   1  1780.325562  0.883157  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop([0, 47, 48, 49, 50], axis=1)\n",
    "y_train = train[48]\n",
    "\n",
    "X_test = test.drop([0, 47, 48, 49, 50], axis=1)\n",
    "y_test = test[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = X_train.align(X_test, join='outer', fill_value=0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33776, 46)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33776, 46)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "            18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "            35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "            18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "            35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46],\n",
       "           dtype='int64')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tuning on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.172978</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.004101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153045</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.163933</td>\n",
       "      <td>0.004897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.148056</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.159077</td>\n",
       "      <td>0.004323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.145303</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.155436</td>\n",
       "      <td>0.004247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.142268</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.152327</td>\n",
       "      <td>0.004475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.140640</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.151409</td>\n",
       "      <td>0.004321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.138071</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.149751</td>\n",
       "      <td>0.004280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.137094</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.148744</td>\n",
       "      <td>0.004191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.136332</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.147827</td>\n",
       "      <td>0.003618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.135422</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.145962</td>\n",
       "      <td>0.004306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.134704</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.145044</td>\n",
       "      <td>0.003970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.133401</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.144126</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.142971</td>\n",
       "      <td>0.003480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.130899</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.142201</td>\n",
       "      <td>0.004253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.130152</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.141165</td>\n",
       "      <td>0.003985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.128960</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.140751</td>\n",
       "      <td>0.003799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.127672</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.140011</td>\n",
       "      <td>0.004747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.127028</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.139951</td>\n",
       "      <td>0.004196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.126428</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.138886</td>\n",
       "      <td>0.004660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.125844</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.138441</td>\n",
       "      <td>0.004517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.124956</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.137020</td>\n",
       "      <td>0.004576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.124475</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.136547</td>\n",
       "      <td>0.004547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.123416</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.136191</td>\n",
       "      <td>0.004440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.122816</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.135569</td>\n",
       "      <td>0.004532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.122069</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.134652</td>\n",
       "      <td>0.004779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.121158</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.134296</td>\n",
       "      <td>0.004418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.120448</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.133941</td>\n",
       "      <td>0.004431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.119649</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.133764</td>\n",
       "      <td>0.004975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.119020</td>\n",
       "      <td>0.000412</td>\n",
       "      <td>0.133349</td>\n",
       "      <td>0.004318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.118124</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.132639</td>\n",
       "      <td>0.005174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>0.053944</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.108332</td>\n",
       "      <td>0.004014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.053840</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.108183</td>\n",
       "      <td>0.003834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.053618</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.108272</td>\n",
       "      <td>0.003827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>0.053396</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.108391</td>\n",
       "      <td>0.003829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0.053240</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.108272</td>\n",
       "      <td>0.004055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>0.052989</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.108272</td>\n",
       "      <td>0.004187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.052930</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>0.108420</td>\n",
       "      <td>0.003893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.052826</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.108332</td>\n",
       "      <td>0.003992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.052656</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.108213</td>\n",
       "      <td>0.003869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>0.052441</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.108006</td>\n",
       "      <td>0.003740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>0.052138</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.108272</td>\n",
       "      <td>0.003776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>0.051997</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.108095</td>\n",
       "      <td>0.003672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0.051923</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.108124</td>\n",
       "      <td>0.003538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0.051805</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.108183</td>\n",
       "      <td>0.003397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0.051575</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.108154</td>\n",
       "      <td>0.003099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.051338</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.107858</td>\n",
       "      <td>0.003144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.051294</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.107858</td>\n",
       "      <td>0.003511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.051101</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.108006</td>\n",
       "      <td>0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.050849</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.107710</td>\n",
       "      <td>0.003455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.050701</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.107710</td>\n",
       "      <td>0.003579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.050664</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.107651</td>\n",
       "      <td>0.003473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.050354</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.107799</td>\n",
       "      <td>0.003515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>0.050206</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.107828</td>\n",
       "      <td>0.003637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.049954</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.107473</td>\n",
       "      <td>0.003662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>0.049769</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.107650</td>\n",
       "      <td>0.003704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>0.049651</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.107562</td>\n",
       "      <td>0.003937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.049495</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.108094</td>\n",
       "      <td>0.003759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>0.049569</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.107561</td>\n",
       "      <td>0.003821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.049480</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.107443</td>\n",
       "      <td>0.003843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.049288</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.107265</td>\n",
       "      <td>0.003801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.172978         0.002216         0.183000        0.004101\n",
       "1            0.153045         0.002330         0.163933        0.004897\n",
       "2            0.148056         0.002379         0.159077        0.004323\n",
       "3            0.145303         0.001870         0.155436        0.004247\n",
       "4            0.142268         0.001531         0.152327        0.004475\n",
       "5            0.140640         0.000999         0.151409        0.004321\n",
       "6            0.138071         0.001499         0.149751        0.004280\n",
       "7            0.137094         0.001291         0.148744        0.004191\n",
       "8            0.136332         0.001200         0.147827        0.003618\n",
       "9            0.135422         0.001097         0.145962        0.004306\n",
       "10           0.134704         0.000672         0.145044        0.003970\n",
       "11           0.133401         0.000237         0.144126        0.004000\n",
       "12           0.132231         0.000748         0.142971        0.003480\n",
       "13           0.130899         0.000784         0.142201        0.004253\n",
       "14           0.130152         0.000478         0.141165        0.003985\n",
       "15           0.128960         0.000687         0.140751        0.003799\n",
       "16           0.127672         0.000653         0.140011        0.004747\n",
       "17           0.127028         0.000610         0.139951        0.004196\n",
       "18           0.126428         0.000508         0.138886        0.004660\n",
       "19           0.125844         0.000679         0.138441        0.004517\n",
       "20           0.124956         0.000608         0.137020        0.004576\n",
       "21           0.124475         0.000705         0.136547        0.004547\n",
       "22           0.123416         0.000394         0.136191        0.004440\n",
       "23           0.122816         0.000532         0.135569        0.004532\n",
       "24           0.122069         0.000458         0.134652        0.004779\n",
       "25           0.121158         0.000441         0.134296        0.004418\n",
       "26           0.120448         0.000405         0.133941        0.004431\n",
       "27           0.119649         0.000565         0.133764        0.004975\n",
       "28           0.119020         0.000412         0.133349        0.004318\n",
       "29           0.118124         0.000455         0.132639        0.005174\n",
       "..                ...              ...              ...             ...\n",
       "223          0.053944         0.000854         0.108332        0.004014\n",
       "224          0.053840         0.000822         0.108183        0.003834\n",
       "225          0.053618         0.000794         0.108272        0.003827\n",
       "226          0.053396         0.000935         0.108391        0.003829\n",
       "227          0.053240         0.000992         0.108272        0.004055\n",
       "228          0.052989         0.000936         0.108272        0.004187\n",
       "229          0.052930         0.000899         0.108420        0.003893\n",
       "230          0.052826         0.001015         0.108332        0.003992\n",
       "231          0.052656         0.000928         0.108213        0.003869\n",
       "232          0.052441         0.001009         0.108006        0.003740\n",
       "233          0.052138         0.000971         0.108272        0.003776\n",
       "234          0.051997         0.000976         0.108095        0.003672\n",
       "235          0.051923         0.000987         0.108124        0.003538\n",
       "236          0.051805         0.000941         0.108183        0.003397\n",
       "237          0.051575         0.000942         0.108154        0.003099\n",
       "238          0.051338         0.000809         0.107858        0.003144\n",
       "239          0.051294         0.000815         0.107858        0.003511\n",
       "240          0.051101         0.000822         0.108006        0.003648\n",
       "241          0.050849         0.000683         0.107710        0.003455\n",
       "242          0.050701         0.000659         0.107710        0.003579\n",
       "243          0.050664         0.000605         0.107651        0.003473\n",
       "244          0.050354         0.000635         0.107799        0.003515\n",
       "245          0.050206         0.000635         0.107828        0.003637\n",
       "246          0.049954         0.000619         0.107473        0.003662\n",
       "247          0.049769         0.000620         0.107650        0.003704\n",
       "248          0.049651         0.000665         0.107562        0.003937\n",
       "249          0.049495         0.000549         0.108094        0.003759\n",
       "250          0.049569         0.000588         0.107561        0.003821\n",
       "251          0.049480         0.000616         0.107443        0.003843\n",
       "252          0.049288         0.000620         0.107265        0.003801\n",
       "\n",
       "[253 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'max_depth': 9, 'min_child_weight': 51}\n",
      "Run 0 best score:  0.8930897678825201\n",
      "Run 1 best param:  {'max_depth': 7, 'min_child_weight': 51}\n",
      "Run 1 best score:  0.8925568450971104\n",
      "Run 2 best param:  {'max_depth': 7, 'min_child_weight': 51}\n",
      "Run 2 best score:  0.8928233064898152\n",
      "Run 3 best param:  {'max_depth': 9, 'min_child_weight': 51}\n",
      "Run 3 best score:  0.8926160587399337\n",
      "Run 4 best param:  {'max_depth': 7, 'min_child_weight': 51}\n",
      "Run 4 best score:  0.8929417337754618\n",
      "Run 5 best param:  {'max_depth': 9, 'min_child_weight': 51}\n",
      "Run 5 best score:  0.8930009474182852\n",
      "Best params:  params               {'max_depth': 9, 'min_child_weight': 51}\n",
      "mean_test_score_0                                     0.89309\n",
      "mean_test_score_1                                    0.892409\n",
      "mean_test_score_2                                    0.892142\n",
      "mean_test_score_3                                    0.892616\n",
      "mean_test_score_4                                    0.891757\n",
      "mean_test_score_5                                    0.893001\n",
      "avg                                                  0.892503\n",
      "Name: 25, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1 = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1,300,50)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=252,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1.fit(X_train,y_train)    \n",
    "    if grid_score1.empty:\n",
    "        grid_score1 = pd.DataFrame(gsearch1.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1.best_score_)\n",
    "\n",
    "grid_score1['avg'] = grid_score1.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1.loc[grid_score1.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'max_depth': 7, 'min_child_weight': 49}\n",
      "Run 0 best score:  0.8932081951681667\n",
      "Run 1 best param:  {'max_depth': 9, 'min_child_weight': 47}\n",
      "Run 1 best score:  0.8928825201326386\n",
      "Run 2 best param:  {'max_depth': 9, 'min_child_weight': 53}\n",
      "Run 2 best score:  0.8933266224538133\n",
      "Run 3 best param:  {'max_depth': 7, 'min_child_weight': 49}\n",
      "Run 3 best score:  0.8926160587399337\n",
      "Run 4 best param:  {'max_depth': 9, 'min_child_weight': 49}\n",
      "Run 4 best score:  0.8935338702036949\n",
      "Run 5 best param:  {'max_depth': 7, 'min_child_weight': 53}\n",
      "Run 5 best score:  0.8938299384178114\n",
      "Best params:  params               {'max_depth': 9, 'min_child_weight': 45}\n",
      "mean_test_score_0                                    0.892912\n",
      "mean_test_score_1                                    0.892409\n",
      "mean_test_score_2                                    0.893031\n",
      "mean_test_score_3                                    0.891965\n",
      "mean_test_score_4                                    0.893267\n",
      "mean_test_score_5                                    0.893475\n",
      "avg                                                  0.892843\n",
      "Name: 24, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1b = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(45,56,2)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1b = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=252,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1b = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1b,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1b.fit(X_train,y_train)    \n",
    "    if grid_score1b.empty:\n",
    "        grid_score1b = pd.DataFrame(gsearch1b.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1b.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1b['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1b.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1b.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1b.best_score_)\n",
    "\n",
    "grid_score1b['avg'] = grid_score1b.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1b.loc[grid_score1b.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'max_depth': 9, 'min_child_weight': 46}\n",
      "Run 0 best score:  0.8934450497394599\n",
      "Run 1 best param:  {'max_depth': 10, 'min_child_weight': 46}\n",
      "Run 1 best score:  0.8929417337754618\n",
      "Run 2 best param:  {'max_depth': 10, 'min_child_weight': 46}\n",
      "Run 2 best score:  0.8938595452392231\n",
      "Run 3 best param:  {'max_depth': 9, 'min_child_weight': 45}\n",
      "Run 3 best score:  0.8919647086688773\n",
      "Run 4 best param:  {'max_depth': 10, 'min_child_weight': 45}\n",
      "Run 4 best score:  0.8934450497394599\n",
      "Run 5 best param:  {'max_depth': 10, 'min_child_weight': 45}\n",
      "Run 5 best score:  0.8937707247749882\n",
      "Best params:  params               {'max_depth': 9, 'min_child_weight': 45}\n",
      "mean_test_score_0                                    0.892912\n",
      "mean_test_score_1                                    0.892409\n",
      "mean_test_score_2                                    0.893031\n",
      "mean_test_score_3                                    0.891965\n",
      "mean_test_score_4                                    0.893267\n",
      "mean_test_score_5                                    0.893475\n",
      "avg                                                  0.892843\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1b = {\n",
    " 'max_depth':[9, 10],\n",
    " 'min_child_weight':[45, 46]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1b = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=252,\n",
    "        max_depth=5,\n",
    "        min_child_weight=45,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1b = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1b,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1b.fit(X_train,y_train)    \n",
    "    if grid_score1b.empty:\n",
    "        grid_score1b = pd.DataFrame(gsearch1b.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1b.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1b['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1b.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1b.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1b.best_score_)\n",
    "\n",
    "grid_score1b['avg'] = grid_score1b.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1b.loc[grid_score1b.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'gamma': 0.4}\n",
      "Run 0 best score:  0.893948365703458\n",
      "Run 1 best param:  {'gamma': 0.4}\n",
      "Run 1 best score:  0.8926160587399337\n",
      "Run 2 best param:  {'gamma': 0.0}\n",
      "Run 2 best score:  0.8930305542396968\n",
      "Run 3 best param:  {'gamma': 0.1}\n",
      "Run 3 best score:  0.8924680246328754\n",
      "Run 4 best param:  {'gamma': 0.2}\n",
      "Run 4 best score:  0.8942740407389863\n",
      "Run 5 best param:  {'gamma': 0.4}\n",
      "Run 5 best score:  0.8936522974893415\n",
      "Best params:  params               {'gamma': 0.4}\n",
      "mean_test_score_0          0.893948\n",
      "mean_test_score_1          0.892616\n",
      "mean_test_score_2          0.892498\n",
      "mean_test_score_3          0.891905\n",
      "mean_test_score_4          0.894274\n",
      "mean_test_score_5          0.893652\n",
      "avg                        0.893149\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score3 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=252,\n",
    "        max_depth=9,\n",
    "        min_child_weight=45,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch3 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test3,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch3.fit(X_train,y_train)    \n",
    "    if grid_score3.empty:\n",
    "        grid_score3 = pd.DataFrame(gsearch3.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score3.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score3['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch3.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch3.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch3.best_score_)\n",
    "\n",
    "grid_score3['avg'] = grid_score3.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score3.loc[grid_score3.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recablirating the n_estimators and 1st tune the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.167375</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.176012</td>\n",
       "      <td>0.003202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.149670</td>\n",
       "      <td>0.004855</td>\n",
       "      <td>0.159374</td>\n",
       "      <td>0.002220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.143985</td>\n",
       "      <td>0.002813</td>\n",
       "      <td>0.154488</td>\n",
       "      <td>0.003537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.139715</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.152031</td>\n",
       "      <td>0.003386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.137509</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.149011</td>\n",
       "      <td>0.002720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.135126</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>0.146791</td>\n",
       "      <td>0.002856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.133046</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.145103</td>\n",
       "      <td>0.004105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.131573</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.143208</td>\n",
       "      <td>0.003325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.130211</td>\n",
       "      <td>0.001225</td>\n",
       "      <td>0.142261</td>\n",
       "      <td>0.002701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.129071</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.140929</td>\n",
       "      <td>0.003406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.128427</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.140455</td>\n",
       "      <td>0.003531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.127220</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.139004</td>\n",
       "      <td>0.002861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.126066</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.138323</td>\n",
       "      <td>0.002892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.124645</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>0.136843</td>\n",
       "      <td>0.003071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.123557</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.136102</td>\n",
       "      <td>0.003827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.122794</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>0.135747</td>\n",
       "      <td>0.004144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.121529</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.135007</td>\n",
       "      <td>0.003918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.120700</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.133882</td>\n",
       "      <td>0.003756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.119886</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.133438</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.118894</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.133171</td>\n",
       "      <td>0.003662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.118168</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>0.132372</td>\n",
       "      <td>0.003126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.117509</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.132520</td>\n",
       "      <td>0.002875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.116681</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.131661</td>\n",
       "      <td>0.002893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.115755</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>0.130921</td>\n",
       "      <td>0.002676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.115045</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.130448</td>\n",
       "      <td>0.002721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.114290</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.129649</td>\n",
       "      <td>0.002476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.113461</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.129293</td>\n",
       "      <td>0.002587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.112654</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.128612</td>\n",
       "      <td>0.002369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.111810</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.127665</td>\n",
       "      <td>0.002774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.111203</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.128079</td>\n",
       "      <td>0.003025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.046660</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.003199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.046549</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.105548</td>\n",
       "      <td>0.003338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.046460</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.105341</td>\n",
       "      <td>0.003523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.046275</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.105371</td>\n",
       "      <td>0.003265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.046349</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.105548</td>\n",
       "      <td>0.003518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.046120</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.105311</td>\n",
       "      <td>0.003312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.046031</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.105460</td>\n",
       "      <td>0.003646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.045831</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.105756</td>\n",
       "      <td>0.003485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.045824</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.105845</td>\n",
       "      <td>0.003322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.045720</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.105578</td>\n",
       "      <td>0.003237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.045735</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.105844</td>\n",
       "      <td>0.003287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.045587</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.105756</td>\n",
       "      <td>0.003001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.045380</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.105637</td>\n",
       "      <td>0.003223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.045202</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.105637</td>\n",
       "      <td>0.003208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.045269</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.105785</td>\n",
       "      <td>0.003301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.045195</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.105696</td>\n",
       "      <td>0.003509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.045069</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.003518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.044817</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.105489</td>\n",
       "      <td>0.003540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0.044795</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.105608</td>\n",
       "      <td>0.003607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.044610</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.105608</td>\n",
       "      <td>0.003726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>0.044381</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.105371</td>\n",
       "      <td>0.003516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>0.044255</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.105608</td>\n",
       "      <td>0.003520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>0.044026</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.105933</td>\n",
       "      <td>0.003635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.043981</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.105667</td>\n",
       "      <td>0.003602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>0.043937</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.105608</td>\n",
       "      <td>0.003611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>0.043811</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.105489</td>\n",
       "      <td>0.003414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0.043715</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.105578</td>\n",
       "      <td>0.003669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>0.043574</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.105252</td>\n",
       "      <td>0.003529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>0.043485</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.105104</td>\n",
       "      <td>0.003663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>0.043426</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.104956</td>\n",
       "      <td>0.003452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.167375         0.002291         0.176012        0.003202\n",
       "1            0.149670         0.004855         0.159374        0.002220\n",
       "2            0.143985         0.002813         0.154488        0.003537\n",
       "3            0.139715         0.002060         0.152031        0.003386\n",
       "4            0.137509         0.001978         0.149011        0.002720\n",
       "5            0.135126         0.002682         0.146791        0.002856\n",
       "6            0.133046         0.001904         0.145103        0.004105\n",
       "7            0.131573         0.001386         0.143208        0.003325\n",
       "8            0.130211         0.001225         0.142261        0.002701\n",
       "9            0.129071         0.000942         0.140929        0.003406\n",
       "10           0.128427         0.000960         0.140455        0.003531\n",
       "11           0.127220         0.000574         0.139004        0.002861\n",
       "12           0.126066         0.000685         0.138323        0.002892\n",
       "13           0.124645         0.000296         0.136843        0.003071\n",
       "14           0.123557         0.000627         0.136102        0.003827\n",
       "15           0.122794         0.000854         0.135747        0.004144\n",
       "16           0.121529         0.001177         0.135007        0.003918\n",
       "17           0.120700         0.001086         0.133882        0.003756\n",
       "18           0.119886         0.001037         0.133438        0.003488\n",
       "19           0.118894         0.000912         0.133171        0.003662\n",
       "20           0.118168         0.000730         0.132372        0.003126\n",
       "21           0.117509         0.000749         0.132520        0.002875\n",
       "22           0.116681         0.000923         0.131661        0.002893\n",
       "23           0.115755         0.000742         0.130921        0.002676\n",
       "24           0.115045         0.000826         0.130448        0.002721\n",
       "25           0.114290         0.000740         0.129649        0.002476\n",
       "26           0.113461         0.001004         0.129293        0.002587\n",
       "27           0.112654         0.001235         0.128612        0.002369\n",
       "28           0.111810         0.000903         0.127665        0.002774\n",
       "29           0.111203         0.000740         0.128079        0.003025\n",
       "..                ...              ...              ...             ...\n",
       "286          0.046660         0.000881         0.105400        0.003199\n",
       "287          0.046549         0.000925         0.105548        0.003338\n",
       "288          0.046460         0.000856         0.105341        0.003523\n",
       "289          0.046275         0.000911         0.105371        0.003265\n",
       "290          0.046349         0.000842         0.105548        0.003518\n",
       "291          0.046120         0.000988         0.105311        0.003312\n",
       "292          0.046031         0.000918         0.105460        0.003646\n",
       "293          0.045831         0.001010         0.105756        0.003485\n",
       "294          0.045824         0.000979         0.105845        0.003322\n",
       "295          0.045720         0.000992         0.105578        0.003237\n",
       "296          0.045735         0.000960         0.105844        0.003287\n",
       "297          0.045587         0.000955         0.105756        0.003001\n",
       "298          0.045380         0.000825         0.105637        0.003223\n",
       "299          0.045202         0.000804         0.105637        0.003208\n",
       "300          0.045269         0.000898         0.105785        0.003301\n",
       "301          0.045195         0.000863         0.105696        0.003509\n",
       "302          0.045069         0.000796         0.105400        0.003518\n",
       "303          0.044817         0.000848         0.105489        0.003540\n",
       "304          0.044795         0.000869         0.105608        0.003607\n",
       "305          0.044610         0.000931         0.105608        0.003726\n",
       "306          0.044381         0.000964         0.105371        0.003516\n",
       "307          0.044255         0.000910         0.105608        0.003520\n",
       "308          0.044026         0.000928         0.105933        0.003635\n",
       "309          0.043981         0.000981         0.105667        0.003602\n",
       "310          0.043937         0.000829         0.105608        0.003611\n",
       "311          0.043811         0.000786         0.105489        0.003414\n",
       "312          0.043715         0.000754         0.105578        0.003669\n",
       "313          0.043574         0.000739         0.105252        0.003529\n",
       "314          0.043485         0.000877         0.105104        0.003663\n",
       "315          0.043426         0.000822         0.104956        0.003452\n",
       "\n",
       "[316 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=252,\n",
    "    max_depth=9,\n",
    "    min_child_weight=45,\n",
    "    gamma=0.4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    seed=0\n",
    "    )\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'n_estimators': 315}\n",
      "Run 0 best score:  0.8941852202747513\n",
      "Run 1 best param:  {'n_estimators': 300}\n",
      "Run 1 best score:  0.8925272382756987\n",
      "Run 2 best param:  {'n_estimators': 500}\n",
      "Run 2 best score:  0.8927344860255803\n",
      "Run 3 best param:  {'n_estimators': 500}\n",
      "Run 3 best score:  0.8929121269540502\n",
      "Run 4 best param:  {'n_estimators': 400}\n",
      "Run 4 best score:  0.8949549976314543\n",
      "Run 5 best param:  {'n_estimators': 315}\n",
      "Run 5 best score:  0.8939187588820464\n",
      "Best params:  params               {'n_estimators': 400}\n",
      "mean_test_score_0                   0.8938\n",
      "mean_test_score_1                 0.892231\n",
      "mean_test_score_2                 0.892468\n",
      "mean_test_score_3                 0.892438\n",
      "mean_test_score_4                 0.894955\n",
      "mean_test_score_5                 0.893593\n",
      "avg                               0.893248\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(100, 1000, 100)]+[315]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=315,\n",
    "        max_depth=9,\n",
    "        min_child_weight=45,\n",
    "        gamma=0.4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'n_estimators': 400}\n",
      "Run 0 best score:  0.8938003315963998\n",
      "Run 1 best param:  {'n_estimators': 350}\n",
      "Run 1 best score:  0.8923792041686405\n",
      "Run 2 best param:  {'n_estimators': 430}\n",
      "Run 2 best score:  0.8927936996684036\n",
      "Run 3 best param:  {'n_estimators': 440}\n",
      "Run 3 best score:  0.8926160587399337\n",
      "Run 4 best param:  {'n_estimators': 400}\n",
      "Run 4 best score:  0.8949549976314543\n",
      "Run 5 best param:  {'n_estimators': 430}\n",
      "Run 5 best score:  0.8936522974893415\n",
      "Best params:  params               {'n_estimators': 400}\n",
      "mean_test_score_0                   0.8938\n",
      "mean_test_score_1                 0.892231\n",
      "mean_test_score_2                 0.892468\n",
      "mean_test_score_3                 0.892438\n",
      "mean_test_score_4                 0.894955\n",
      "mean_test_score_5                 0.893593\n",
      "avg                               0.893248\n",
      "Name: 5, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(350, 451, 10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=315,\n",
    "        max_depth=9,\n",
    "        min_child_weight=45,\n",
    "        gamma=0.4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "Run 0 best score:  0.8945405021316911\n",
      "Run 1 best param:  {'colsample_bytree': 0.8, 'subsample': 0.9}\n",
      "Run 1 best score:  0.8935338702036949\n",
      "Run 2 best param:  {'colsample_bytree': 0.9, 'subsample': 0.9}\n",
      "Run 2 best score:  0.8940963998105164\n",
      "Run 3 best param:  {'colsample_bytree': 0.8, 'subsample': 0.7}\n",
      "Run 3 best score:  0.8928825201326386\n",
      "Run 4 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 4 best score:  0.8949549976314543\n",
      "Run 5 best param:  {'colsample_bytree': 0.6, 'subsample': 0.9}\n",
      "Run 5 best score:  0.8938891520606348\n",
      "Best params:  params               {'colsample_bytree': 0.6, 'subsample': 0.9}\n",
      "mean_test_score_0                                       0.893238\n",
      "mean_test_score_1                                       0.893149\n",
      "mean_test_score_2                                       0.893534\n",
      "mean_test_score_3                                        0.89232\n",
      "mean_test_score_4                                       0.893919\n",
      "mean_test_score_5                                       0.893889\n",
      "avg                                                     0.893341\n",
      "Name: 3, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score4 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=400,\n",
    "        max_depth=9,\n",
    "        min_child_weight=45,\n",
    "        gamma=0.4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test4,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch4.fit(X_train,y_train)    \n",
    "    if grid_score4.empty:\n",
    "        grid_score4 = pd.DataFrame(gsearch4.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score4.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score4['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch4.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch4.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch4.best_score_)\n",
    "\n",
    "grid_score4['avg'] = grid_score4.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score4.loc[grid_score4.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'colsample_bytree': 0.65, 'subsample': 0.9}\n",
      "Run 0 best score:  0.8943924680246329\n",
      "Run 1 best param:  {'colsample_bytree': 0.6, 'subsample': 0.95}\n",
      "Run 1 best score:  0.8939187588820464\n",
      "Run 2 best param:  {'colsample_bytree': 0.65, 'subsample': 0.95}\n",
      "Run 2 best score:  0.8945701089531028\n",
      "Run 3 best param:  {'colsample_bytree': 0.6, 'subsample': 0.85}\n",
      "Run 3 best score:  0.8937115111321648\n",
      "Run 4 best param:  {'colsample_bytree': 0.6, 'subsample': 0.9}\n",
      "Run 4 best score:  0.8939187588820464\n",
      "Run 5 best param:  {'colsample_bytree': 0.55, 'subsample': 0.95}\n",
      "Run 5 best score:  0.8940075793462814\n",
      "Best params:  params               {'colsample_bytree': 0.65, 'subsample': 0.9}\n",
      "mean_test_score_0                                        0.894392\n",
      "mean_test_score_1                                        0.892942\n",
      "mean_test_score_2                                        0.893948\n",
      "mean_test_score_3                                        0.892616\n",
      "mean_test_score_4                                        0.893623\n",
      "mean_test_score_5                                        0.893504\n",
      "avg                                                      0.893504\n",
      "Name: 7, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Carefully search for each neighboring 0.05\n",
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(85,100,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(55,70,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score5 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=400,\n",
    "        max_depth=9,\n",
    "        min_child_weight=45,\n",
    "        gamma=0.4,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch5 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test5,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch5.fit(X_train,y_train)    \n",
    "    if grid_score5.empty:\n",
    "        grid_score5 = pd.DataFrame(gsearch5.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score5.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score5['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch5.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch5.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch5.best_score_)\n",
    "\n",
    "grid_score5['avg'] = grid_score5.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score5.loc[grid_score5.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'reg_alpha': 0.01}\n",
      "Run 0 best score:  0.8946885362387494\n",
      "Run 1 best param:  {'reg_alpha': 1}\n",
      "Run 1 best score:  0.8940667929891047\n",
      "Run 2 best param:  {'reg_alpha': 1}\n",
      "Run 2 best score:  0.8945108953102795\n",
      "Run 3 best param:  {'reg_alpha': 0.1}\n",
      "Run 3 best score:  0.8933858360966367\n",
      "Run 4 best param:  {'reg_alpha': 0}\n",
      "Run 4 best score:  0.8936226906679299\n",
      "Run 5 best param:  {'reg_alpha': 1}\n",
      "Run 5 best score:  0.8939187588820464\n",
      "Best params:  params               {'reg_alpha': 1}\n",
      "mean_test_score_0            0.893682\n",
      "mean_test_score_1            0.894067\n",
      "mean_test_score_2            0.894511\n",
      "mean_test_score_3            0.892883\n",
      "mean_test_score_4            0.893238\n",
      "mean_test_score_5            0.893919\n",
      "avg                          0.893716\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[0, 1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score6 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=400,\n",
    "        max_depth=9,\n",
    "        min_child_weight=45,\n",
    "        gamma=0.4,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.65,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch6 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test6,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch6.fit(X_train,y_train)    \n",
    "    if grid_score6.empty:\n",
    "        grid_score6 = pd.DataFrame(gsearch6.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score6.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score6['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch6.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch6.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch6.best_score_)\n",
    "\n",
    "grid_score6['avg'] = grid_score6.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score6.loc[grid_score6.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'reg_alpha': 3}\n",
      "Run 0 best score:  0.8951622453813358\n",
      "Run 1 best param:  {'reg_alpha': 1}\n",
      "Run 1 best score:  0.8940667929891047\n",
      "Run 2 best param:  {'reg_alpha': 1}\n",
      "Run 2 best score:  0.8945108953102795\n",
      "Run 3 best param:  {'reg_alpha': 7}\n",
      "Run 3 best score:  0.8931785883467551\n",
      "Run 4 best param:  {'reg_alpha': 9}\n",
      "Run 4 best score:  0.8945405021316911\n",
      "Run 5 best param:  {'reg_alpha': 9}\n",
      "Run 5 best score:  0.8940075793462814\n",
      "Best params:  params               {'reg_alpha': 3}\n",
      "mean_test_score_0            0.895162\n",
      "mean_test_score_1            0.893267\n",
      "mean_test_score_2            0.893712\n",
      "mean_test_score_3            0.892942\n",
      "mean_test_score_4            0.894067\n",
      "mean_test_score_5            0.893475\n",
      "avg                          0.893771\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':range(1, 10)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score7 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=400,\n",
    "        max_depth=9,\n",
    "        min_child_weight=45,\n",
    "        gamma=0.4,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.65,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch7 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test7,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch7.fit(X_train,y_train)    \n",
    "    if grid_score7.empty:\n",
    "        grid_score7 = pd.DataFrame(gsearch7.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score7.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score7['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch7.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch7.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch7.best_score_)\n",
    "\n",
    "grid_score7['avg'] = grid_score7.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score7.loc[grid_score7.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the learning rate and tune n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.168552</td>\n",
       "      <td>0.006524</td>\n",
       "      <td>0.179062</td>\n",
       "      <td>0.005558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.150491</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.159936</td>\n",
       "      <td>0.004172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.144955</td>\n",
       "      <td>0.003038</td>\n",
       "      <td>0.155377</td>\n",
       "      <td>0.003556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.142698</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.152919</td>\n",
       "      <td>0.004687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.140914</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>0.151439</td>\n",
       "      <td>0.004636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.139278</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>0.149011</td>\n",
       "      <td>0.005123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.138138</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.148093</td>\n",
       "      <td>0.005377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.137568</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.146613</td>\n",
       "      <td>0.005416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.137035</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.147146</td>\n",
       "      <td>0.005935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.136694</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.146494</td>\n",
       "      <td>0.005112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.136532</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.146909</td>\n",
       "      <td>0.004882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.136421</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>0.146524</td>\n",
       "      <td>0.004789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.136236</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.146317</td>\n",
       "      <td>0.004971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.135903</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.146021</td>\n",
       "      <td>0.005078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.135488</td>\n",
       "      <td>0.001796</td>\n",
       "      <td>0.146287</td>\n",
       "      <td>0.004403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.135681</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>0.146524</td>\n",
       "      <td>0.004552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.134918</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.145429</td>\n",
       "      <td>0.004705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.134378</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.145281</td>\n",
       "      <td>0.005014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.134200</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.145547</td>\n",
       "      <td>0.005449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.133838</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.144984</td>\n",
       "      <td>0.005164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.133623</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.144659</td>\n",
       "      <td>0.005282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.133223</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.143919</td>\n",
       "      <td>0.005437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.133157</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.144126</td>\n",
       "      <td>0.004855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.133038</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.144511</td>\n",
       "      <td>0.005421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.132964</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.144837</td>\n",
       "      <td>0.005542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.133090</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.144777</td>\n",
       "      <td>0.005033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.132853</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.144659</td>\n",
       "      <td>0.004542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.132713</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>0.144481</td>\n",
       "      <td>0.004603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.132120</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.144718</td>\n",
       "      <td>0.004667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.131921</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.144452</td>\n",
       "      <td>0.004750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>0.074720</td>\n",
       "      <td>0.000455</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>0.003139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>0.074669</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.110463</td>\n",
       "      <td>0.003151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>0.074713</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.110404</td>\n",
       "      <td>0.003133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113</th>\n",
       "      <td>0.074691</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>0.003146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1114</th>\n",
       "      <td>0.074676</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.110493</td>\n",
       "      <td>0.003204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>0.074617</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.110493</td>\n",
       "      <td>0.003171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>0.074602</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>0.003163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>0.074565</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.110404</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1118</th>\n",
       "      <td>0.074565</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.110493</td>\n",
       "      <td>0.003209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>0.074528</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.110552</td>\n",
       "      <td>0.003199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1120</th>\n",
       "      <td>0.074521</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.110493</td>\n",
       "      <td>0.003197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121</th>\n",
       "      <td>0.074469</td>\n",
       "      <td>0.000460</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>0.003186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>0.074446</td>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.110404</td>\n",
       "      <td>0.003180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1123</th>\n",
       "      <td>0.074454</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>0.003309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124</th>\n",
       "      <td>0.074469</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.110404</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>0.074454</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.110404</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>0.074402</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>0.003346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>0.074409</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.110493</td>\n",
       "      <td>0.003439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1128</th>\n",
       "      <td>0.074343</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.110404</td>\n",
       "      <td>0.003368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1129</th>\n",
       "      <td>0.074328</td>\n",
       "      <td>0.000497</td>\n",
       "      <td>0.110345</td>\n",
       "      <td>0.003283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>0.074313</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.110404</td>\n",
       "      <td>0.003374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>0.074283</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.110463</td>\n",
       "      <td>0.003428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>0.074291</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.110463</td>\n",
       "      <td>0.003466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1133</th>\n",
       "      <td>0.074283</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>0.003502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1134</th>\n",
       "      <td>0.074224</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.110374</td>\n",
       "      <td>0.003478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>0.074209</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.110463</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>0.074187</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.110463</td>\n",
       "      <td>0.003549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>0.074217</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.110522</td>\n",
       "      <td>0.003603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>0.074195</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.110315</td>\n",
       "      <td>0.003509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>0.074165</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.110286</td>\n",
       "      <td>0.003462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0             0.168552         0.006524         0.179062        0.005558\n",
       "1             0.150491         0.005126         0.159936        0.004172\n",
       "2             0.144955         0.003038         0.155377        0.003556\n",
       "3             0.142698         0.003865         0.152919        0.004687\n",
       "4             0.140914         0.003461         0.151439        0.004636\n",
       "5             0.139278         0.003105         0.149011        0.005123\n",
       "6             0.138138         0.002688         0.148093        0.005377\n",
       "7             0.137568         0.002463         0.146613        0.005416\n",
       "8             0.137035         0.002564         0.147146        0.005935\n",
       "9             0.136694         0.003268         0.146494        0.005112\n",
       "10            0.136532         0.003304         0.146909        0.004882\n",
       "11            0.136421         0.002610         0.146524        0.004789\n",
       "12            0.136236         0.002885         0.146317        0.004971\n",
       "13            0.135903         0.002445         0.146021        0.005078\n",
       "14            0.135488         0.001796         0.146287        0.004403\n",
       "15            0.135681         0.001516         0.146524        0.004552\n",
       "16            0.134918         0.001007         0.145429        0.004705\n",
       "17            0.134378         0.000878         0.145281        0.005014\n",
       "18            0.134200         0.000604         0.145547        0.005449\n",
       "19            0.133838         0.000545         0.144984        0.005164\n",
       "20            0.133623         0.000631         0.144659        0.005282\n",
       "21            0.133223         0.000910         0.143919        0.005437\n",
       "22            0.133157         0.001041         0.144126        0.004855\n",
       "23            0.133038         0.001184         0.144511        0.005421\n",
       "24            0.132964         0.001340         0.144837        0.005542\n",
       "25            0.133090         0.001120         0.144777        0.005033\n",
       "26            0.132853         0.001169         0.144659        0.004542\n",
       "27            0.132713         0.001129         0.144481        0.004603\n",
       "28            0.132120         0.000979         0.144718        0.004667\n",
       "29            0.131921         0.001029         0.144452        0.004750\n",
       "...                ...              ...              ...             ...\n",
       "1110          0.074720         0.000455         0.110434        0.003139\n",
       "1111          0.074669         0.000485         0.110463        0.003151\n",
       "1112          0.074713         0.000501         0.110404        0.003133\n",
       "1113          0.074691         0.000461         0.110434        0.003146\n",
       "1114          0.074676         0.000505         0.110493        0.003204\n",
       "1115          0.074617         0.000514         0.110493        0.003171\n",
       "1116          0.074602         0.000539         0.110434        0.003163\n",
       "1117          0.074565         0.000489         0.110404        0.003110\n",
       "1118          0.074565         0.000436         0.110493        0.003209\n",
       "1119          0.074528         0.000500         0.110552        0.003199\n",
       "1120          0.074521         0.000469         0.110493        0.003197\n",
       "1121          0.074469         0.000460         0.110434        0.003186\n",
       "1122          0.074446         0.000535         0.110404        0.003180\n",
       "1123          0.074454         0.000512         0.110434        0.003309\n",
       "1124          0.074469         0.000438         0.110404        0.003300\n",
       "1125          0.074454         0.000467         0.110404        0.003300\n",
       "1126          0.074402         0.000490         0.110434        0.003346\n",
       "1127          0.074409         0.000517         0.110493        0.003439\n",
       "1128          0.074343         0.000510         0.110404        0.003368\n",
       "1129          0.074328         0.000497         0.110345        0.003283\n",
       "1130          0.074313         0.000514         0.110404        0.003374\n",
       "1131          0.074283         0.000515         0.110463        0.003428\n",
       "1132          0.074291         0.000515         0.110463        0.003466\n",
       "1133          0.074283         0.000530         0.110434        0.003502\n",
       "1134          0.074224         0.000511         0.110374        0.003478\n",
       "1135          0.074209         0.000508         0.110463        0.003510\n",
       "1136          0.074187         0.000480         0.110463        0.003549\n",
       "1137          0.074217         0.000509         0.110522        0.003603\n",
       "1138          0.074195         0.000487         0.110315        0.003509\n",
       "1139          0.074165         0.000471         0.110286        0.003462\n",
       "\n",
       "[1140 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=400,\n",
    "    max_depth=9,\n",
    "    min_child_weight=45,\n",
    "    gamma=0.4,\n",
    "    reg_alpha=3,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.65,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 6 times\n",
      "Run 0 best param:  {'n_estimators': 2500}\n",
      "Run 0 best score:  0.8945405021316911\n",
      "Run 1 best param:  {'n_estimators': 2500}\n",
      "Run 1 best score:  0.8931193747039318\n",
      "Run 2 best param:  {'n_estimators': 2400}\n",
      "Run 2 best score:  0.8942444339175746\n",
      "Run 3 best param:  {'n_estimators': 2400}\n",
      "Run 3 best score:  0.8940667929891047\n",
      "Run 4 best param:  {'n_estimators': 2400}\n",
      "Run 4 best score:  0.8952510658455708\n",
      "Run 5 best param:  {'n_estimators': 2500}\n",
      "Run 5 best score:  0.8945405021316911\n",
      "Best params:  params               {'n_estimators': 2500}\n",
      "mean_test_score_0                  0.894541\n",
      "mean_test_score_1                  0.893119\n",
      "mean_test_score_2                  0.894126\n",
      "mean_test_score_3                  0.894008\n",
      "mean_test_score_4                  0.895103\n",
      "mean_test_score_5                  0.894541\n",
      "avg                                0.894239\n",
      "Name: 15, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(1000, 2501, 100)]+[1139]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=1139,\n",
    "        max_depth=9,\n",
    "        min_child_weight=45,\n",
    "        gamma=0.4,\n",
    "        reg_alpha=3,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.65,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0: 89.14%\n",
      "Accuracy 1: 89.17%\n",
      "Accuracy 2: 89.15%\n",
      "Accuracy 3: 89.21%\n",
      "Accuracy 4: 89.14%\n",
      "Accuracy 5: 89.16%\n",
      "Average accuracy is: 89.16%\n"
     ]
    }
   ],
   "source": [
    "accuracy_array = []\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=2500,\n",
    "        max_depth=9,\n",
    "        min_child_weight=45,\n",
    "        gamma=0.4,\n",
    "        reg_alpha=3,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.65,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        seed=i\n",
    "    )\n",
    "    model = xgb.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_array.append(accuracy)\n",
    "    print('Accuracy {}: %.2f%%'.format(i) % (accuracy * 100.0))\n",
    "mean_accuracy_score = sum(accuracy_array) / NUM_TRIALS\n",
    "print('Average accuracy is: %.2f%%' % (mean_accuracy_score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
