{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank additional dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import xgboost\n",
    "import copy\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/tai/Projects/research-project-Roland')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/bank_additional/bank_additional.0.train\", encoding='latin1', \n",
    "                 names=['age',\n",
    "                        'job',\n",
    "                        'marital',\n",
    "                        'education',\n",
    "                        'default',\n",
    "                        'housing',\n",
    "                        'loan',\n",
    "                        'contact',\n",
    "                        'month',\n",
    "                        'day_of_week',\n",
    "                        'duration',\n",
    "                        'campaign',\n",
    "                        'pdays',\n",
    "                        'previous',\n",
    "                        'poutcome',\n",
    "                        'emp_var_rate',\n",
    "                        'cons_price_idx',\n",
    "                        'cons_conf_idx',\n",
    "                        'euribor3m',\n",
    "                        'nr_employed',\n",
    "                        'subscribe'],\n",
    "                 na_values='?',\n",
    "                 low_memory=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/bank_additional/bank_additional.0.test\", encoding='latin1', \n",
    "                 names=['age',\n",
    "                        'job',\n",
    "                        'marital',\n",
    "                        'education',\n",
    "                        'default',\n",
    "                        'housing',\n",
    "                        'loan',\n",
    "                        'contact',\n",
    "                        'month',\n",
    "                        'day_of_week',\n",
    "                        'duration',\n",
    "                        'campaign',\n",
    "                        'pdays',\n",
    "                        'previous',\n",
    "                        'poutcome',\n",
    "                        'emp_var_rate',\n",
    "                        'cons_price_idx',\n",
    "                        'cons_conf_idx',\n",
    "                        'euribor3m',\n",
    "                        'nr_employed',\n",
    "                        'subscribe'],\n",
    "                 na_values='?',\n",
    "                 low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', 2000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covert the output as binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['has_subcribe'] = np.where(train.subscribe == 'yes', 1, 0)\n",
    "train=train.drop(['subscribe'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['has_subcribe'] = np.where(test.subscribe == 'yes', 1, 0)\n",
    "test=test.drop(['subscribe'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the numeric number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,'age'] = pd.to_numeric(train['age'], downcast='integer', errors='coerce')\n",
    "train.loc[:, 'duration'] = pd.to_numeric(train['duration'], downcast='integer', errors='coerce')\n",
    "train.loc[:, 'campaign'] = pd.to_numeric(train['campaign'], downcast='integer', errors='coerce')\n",
    "train.loc[:, 'pdays'] = pd.to_numeric(train['pdays'], downcast='integer', errors='coerce')\n",
    "train.loc[:, 'previous'] = pd.to_numeric(train['previous'], downcast='integer', errors='coerce')\n",
    "train.loc[:, 'emp_var_rate'] = pd.to_numeric(train['emp_var_rate'], downcast='float', errors='coerce')\n",
    "train.loc[:, 'cons_price_idx'] = pd.to_numeric(train['cons_price_idx'], downcast='float', errors='coerce')\n",
    "train.loc[:, 'cons_conf_idx'] = pd.to_numeric(train['cons_conf_idx'], downcast='float', errors='coerce')\n",
    "train.loc[:, 'euribor3m'] = pd.to_numeric(train['euribor3m'], downcast='float', errors='coerce')\n",
    "train.loc[:,'nr_employed'] = pd.to_numeric(train['nr_employed'], downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:,'age'] = pd.to_numeric(test['age'], downcast='integer', errors='coerce')\n",
    "test.loc[:, 'duration'] = pd.to_numeric(test['duration'], downcast='integer', errors='coerce')\n",
    "test.loc[:, 'campaign'] = pd.to_numeric(test['campaign'], downcast='integer', errors='coerce')\n",
    "test.loc[:, 'pdays'] = pd.to_numeric(test['pdays'], downcast='integer', errors='coerce')\n",
    "test.loc[:, 'previous'] = pd.to_numeric(test['previous'], downcast='integer', errors='coerce')\n",
    "test.loc[:, 'emp_var_rate'] = pd.to_numeric(test['emp_var_rate'], downcast='float', errors='coerce')\n",
    "test.loc[:, 'cons_price_idx'] = pd.to_numeric(test['cons_price_idx'], downcast='float', errors='coerce')\n",
    "test.loc[:, 'cons_conf_idx'] = pd.to_numeric(test['cons_conf_idx'], downcast='float', errors='coerce')\n",
    "test.loc[:, 'euribor3m'] = pd.to_numeric(test['euribor3m'], downcast='float', errors='coerce')\n",
    "test.loc[:,'nr_employed'] = pd.to_numeric(test['nr_employed'], downcast='float', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "job                177\n",
       "marital             37\n",
       "education          874\n",
       "default           4313\n",
       "housing            498\n",
       "loan               498\n",
       "contact              0\n",
       "month                0\n",
       "day_of_week          0\n",
       "duration             0\n",
       "campaign             0\n",
       "pdays                0\n",
       "previous             0\n",
       "poutcome             0\n",
       "emp_var_rate         0\n",
       "cons_price_idx       0\n",
       "cons_conf_idx        0\n",
       "euribor3m            0\n",
       "nr_employed          0\n",
       "has_subcribe         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  0\n",
       "job                153\n",
       "marital             43\n",
       "education          857\n",
       "default           4284\n",
       "housing            492\n",
       "loan               492\n",
       "contact              0\n",
       "month                0\n",
       "day_of_week          0\n",
       "duration             0\n",
       "campaign             0\n",
       "pdays                0\n",
       "previous             0\n",
       "poutcome             0\n",
       "emp_var_rate         0\n",
       "cons_price_idx       0\n",
       "cons_conf_idx        0\n",
       "euribor3m            0\n",
       "nr_employed          0\n",
       "has_subcribe         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20594 entries, 0 to 20593\n",
      "Data columns (total 21 columns):\n",
      "age               20594 non-null int8\n",
      "job               20417 non-null object\n",
      "marital           20557 non-null object\n",
      "education         19720 non-null object\n",
      "default           16281 non-null object\n",
      "housing           20096 non-null object\n",
      "loan              20096 non-null object\n",
      "contact           20594 non-null object\n",
      "month             20594 non-null object\n",
      "day_of_week       20594 non-null object\n",
      "duration          20594 non-null int16\n",
      "campaign          20594 non-null int8\n",
      "pdays             20594 non-null int16\n",
      "previous          20594 non-null int8\n",
      "poutcome          20594 non-null object\n",
      "emp_var_rate      20594 non-null float32\n",
      "cons_price_idx    20594 non-null float32\n",
      "cons_conf_idx     20594 non-null float32\n",
      "euribor3m         20594 non-null float32\n",
      "nr_employed       20594 non-null float32\n",
      "has_subcribe      20594 non-null int64\n",
      "dtypes: float32(5), int16(2), int64(1), int8(3), object(10)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tai/.conda/envs/research/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3291: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "train['job'] = train['job'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'admin',\n",
    "                                                 'blue-collar',\n",
    "                                                 'entrepreneur',\n",
    "                                                 'housemaid',\n",
    "                                                 'management',\n",
    "                                                 'retired',\n",
    "                                                 'self-employed',\n",
    "                                                 'services',\n",
    "                                                 'student',\n",
    "                                                 'technician',\n",
    "                                                 'unemployed'\n",
    "                                               ])\n",
    "train['marital'] = train['marital'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'divorced', 'married', 'single'\n",
    "                                               ])\n",
    "\n",
    "train['education'] = train['education'].astype('category',\n",
    "                                               categories=[\n",
    "                                                   'basic.4y',\n",
    "                                                   'basic.6y',\n",
    "                                                 'basic.9y',\n",
    "                                                 'high.school',\n",
    "                                                 'illiterate',\n",
    "                                                 'professional.course',\n",
    "                                                 'university.degree'])\n",
    "train['default'] = train['default'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'no', 'yes'\n",
    "                                               ])\n",
    "train['housing'] = train['housing'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'no', 'yes'\n",
    "                                               ])\n",
    "train['loan'] = train['loan'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'no', 'yes'\n",
    "                                               ])\n",
    "train['contact'] = train['contact'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'cellular','telephone'\n",
    "                                               ])\n",
    "train['month'] = train['month'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'jan', 'feb', 'mar', \n",
    "                                                   'apr', 'may', 'jun', \n",
    "                                                   'jul', 'aug', 'sep',\n",
    "                                                   'oct', 'nov', 'dec'\n",
    "                                               ])\n",
    "train['day_of_week'] = train['day_of_week'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'mon','tue','wed','thu','fri'\n",
    "                                               ])\n",
    "train['poutcome'] = train['poutcome'].astype('category',\n",
    "                                               categories=[\n",
    "                                                'failure','nonexistent','success'\n",
    "                                               ])\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=['contact', 'month', 'day_of_week', 'poutcome'])\n",
    "train = pd.get_dummies(train, columns=['job', 'marital', 'education', 'default', 'housing', 'loan'], dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['job'] = test['job'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'admin',\n",
    "                                                 'blue-collar',\n",
    "                                                 'entrepreneur',\n",
    "                                                 'housemaid',\n",
    "                                                 'management',\n",
    "                                                 'retired',\n",
    "                                                 'self-employed',\n",
    "                                                 'services',\n",
    "                                                 'student',\n",
    "                                                 'technician',\n",
    "                                                 'unemployed'\n",
    "                                               ])\n",
    "test['marital'] = test['marital'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'divorced', 'married', 'single'\n",
    "                                               ])\n",
    "\n",
    "test['education'] = test['education'].astype('category',\n",
    "                                               categories=[\n",
    "                                                   'basic.4y',\n",
    "                                                   'basic.6y',\n",
    "                                                 'basic.9y',\n",
    "                                                 'high.school',\n",
    "                                                 'illiterate',\n",
    "                                                 'professional.course',\n",
    "                                                 'university.degree'])\n",
    "test['default'] = test['default'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'no', 'yes'\n",
    "                                               ])\n",
    "test['housing'] = test['housing'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'no', 'yes'\n",
    "                                               ])\n",
    "test['loan'] = test['loan'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'no', 'yes'\n",
    "                                               ])\n",
    "test['contact'] = test['contact'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'cellular','telephone'\n",
    "                                               ])\n",
    "test['month'] = test['month'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'jan', 'feb', 'mar', \n",
    "                                                   'apr', 'may', 'jun', \n",
    "                                                   'jul', 'aug', 'sep',\n",
    "                                                   'oct', 'nov', 'dec'\n",
    "                                               ])\n",
    "test['day_of_week'] = test['day_of_week'].astype('category',\n",
    "                                               categories=[\n",
    "                                                 'mon','tue','wed','thu','fri'\n",
    "                                               ])\n",
    "test['poutcome'] = test['poutcome'].astype('category',\n",
    "                                               categories=[\n",
    "                                                'failure','nonexistent','success'\n",
    "                                               ])\n",
    "                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.get_dummies(test, columns=['contact', 'month', 'day_of_week', 'poutcome'])\n",
    "test = pd.get_dummies(test, columns=['job', 'marital', 'education', 'default', 'housing', 'loan'], dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['has_subcribe'], axis=1)\n",
    "y_train = train.has_subcribe\n",
    "\n",
    "X_test = test.drop(['has_subcribe'], axis=1)\n",
    "y_test = test.has_subcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test = X_train.align(X_test, join='outer', fill_value=0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20594, 65)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20594, 65)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'contact_cellular', 'contact_telephone', 'month_jan', 'month_feb', 'month_mar', 'month_apr', 'month_may', 'month_jun', 'month_jul', 'month_aug', 'month_sep', 'month_oct', 'month_nov', 'month_dec', 'day_of_week_mon', 'day_of_week_tue', 'day_of_week_wed', 'day_of_week_thu', 'day_of_week_fri', 'poutcome_failure', 'poutcome_nonexistent', 'poutcome_success', 'job_admin', 'job_blue-collar', 'job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired', 'job_self-employed', 'job_services', 'job_student', 'job_technician', 'job_unemployed', 'job_nan', 'marital_divorced', 'marital_married', 'marital_single', 'marital_nan', 'education_basic.4y', 'education_basic.6y', 'education_basic.9y', 'education_high.school', 'education_illiterate', 'education_professional.course', 'education_university.degree', 'education_nan', 'default_no', 'default_yes',\n",
       "       'default_nan', 'housing_no', 'housing_yes', 'housing_nan', 'loan_no', 'loan_yes', 'loan_nan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate', 'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'contact_cellular', 'contact_telephone', 'month_jan', 'month_feb', 'month_mar', 'month_apr', 'month_may', 'month_jun', 'month_jul', 'month_aug', 'month_sep', 'month_oct', 'month_nov', 'month_dec', 'day_of_week_mon', 'day_of_week_tue', 'day_of_week_wed', 'day_of_week_thu', 'day_of_week_fri', 'poutcome_failure', 'poutcome_nonexistent', 'poutcome_success', 'job_admin', 'job_blue-collar', 'job_entrepreneur', 'job_housemaid', 'job_management', 'job_retired', 'job_self-employed', 'job_services', 'job_student', 'job_technician', 'job_unemployed', 'job_nan', 'marital_divorced', 'marital_married', 'marital_single', 'marital_nan', 'education_basic.4y', 'education_basic.6y', 'education_basic.9y', 'education_high.school', 'education_illiterate', 'education_professional.course', 'education_university.degree', 'education_nan', 'default_no', 'default_yes',\n",
       "       'default_nan', 'housing_no', 'housing_yes', 'housing_nan', 'loan_no', 'loan_yes', 'loan_nan'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Tuning on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081832</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.087793</td>\n",
       "      <td>0.004095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.089346</td>\n",
       "      <td>0.003929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.080412</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>0.087501</td>\n",
       "      <td>0.003577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.080776</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.088472</td>\n",
       "      <td>0.002506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.080108</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.087890</td>\n",
       "      <td>0.003545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.079234</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.086724</td>\n",
       "      <td>0.002608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.080363</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>0.088327</td>\n",
       "      <td>0.003227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.078785</td>\n",
       "      <td>0.002858</td>\n",
       "      <td>0.088375</td>\n",
       "      <td>0.003643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.078554</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>0.087307</td>\n",
       "      <td>0.003098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.077583</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.087113</td>\n",
       "      <td>0.002578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.078166</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.086627</td>\n",
       "      <td>0.002435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.076928</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.086870</td>\n",
       "      <td>0.001905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.076636</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.086822</td>\n",
       "      <td>0.001605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.076260</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.086773</td>\n",
       "      <td>0.001543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.076734</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.086579</td>\n",
       "      <td>0.001674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.076139</td>\n",
       "      <td>0.002168</td>\n",
       "      <td>0.086336</td>\n",
       "      <td>0.001974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.075665</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.086287</td>\n",
       "      <td>0.002157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.075483</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>0.086287</td>\n",
       "      <td>0.001947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.074997</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.085802</td>\n",
       "      <td>0.002263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.075083</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.086239</td>\n",
       "      <td>0.002007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.074548</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.086239</td>\n",
       "      <td>0.001580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.074415</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>0.085704</td>\n",
       "      <td>0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.074439</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.085705</td>\n",
       "      <td>0.001445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.073942</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.085073</td>\n",
       "      <td>0.001104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.073541</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.085025</td>\n",
       "      <td>0.001403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.073249</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.085219</td>\n",
       "      <td>0.001865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.073128</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.085413</td>\n",
       "      <td>0.001701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.072667</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.085705</td>\n",
       "      <td>0.001708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.072764</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.085413</td>\n",
       "      <td>0.002025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.072230</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.085413</td>\n",
       "      <td>0.001611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.072084</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.085365</td>\n",
       "      <td>0.001888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.072242</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.085414</td>\n",
       "      <td>0.002050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.072011</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.085268</td>\n",
       "      <td>0.002119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.071829</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.085316</td>\n",
       "      <td>0.002449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.071489</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.084928</td>\n",
       "      <td>0.003015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.071562</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.084491</td>\n",
       "      <td>0.003110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.071089</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.084394</td>\n",
       "      <td>0.002749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.070955</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.083957</td>\n",
       "      <td>0.002765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.070955</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.083908</td>\n",
       "      <td>0.002936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.070603</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.084248</td>\n",
       "      <td>0.002848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.070579</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.084102</td>\n",
       "      <td>0.002653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.070445</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.084005</td>\n",
       "      <td>0.003025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.070506</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.084054</td>\n",
       "      <td>0.003229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.070433</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.084345</td>\n",
       "      <td>0.003278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.070202</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.084248</td>\n",
       "      <td>0.003396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.069887</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.084102</td>\n",
       "      <td>0.003419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.069693</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.084248</td>\n",
       "      <td>0.003541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.069462</td>\n",
       "      <td>0.001448</td>\n",
       "      <td>0.084297</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.069365</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.084005</td>\n",
       "      <td>0.003188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.068940</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.083811</td>\n",
       "      <td>0.002857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.068940</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.083908</td>\n",
       "      <td>0.002462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.068734</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.083860</td>\n",
       "      <td>0.003170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.068649</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.084151</td>\n",
       "      <td>0.002926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.068382</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.083957</td>\n",
       "      <td>0.002856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.068272</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.084102</td>\n",
       "      <td>0.002642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.068187</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.084102</td>\n",
       "      <td>0.003121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.068163</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.083665</td>\n",
       "      <td>0.002935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.067981</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.083520</td>\n",
       "      <td>0.003005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.067847</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.083277</td>\n",
       "      <td>0.003023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.067325</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.083180</td>\n",
       "      <td>0.002799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.067253</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.083325</td>\n",
       "      <td>0.002982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.066937</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.083180</td>\n",
       "      <td>0.003089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.066718</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.083034</td>\n",
       "      <td>0.002889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.066670</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.082792</td>\n",
       "      <td>0.002559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.066476</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.083131</td>\n",
       "      <td>0.002622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.066281</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.083325</td>\n",
       "      <td>0.002625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.066026</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.083131</td>\n",
       "      <td>0.002595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.065966</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.083325</td>\n",
       "      <td>0.002356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.065929</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.082791</td>\n",
       "      <td>0.002246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.065857</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.082888</td>\n",
       "      <td>0.002280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.065650</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.082646</td>\n",
       "      <td>0.002511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.065334</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.082985</td>\n",
       "      <td>0.002492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.065456</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.083131</td>\n",
       "      <td>0.002631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.065371</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.082694</td>\n",
       "      <td>0.002626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.065092</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.082548</td>\n",
       "      <td>0.002942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.064618</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.002787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.064594</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.082112</td>\n",
       "      <td>0.002964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.064448</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.082403</td>\n",
       "      <td>0.002888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.063951</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.082451</td>\n",
       "      <td>0.002702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.063951</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.002911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.063878</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.002709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.063562</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.082257</td>\n",
       "      <td>0.003125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.063380</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.082694</td>\n",
       "      <td>0.003433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.063113</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.082451</td>\n",
       "      <td>0.003209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.062907</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.082451</td>\n",
       "      <td>0.003522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.062797</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.003607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.062627</td>\n",
       "      <td>0.001291</td>\n",
       "      <td>0.082694</td>\n",
       "      <td>0.003604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.062287</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.082548</td>\n",
       "      <td>0.003648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.062118</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.082160</td>\n",
       "      <td>0.003736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.062020</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.082208</td>\n",
       "      <td>0.003872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.061923</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.082354</td>\n",
       "      <td>0.003616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.061511</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.082451</td>\n",
       "      <td>0.003704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.061486</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.082451</td>\n",
       "      <td>0.002992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.061207</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.082548</td>\n",
       "      <td>0.003295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.060867</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.082403</td>\n",
       "      <td>0.003132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.060734</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.082403</td>\n",
       "      <td>0.003562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.060588</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.082403</td>\n",
       "      <td>0.003426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.060248</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.082549</td>\n",
       "      <td>0.003207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.060115</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.082354</td>\n",
       "      <td>0.003374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.059945</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.082306</td>\n",
       "      <td>0.003316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.059787</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>0.082112</td>\n",
       "      <td>0.003296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.059847</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.082160</td>\n",
       "      <td>0.003271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.059750</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.082306</td>\n",
       "      <td>0.002881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.059483</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.082403</td>\n",
       "      <td>0.002767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.059350</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.002872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.059277</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.082451</td>\n",
       "      <td>0.002868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.058998</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.082257</td>\n",
       "      <td>0.003024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.058816</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.082257</td>\n",
       "      <td>0.002859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.058573</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.082063</td>\n",
       "      <td>0.003099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>0.058294</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.082160</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>0.058184</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.082209</td>\n",
       "      <td>0.002922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.058051</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.082160</td>\n",
       "      <td>0.002814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>0.057905</td>\n",
       "      <td>0.000706</td>\n",
       "      <td>0.082305</td>\n",
       "      <td>0.002598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.057650</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.082403</td>\n",
       "      <td>0.002738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.057590</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.082451</td>\n",
       "      <td>0.003151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>0.057359</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.082306</td>\n",
       "      <td>0.003081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>0.057225</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.082209</td>\n",
       "      <td>0.003129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.056934</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.082111</td>\n",
       "      <td>0.002792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.056667</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.082306</td>\n",
       "      <td>0.002747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>0.056728</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.081966</td>\n",
       "      <td>0.003101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0            0.081832         0.000812         0.087793        0.004095\n",
       "1            0.081213         0.003509         0.089346        0.003929\n",
       "2            0.080412         0.002331         0.087501        0.003577\n",
       "3            0.080776         0.002076         0.088472        0.002506\n",
       "4            0.080108         0.002407         0.087890        0.003545\n",
       "5            0.079234         0.002656         0.086724        0.002608\n",
       "6            0.080363         0.003373         0.088327        0.003227\n",
       "7            0.078785         0.002858         0.088375        0.003643\n",
       "8            0.078554         0.002753         0.087307        0.003098\n",
       "9            0.077583         0.002305         0.087113        0.002578\n",
       "10           0.078166         0.002347         0.086627        0.002435\n",
       "11           0.076928         0.002469         0.086870        0.001905\n",
       "12           0.076636         0.002050         0.086822        0.001605\n",
       "13           0.076260         0.002347         0.086773        0.001543\n",
       "14           0.076734         0.002357         0.086579        0.001674\n",
       "15           0.076139         0.002168         0.086336        0.001974\n",
       "16           0.075665         0.002221         0.086287        0.002157\n",
       "17           0.075483         0.001611         0.086287        0.001947\n",
       "18           0.074997         0.001405         0.085802        0.002263\n",
       "19           0.075083         0.001874         0.086239        0.002007\n",
       "20           0.074548         0.001522         0.086239        0.001580\n",
       "21           0.074415         0.001511         0.085704        0.001706\n",
       "22           0.074439         0.001846         0.085705        0.001445\n",
       "23           0.073942         0.001690         0.085073        0.001104\n",
       "24           0.073541         0.001646         0.085025        0.001403\n",
       "25           0.073249         0.001665         0.085219        0.001865\n",
       "26           0.073128         0.001423         0.085413        0.001701\n",
       "27           0.072667         0.001192         0.085705        0.001708\n",
       "28           0.072764         0.001393         0.085413        0.002025\n",
       "29           0.072230         0.000904         0.085413        0.001611\n",
       "30           0.072084         0.001003         0.085365        0.001888\n",
       "31           0.072242         0.001015         0.085414        0.002050\n",
       "32           0.072011         0.001145         0.085268        0.002119\n",
       "33           0.071829         0.001018         0.085316        0.002449\n",
       "34           0.071489         0.001273         0.084928        0.003015\n",
       "35           0.071562         0.001468         0.084491        0.003110\n",
       "36           0.071089         0.001592         0.084394        0.002749\n",
       "37           0.070955         0.001771         0.083957        0.002765\n",
       "38           0.070955         0.001600         0.083908        0.002936\n",
       "39           0.070603         0.001457         0.084248        0.002848\n",
       "40           0.070579         0.001348         0.084102        0.002653\n",
       "41           0.070445         0.001388         0.084005        0.003025\n",
       "42           0.070506         0.001382         0.084054        0.003229\n",
       "43           0.070433         0.001626         0.084345        0.003278\n",
       "44           0.070202         0.001454         0.084248        0.003396\n",
       "45           0.069887         0.001530         0.084102        0.003419\n",
       "46           0.069693         0.001263         0.084248        0.003541\n",
       "47           0.069462         0.001448         0.084297        0.003400\n",
       "48           0.069365         0.001432         0.084005        0.003188\n",
       "49           0.068940         0.001314         0.083811        0.002857\n",
       "50           0.068940         0.001488         0.083908        0.002462\n",
       "51           0.068734         0.001302         0.083860        0.003170\n",
       "52           0.068649         0.001221         0.084151        0.002926\n",
       "53           0.068382         0.001271         0.083957        0.002856\n",
       "54           0.068272         0.001279         0.084102        0.002642\n",
       "55           0.068187         0.001289         0.084102        0.003121\n",
       "56           0.068163         0.001045         0.083665        0.002935\n",
       "57           0.067981         0.001188         0.083520        0.003005\n",
       "58           0.067847         0.001164         0.083277        0.003023\n",
       "59           0.067325         0.001222         0.083180        0.002799\n",
       "60           0.067253         0.001379         0.083325        0.002982\n",
       "61           0.066937         0.001467         0.083180        0.003089\n",
       "62           0.066718         0.001192         0.083034        0.002889\n",
       "63           0.066670         0.001279         0.082792        0.002559\n",
       "64           0.066476         0.001280         0.083131        0.002622\n",
       "65           0.066281         0.001290         0.083325        0.002625\n",
       "66           0.066026         0.001414         0.083131        0.002595\n",
       "67           0.065966         0.001399         0.083325        0.002356\n",
       "68           0.065929         0.001348         0.082791        0.002246\n",
       "69           0.065857         0.001392         0.082888        0.002280\n",
       "70           0.065650         0.001506         0.082646        0.002511\n",
       "71           0.065334         0.001518         0.082985        0.002492\n",
       "72           0.065456         0.001424         0.083131        0.002631\n",
       "73           0.065371         0.001301         0.082694        0.002626\n",
       "74           0.065092         0.001181         0.082548        0.002942\n",
       "75           0.064618         0.001002         0.082500        0.002787\n",
       "76           0.064594         0.001019         0.082112        0.002964\n",
       "77           0.064448         0.001161         0.082403        0.002888\n",
       "78           0.063951         0.001341         0.082451        0.002702\n",
       "79           0.063951         0.001261         0.082500        0.002911\n",
       "80           0.063878         0.001454         0.082500        0.002709\n",
       "81           0.063562         0.001240         0.082257        0.003125\n",
       "82           0.063380         0.001126         0.082694        0.003433\n",
       "83           0.063113         0.001286         0.082451        0.003209\n",
       "84           0.062907         0.001236         0.082451        0.003522\n",
       "85           0.062797         0.001300         0.082500        0.003607\n",
       "86           0.062627         0.001291         0.082694        0.003604\n",
       "87           0.062287         0.001121         0.082548        0.003648\n",
       "88           0.062118         0.001404         0.082160        0.003736\n",
       "89           0.062020         0.001247         0.082208        0.003872\n",
       "90           0.061923         0.001230         0.082354        0.003616\n",
       "91           0.061511         0.001215         0.082451        0.003704\n",
       "92           0.061486         0.001145         0.082451        0.002992\n",
       "93           0.061207         0.001316         0.082548        0.003295\n",
       "94           0.060867         0.001150         0.082403        0.003132\n",
       "95           0.060734         0.001166         0.082403        0.003562\n",
       "96           0.060588         0.001163         0.082403        0.003426\n",
       "97           0.060248         0.001018         0.082549        0.003207\n",
       "98           0.060115         0.001046         0.082354        0.003374\n",
       "99           0.059945         0.001025         0.082306        0.003316\n",
       "100          0.059787         0.001205         0.082112        0.003296\n",
       "101          0.059847         0.001187         0.082160        0.003271\n",
       "102          0.059750         0.001054         0.082306        0.002881\n",
       "103          0.059483         0.001041         0.082403        0.002767\n",
       "104          0.059350         0.000943         0.082500        0.002872\n",
       "105          0.059277         0.000968         0.082451        0.002868\n",
       "106          0.058998         0.001033         0.082257        0.003024\n",
       "107          0.058816         0.000950         0.082257        0.002859\n",
       "108          0.058573         0.000913         0.082063        0.003099\n",
       "109          0.058294         0.000799         0.082160        0.003200\n",
       "110          0.058184         0.000717         0.082209        0.002922\n",
       "111          0.058051         0.000736         0.082160        0.002814\n",
       "112          0.057905         0.000706         0.082305        0.002598\n",
       "113          0.057650         0.000842         0.082403        0.002738\n",
       "114          0.057590         0.000845         0.082451        0.003151\n",
       "115          0.057359         0.000823         0.082306        0.003081\n",
       "116          0.057225         0.000918         0.082209        0.003129\n",
       "117          0.056934         0.001016         0.082111        0.002792\n",
       "118          0.056667         0.000956         0.082306        0.002747\n",
       "119          0.056728         0.000961         0.081966        0.003101"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=5,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_depth and min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'max_depth': 3, 'min_child_weight': 1}\n",
      "Run 0 best score:  0.916577643973973\n",
      "Run 1 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 1 best score:  0.9154608138292707\n",
      "Run 2 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 2 best score:  0.9164805283092162\n",
      "Run 3 best param:  {'max_depth': 3, 'min_child_weight': 1}\n",
      "Run 3 best score:  0.9161406234825678\n",
      "Run 4 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 4 best score:  0.9166262018063513\n",
      "Run 5 best param:  {'max_depth': 5, 'min_child_weight': 81}\n",
      "Run 5 best score:  0.9155579294940274\n",
      "Run 6 best param:  {'max_depth': 9, 'min_child_weight': 41}\n",
      "Run 6 best score:  0.915897834320676\n",
      "Run 7 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 7 best score:  0.9157521608235408\n",
      "Run 8 best param:  {'max_depth': 3, 'min_child_weight': 1}\n",
      "Run 8 best score:  0.9171603379625134\n",
      "Run 9 best param:  {'max_depth': 5, 'min_child_weight': 1}\n",
      "Run 9 best score:  0.9159463921530543\n",
      "Best params:  params               {'max_depth': 3, 'min_child_weight': 1}\n",
      "mean_test_score_0                                   0.916578\n",
      "mean_test_score_1                                   0.915072\n",
      "mean_test_score_2                                   0.915752\n",
      "mean_test_score_3                                   0.916141\n",
      "mean_test_score_4                                   0.915801\n",
      "mean_test_score_5                                   0.913276\n",
      "mean_test_score_6                                   0.915801\n",
      "mean_test_score_7                                   0.915509\n",
      "mean_test_score_8                                    0.91716\n",
      "mean_test_score_9                                   0.914878\n",
      "avg                                                 0.915597\n",
      "Name: 5, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1 = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1,200,40)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=119,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1.fit(X_train,y_train)    \n",
    "    if grid_score1.empty:\n",
    "        grid_score1 = pd.DataFrame(gsearch1.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1.best_score_)\n",
    "\n",
    "grid_score1['avg'] = grid_score1.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1.loc[grid_score1.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 0 best score:  0.9175002427891619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueManagerThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tai/.conda/envs/research/lib/python3.7/threading.py\", line 917, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/tai/.conda/envs/research/lib/python3.7/threading.py\", line 865, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 674, in _queue_management_worker\n",
      "    recursive_terminate(p)\n",
      "  File \"/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/utils.py\", line 28, in recursive_terminate\n",
      "    _recursive_terminate_without_psutil(process)\n",
      "  File \"/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/utils.py\", line 53, in _recursive_terminate_without_psutil\n",
      "    _recursive_terminate(process.pid)\n",
      "  File \"/home/tai/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/backend/utils.py\", line 94, in _recursive_terminate\n",
      "    stderr=None\n",
      "  File \"/home/tai/.conda/envs/research/lib/python3.7/subprocess.py\", line 395, in check_output\n",
      "    **kwargs).stdout\n",
      "  File \"/home/tai/.conda/envs/research/lib/python3.7/subprocess.py\", line 487, in run\n",
      "    output=stdout, stderr=stderr)\n",
      "subprocess.CalledProcessError: Command '['pgrep', '-P', '125682']' died with <Signals.SIGINT: 2>.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-c3397b333f95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                             \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfive_folds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                             return_train_score=False)\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mgsearch1b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrid_score1b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mgrid_score1b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgsearch1b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    853\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransportableException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    536\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    537\u001b[0m         \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mdelete_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1094\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m                 \u001b[0mqmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0mcq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/research/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test1b = {\n",
    " 'max_depth':range(1,10,2),\n",
    " 'min_child_weight':range(1, 10, 2)\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score1b = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=119,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch1b = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test1b,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch1b.fit(X_train,y_train)    \n",
    "    if grid_score1b.empty:\n",
    "        grid_score1b = pd.DataFrame(gsearch1b.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score1b.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score1b['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch1b.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch1b.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch1b.best_score_)\n",
    "\n",
    "grid_score1b['avg'] = grid_score1b.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score1b.loc[grid_score1b.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'max_depth': 6, 'min_child_weight': 2}\n",
      "Run 0 best score:  0.9180343789453239\n",
      "Run 1 best param:  {'max_depth': 4, 'min_child_weight': 3}\n",
      "Run 1 best score:  0.9174516849567835\n",
      "Run 2 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 2 best score:  0.916577643973973\n",
      "Run 3 best param:  {'max_depth': 5, 'min_child_weight': 2}\n",
      "Run 3 best score:  0.9173545692920269\n",
      "Run 4 best param:  {'max_depth': 4, 'min_child_weight': 3}\n",
      "Run 4 best score:  0.9170146644653783\n",
      "Run 5 best param:  {'max_depth': 4, 'min_child_weight': 2}\n",
      "Run 5 best score:  0.9155579294940274\n",
      "Run 6 best param:  {'max_depth': 4, 'min_child_weight': 3}\n",
      "Run 6 best score:  0.9174031271244052\n",
      "Run 7 best param:  {'max_depth': 4, 'min_child_weight': 2}\n",
      "Run 7 best score:  0.9164319704768379\n",
      "Run 8 best param:  {'max_depth': 5, 'min_child_weight': 3}\n",
      "Run 8 best score:  0.9175002427891619\n",
      "Run 9 best param:  {'max_depth': 4, 'min_child_weight': 4}\n",
      "Run 9 best score:  0.9166747596387297\n",
      "Best params:  params               {'max_depth': 4, 'min_child_weight': 3}\n",
      "mean_test_score_0                                   0.917452\n",
      "mean_test_score_1                                   0.917452\n",
      "mean_test_score_2                                   0.916238\n",
      "mean_test_score_3                                   0.916626\n",
      "mean_test_score_4                                   0.917015\n",
      "mean_test_score_5                                   0.915267\n",
      "mean_test_score_6                                   0.917403\n",
      "mean_test_score_7                                   0.915995\n",
      "mean_test_score_8                                   0.916578\n",
      "mean_test_score_9                                   0.916092\n",
      "avg                                                 0.916612\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look carefully again the neigbor values\n",
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test2 = {\n",
    " 'max_depth':[4, 5, 6],\n",
    " 'min_child_weight':[2, 3 ,4]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score2 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=119,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,        \n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch2 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test2,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch2.fit(X_train,y_train)    \n",
    "    if grid_score2.empty:\n",
    "        grid_score2 = pd.DataFrame(gsearch2.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score2.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score2['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch2.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch2.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch2.best_score_)\n",
    "\n",
    "grid_score2['avg'] = grid_score2.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score2.loc[grid_score2.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'gamma': 0.0}\n",
      "Run 0 best score:  0.9174516849567835\n",
      "Run 1 best param:  {'gamma': 0.0}\n",
      "Run 1 best score:  0.9174516849567835\n",
      "Run 2 best param:  {'gamma': 0.1}\n",
      "Run 2 best score:  0.9167233174711081\n",
      "Run 3 best param:  {'gamma': 0.1}\n",
      "Run 3 best score:  0.9172574536272701\n",
      "Run 4 best param:  {'gamma': 0.0}\n",
      "Run 4 best score:  0.9170146644653783\n",
      "Run 5 best param:  {'gamma': 0.1}\n",
      "Run 5 best score:  0.9157036029911625\n",
      "Run 6 best param:  {'gamma': 0.0}\n",
      "Run 6 best score:  0.9174031271244052\n",
      "Run 7 best param:  {'gamma': 0.0}\n",
      "Run 7 best score:  0.9159949499854326\n",
      "Run 8 best param:  {'gamma': 0.2}\n",
      "Run 8 best score:  0.9166747596387297\n",
      "Run 9 best param:  {'gamma': 0.4}\n",
      "Run 9 best score:  0.9164805283092162\n",
      "Best params:  params               {'gamma': 0.0}\n",
      "mean_test_score_0          0.917452\n",
      "mean_test_score_1          0.917452\n",
      "mean_test_score_2          0.916238\n",
      "mean_test_score_3          0.916626\n",
      "mean_test_score_4          0.917015\n",
      "mean_test_score_5          0.915267\n",
      "mean_test_score_6          0.917403\n",
      "mean_test_score_7          0.915995\n",
      "mean_test_score_8          0.916578\n",
      "mean_test_score_9          0.916092\n",
      "avg                        0.916612\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score3 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=119,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch3 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test3,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch3.fit(X_train,y_train)    \n",
    "    if grid_score3.empty:\n",
    "        grid_score3 = pd.DataFrame(gsearch3.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score3.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score3['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch3.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch3.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch3.best_score_)\n",
    "\n",
    "grid_score3['avg'] = grid_score3.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score3.loc[grid_score3.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recablirating the n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.086421</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.090949</td>\n",
       "      <td>0.003695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.087623</td>\n",
       "      <td>0.004168</td>\n",
       "      <td>0.092066</td>\n",
       "      <td>0.004460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085984</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.090075</td>\n",
       "      <td>0.003458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085984</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.089103</td>\n",
       "      <td>0.003275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.085207</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>0.089444</td>\n",
       "      <td>0.003296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.085377</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.090075</td>\n",
       "      <td>0.002768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.086263</td>\n",
       "      <td>0.002905</td>\n",
       "      <td>0.090318</td>\n",
       "      <td>0.002730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.084794</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.090706</td>\n",
       "      <td>0.002615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.084272</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.089638</td>\n",
       "      <td>0.002165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.083689</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.089880</td>\n",
       "      <td>0.001738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.084733</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.089880</td>\n",
       "      <td>0.001644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.083435</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.089055</td>\n",
       "      <td>0.001932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.082330</td>\n",
       "      <td>0.002495</td>\n",
       "      <td>0.088861</td>\n",
       "      <td>0.002081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.082415</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.089055</td>\n",
       "      <td>0.001701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.082791</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.089249</td>\n",
       "      <td>0.002642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.082475</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.088084</td>\n",
       "      <td>0.002655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.081468</td>\n",
       "      <td>0.002575</td>\n",
       "      <td>0.087647</td>\n",
       "      <td>0.002826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.081189</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.087841</td>\n",
       "      <td>0.002780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.081080</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.087161</td>\n",
       "      <td>0.002543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.080885</td>\n",
       "      <td>0.002275</td>\n",
       "      <td>0.086724</td>\n",
       "      <td>0.002029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.080339</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.086967</td>\n",
       "      <td>0.002775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.080448</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>0.086579</td>\n",
       "      <td>0.002763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.080181</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.086336</td>\n",
       "      <td>0.001886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.079562</td>\n",
       "      <td>0.001455</td>\n",
       "      <td>0.086287</td>\n",
       "      <td>0.002274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.079635</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.086433</td>\n",
       "      <td>0.002402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.079137</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.086676</td>\n",
       "      <td>0.002022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.078785</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.086676</td>\n",
       "      <td>0.002853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.078494</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.085947</td>\n",
       "      <td>0.002656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.078640</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.086530</td>\n",
       "      <td>0.001799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.078506</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.086142</td>\n",
       "      <td>0.001862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.078324</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.086190</td>\n",
       "      <td>0.001993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.078433</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.086045</td>\n",
       "      <td>0.001528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.078360</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.086482</td>\n",
       "      <td>0.001711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.078093</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.085996</td>\n",
       "      <td>0.002341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.078069</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.085656</td>\n",
       "      <td>0.002206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.077705</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.085511</td>\n",
       "      <td>0.002497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.077911</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.085608</td>\n",
       "      <td>0.002459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.077644</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.085365</td>\n",
       "      <td>0.002901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.077438</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.085510</td>\n",
       "      <td>0.002340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.077328</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.085656</td>\n",
       "      <td>0.001968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.077340</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.085511</td>\n",
       "      <td>0.002336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.076891</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.084588</td>\n",
       "      <td>0.002657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.076879</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.084636</td>\n",
       "      <td>0.002992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.076721</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.084054</td>\n",
       "      <td>0.002896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.076770</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.083860</td>\n",
       "      <td>0.002303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.076685</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.084102</td>\n",
       "      <td>0.002602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.076661</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.084248</td>\n",
       "      <td>0.002664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.076636</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.084054</td>\n",
       "      <td>0.002870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.076394</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.084248</td>\n",
       "      <td>0.002504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.076357</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.084637</td>\n",
       "      <td>0.002463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.076090</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.084297</td>\n",
       "      <td>0.002495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.076066</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.084199</td>\n",
       "      <td>0.002914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.075969</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.084151</td>\n",
       "      <td>0.003022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.075604</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.084394</td>\n",
       "      <td>0.003075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.075701</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.083811</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.075495</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.083908</td>\n",
       "      <td>0.002827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.075265</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.083471</td>\n",
       "      <td>0.002353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.075240</td>\n",
       "      <td>0.001114</td>\n",
       "      <td>0.083762</td>\n",
       "      <td>0.002640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.075350</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.083617</td>\n",
       "      <td>0.002490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.075131</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.084296</td>\n",
       "      <td>0.002759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.075034</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.083908</td>\n",
       "      <td>0.003039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.074864</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.083859</td>\n",
       "      <td>0.002593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.074876</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.083908</td>\n",
       "      <td>0.002713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.074646</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.084005</td>\n",
       "      <td>0.002784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.074548</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.083908</td>\n",
       "      <td>0.002906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.074318</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.083908</td>\n",
       "      <td>0.002865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.074075</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.083811</td>\n",
       "      <td>0.002747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.073917</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.083811</td>\n",
       "      <td>0.003232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.073917</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.083762</td>\n",
       "      <td>0.002960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.073577</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.083714</td>\n",
       "      <td>0.003350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.073419</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.083423</td>\n",
       "      <td>0.003340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.073298</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.083471</td>\n",
       "      <td>0.003271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.073177</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.083665</td>\n",
       "      <td>0.003332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.073043</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.083471</td>\n",
       "      <td>0.002871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.073104</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.083908</td>\n",
       "      <td>0.002920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.072788</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.083860</td>\n",
       "      <td>0.003199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.072594</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.084005</td>\n",
       "      <td>0.003321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.072400</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.083714</td>\n",
       "      <td>0.003621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.072400</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.083277</td>\n",
       "      <td>0.003032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.072169</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.083277</td>\n",
       "      <td>0.002998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.072278</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.083520</td>\n",
       "      <td>0.002756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.072096</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.083471</td>\n",
       "      <td>0.002679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.072048</td>\n",
       "      <td>0.001335</td>\n",
       "      <td>0.083374</td>\n",
       "      <td>0.002544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.071829</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.083374</td>\n",
       "      <td>0.002539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.071805</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.083180</td>\n",
       "      <td>0.002355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.071696</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.083131</td>\n",
       "      <td>0.002547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.071732</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.083131</td>\n",
       "      <td>0.002674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.071611</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.083034</td>\n",
       "      <td>0.002587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0           0.086421         0.001372         0.090949        0.003695\n",
       "1           0.087623         0.004168         0.092066        0.004460\n",
       "2           0.085984         0.002774         0.090075        0.003458\n",
       "3           0.085984         0.002393         0.089103        0.003275\n",
       "4           0.085207         0.002213         0.089444        0.003296\n",
       "5           0.085377         0.003600         0.090075        0.002768\n",
       "6           0.086263         0.002905         0.090318        0.002730\n",
       "7           0.084794         0.003336         0.090706        0.002615\n",
       "8           0.084272         0.003305         0.089638        0.002165\n",
       "9           0.083689         0.002393         0.089880        0.001738\n",
       "10          0.084733         0.003257         0.089880        0.001644\n",
       "11          0.083435         0.002994         0.089055        0.001932\n",
       "12          0.082330         0.002495         0.088861        0.002081\n",
       "13          0.082415         0.003596         0.089055        0.001701\n",
       "14          0.082791         0.003775         0.089249        0.002642\n",
       "15          0.082475         0.003593         0.088084        0.002655\n",
       "16          0.081468         0.002575         0.087647        0.002826\n",
       "17          0.081189         0.002218         0.087841        0.002780\n",
       "18          0.081080         0.001925         0.087161        0.002543\n",
       "19          0.080885         0.002275         0.086724        0.002029\n",
       "20          0.080339         0.001987         0.086967        0.002775\n",
       "21          0.080448         0.001196         0.086579        0.002763\n",
       "22          0.080181         0.002064         0.086336        0.001886\n",
       "23          0.079562         0.001455         0.086287        0.002274\n",
       "24          0.079635         0.001132         0.086433        0.002402\n",
       "25          0.079137         0.001256         0.086676        0.002022\n",
       "26          0.078785         0.000798         0.086676        0.002853\n",
       "27          0.078494         0.000940         0.085947        0.002656\n",
       "28          0.078640         0.001122         0.086530        0.001799\n",
       "29          0.078506         0.001053         0.086142        0.001862\n",
       "30          0.078324         0.000901         0.086190        0.001993\n",
       "31          0.078433         0.001126         0.086045        0.001528\n",
       "32          0.078360         0.000923         0.086482        0.001711\n",
       "33          0.078093         0.000844         0.085996        0.002341\n",
       "34          0.078069         0.000847         0.085656        0.002206\n",
       "35          0.077705         0.001013         0.085511        0.002497\n",
       "36          0.077911         0.000956         0.085608        0.002459\n",
       "37          0.077644         0.001054         0.085365        0.002901\n",
       "38          0.077438         0.000831         0.085510        0.002340\n",
       "39          0.077328         0.000996         0.085656        0.001968\n",
       "40          0.077340         0.000955         0.085511        0.002336\n",
       "41          0.076891         0.000983         0.084588        0.002657\n",
       "42          0.076879         0.000991         0.084636        0.002992\n",
       "43          0.076721         0.000932         0.084054        0.002896\n",
       "44          0.076770         0.000918         0.083860        0.002303\n",
       "45          0.076685         0.001015         0.084102        0.002602\n",
       "46          0.076661         0.000930         0.084248        0.002664\n",
       "47          0.076636         0.001078         0.084054        0.002870\n",
       "48          0.076394         0.001152         0.084248        0.002504\n",
       "49          0.076357         0.001148         0.084637        0.002463\n",
       "50          0.076090         0.001254         0.084297        0.002495\n",
       "51          0.076066         0.001223         0.084199        0.002914\n",
       "52          0.075969         0.001167         0.084151        0.003022\n",
       "53          0.075604         0.001142         0.084394        0.003075\n",
       "54          0.075701         0.001284         0.083811        0.002900\n",
       "55          0.075495         0.001170         0.083908        0.002827\n",
       "56          0.075265         0.001113         0.083471        0.002353\n",
       "57          0.075240         0.001114         0.083762        0.002640\n",
       "58          0.075350         0.001078         0.083617        0.002490\n",
       "59          0.075131         0.001167         0.084296        0.002759\n",
       "60          0.075034         0.001322         0.083908        0.003039\n",
       "61          0.074864         0.001390         0.083859        0.002593\n",
       "62          0.074876         0.001300         0.083908        0.002713\n",
       "63          0.074646         0.001429         0.084005        0.002784\n",
       "64          0.074548         0.001203         0.083908        0.002906\n",
       "65          0.074318         0.001191         0.083908        0.002865\n",
       "66          0.074075         0.001308         0.083811        0.002747\n",
       "67          0.073917         0.001367         0.083811        0.003232\n",
       "68          0.073917         0.001398         0.083762        0.002960\n",
       "69          0.073577         0.001252         0.083714        0.003350\n",
       "70          0.073419         0.001137         0.083423        0.003340\n",
       "71          0.073298         0.001103         0.083471        0.003271\n",
       "72          0.073177         0.001203         0.083665        0.003332\n",
       "73          0.073043         0.001191         0.083471        0.002871\n",
       "74          0.073104         0.001421         0.083908        0.002920\n",
       "75          0.072788         0.001268         0.083860        0.003199\n",
       "76          0.072594         0.001344         0.084005        0.003321\n",
       "77          0.072400         0.001325         0.083714        0.003621\n",
       "78          0.072400         0.001528         0.083277        0.003032\n",
       "79          0.072169         0.001445         0.083277        0.002998\n",
       "80          0.072278         0.001325         0.083520        0.002756\n",
       "81          0.072096         0.001275         0.083471        0.002679\n",
       "82          0.072048         0.001335         0.083374        0.002544\n",
       "83          0.071829         0.001189         0.083374        0.002539\n",
       "84          0.071805         0.001279         0.083180        0.002355\n",
       "85          0.071696         0.001354         0.083131        0.002547\n",
       "86          0.071732         0.001409         0.083131        0.002674\n",
       "87          0.071611         0.001367         0.083034        0.002587"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.1,\n",
    "    n_estimators=5000,\n",
    "    max_depth=4,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=1)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'n_estimators': 100}\n",
      "Run 0 best score:  0.9175973584539187\n",
      "Run 1 best param:  {'n_estimators': 87}\n",
      "Run 1 best score:  0.9170632222977566\n",
      "Run 2 best param:  {'n_estimators': 87}\n",
      "Run 2 best score:  0.9171603379625134\n",
      "Run 3 best param:  {'n_estimators': 100}\n",
      "Run 3 best score:  0.9160920656501894\n",
      "Run 4 best param:  {'n_estimators': 87}\n",
      "Run 4 best score:  0.9167233174711081\n",
      "Run 5 best param:  {'n_estimators': 100}\n",
      "Run 5 best score:  0.916043507817811\n",
      "Run 6 best param:  {'n_estimators': 87}\n",
      "Run 6 best score:  0.9175973584539187\n",
      "Run 7 best param:  {'n_estimators': 100}\n",
      "Run 7 best score:  0.9155579294940274\n",
      "Run 8 best param:  {'n_estimators': 100}\n",
      "Run 8 best score:  0.9170632222977566\n",
      "Run 9 best param:  {'n_estimators': 100}\n",
      "Run 9 best score:  0.9166747596387297\n",
      "Best params:  params               {'n_estimators': 100}\n",
      "mean_test_score_0                 0.917597\n",
      "mean_test_score_1                 0.916723\n",
      "mean_test_score_2                 0.915898\n",
      "mean_test_score_3                 0.916092\n",
      "mean_test_score_4                 0.916481\n",
      "mean_test_score_5                 0.916044\n",
      "mean_test_score_6                 0.917306\n",
      "mean_test_score_7                 0.915558\n",
      "mean_test_score_8                 0.917063\n",
      "mean_test_score_9                 0.916675\n",
      "avg                               0.916544\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test_recalib = {\n",
    " 'n_estimators':[i for i in range(100, 1000, 100)]+[87]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score_recalib = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=87,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch_calib = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test_recalib,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch_calib.fit(X_train,y_train)    \n",
    "    if grid_score_recalib.empty:\n",
    "        grid_score_recalib = pd.DataFrame(gsearch_calib.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score_recalib.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score_recalib['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch_calib.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch_calib.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch_calib.best_score_)\n",
    "\n",
    "grid_score_recalib['avg'] = grid_score_recalib.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score_recalib.loc[grid_score_recalib.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'n_estimators': 110}\n",
      "Run 0 best score:  0.9178401476158105\n",
      "Run 1 best param:  {'n_estimators': 120}\n",
      "Run 1 best score:  0.9173545692920269\n",
      "Run 2 best param:  {'n_estimators': 110}\n",
      "Run 2 best score:  0.9163834126444595\n",
      "Run 3 best param:  {'n_estimators': 150}\n",
      "Run 3 best score:  0.917111780130135\n",
      "Run 4 best param:  {'n_estimators': 120}\n",
      "Run 4 best score:  0.9172088957948917\n",
      "Run 5 best param:  {'n_estimators': 100}\n",
      "Run 5 best score:  0.916043507817811\n",
      "Run 6 best param:  {'n_estimators': 120}\n",
      "Run 6 best score:  0.9174516849567835\n",
      "Run 7 best param:  {'n_estimators': 160}\n",
      "Run 7 best score:  0.916043507817811\n",
      "Run 8 best param:  {'n_estimators': 100}\n",
      "Run 8 best score:  0.9170632222977566\n",
      "Run 9 best param:  {'n_estimators': 100}\n",
      "Run 9 best score:  0.9166747596387297\n",
      "Best params:  params               {'n_estimators': 110}\n",
      "mean_test_score_0                  0.91784\n",
      "mean_test_score_1                 0.917112\n",
      "mean_test_score_2                 0.916383\n",
      "mean_test_score_3                 0.916141\n",
      "mean_test_score_4                  0.91682\n",
      "mean_test_score_5                 0.915412\n",
      "mean_test_score_6                 0.917015\n",
      "mean_test_score_7                 0.915946\n",
      "mean_test_score_8                  0.91682\n",
      "mean_test_score_9                 0.916481\n",
      "avg                               0.916597\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test_recalib = {\n",
    " 'n_estimators':[i for i in range(100, 200, 10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score_recalib = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=87,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch_calib = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test_recalib,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch_calib.fit(X_train,y_train)    \n",
    "    if grid_score_recalib.empty:\n",
    "        grid_score_recalib = pd.DataFrame(gsearch_calib.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score_recalib.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score_recalib['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch_calib.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch_calib.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch_calib.best_score_)\n",
    "\n",
    "grid_score_recalib['avg'] = grid_score_recalib.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score_recalib.loc[grid_score_recalib.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'n_estimators': 110}\n",
      "Run 0 best score:  0.9178401476158105\n",
      "Run 1 best param:  {'n_estimators': 118}\n",
      "Run 1 best score:  0.9175973584539187\n",
      "Run 2 best param:  {'n_estimators': 110}\n",
      "Run 2 best score:  0.9163834126444595\n",
      "Run 3 best param:  {'n_estimators': 117}\n",
      "Run 3 best score:  0.9166262018063513\n",
      "Run 4 best param:  {'n_estimators': 120}\n",
      "Run 4 best score:  0.9172088957948917\n",
      "Run 5 best param:  {'n_estimators': 101}\n",
      "Run 5 best score:  0.9162377391473244\n",
      "Run 6 best param:  {'n_estimators': 101}\n",
      "Run 6 best score:  0.9175973584539187\n",
      "Run 7 best param:  {'n_estimators': 115}\n",
      "Run 7 best score:  0.9159949499854326\n",
      "Run 8 best param:  {'n_estimators': 117}\n",
      "Run 8 best score:  0.9172088957948917\n",
      "Run 9 best param:  {'n_estimators': 100}\n",
      "Run 9 best score:  0.9166747596387297\n",
      "Best params:  params               {'n_estimators': 117}\n",
      "mean_test_score_0                  0.91716\n",
      "mean_test_score_1                 0.917306\n",
      "mean_test_score_2                 0.916044\n",
      "mean_test_score_3                 0.916626\n",
      "mean_test_score_4                 0.917015\n",
      "mean_test_score_5                 0.915364\n",
      "mean_test_score_6                 0.917355\n",
      "mean_test_score_7                 0.915849\n",
      "mean_test_score_8                 0.917209\n",
      "mean_test_score_9                 0.916238\n",
      "avg                               0.916616\n",
      "Name: 17, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test_recalib = {\n",
    " 'n_estimators':[i for i in range(100, 121)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score_recalib = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=87,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch_calib = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test_recalib,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch_calib.fit(X_train,y_train)    \n",
    "    if grid_score_recalib.empty:\n",
    "        grid_score_recalib = pd.DataFrame(gsearch_calib.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score_recalib.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score_recalib['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch_calib.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch_calib.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch_calib.best_score_)\n",
    "\n",
    "grid_score_recalib['avg'] = grid_score_recalib.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score_recalib.loc[grid_score_recalib.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'colsample_bytree': 0.9, 'subsample': 0.7}\n",
      "Run 0 best score:  0.917645916286297\n",
      "Run 1 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 1 best score:  0.9173060114596484\n",
      "Run 2 best param:  {'colsample_bytree': 0.8, 'subsample': 0.7}\n",
      "Run 2 best score:  0.9169175488006216\n",
      "Run 3 best param:  {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "Run 3 best score:  0.9180829367777023\n",
      "Run 4 best param:  {'colsample_bytree': 0.7, 'subsample': 0.6}\n",
      "Run 4 best score:  0.9175973584539187\n",
      "Run 5 best param:  {'colsample_bytree': 0.6, 'subsample': 0.9}\n",
      "Run 5 best score:  0.9157521608235408\n",
      "Run 6 best param:  {'colsample_bytree': 0.7, 'subsample': 0.9}\n",
      "Run 6 best score:  0.9174516849567835\n",
      "Run 7 best param:  {'colsample_bytree': 0.7, 'subsample': 0.6}\n",
      "Run 7 best score:  0.9162862969797029\n",
      "Run 8 best param:  {'colsample_bytree': 0.7, 'subsample': 0.8}\n",
      "Run 8 best score:  0.9178401476158105\n",
      "Run 9 best param:  {'colsample_bytree': 0.7, 'subsample': 0.8}\n",
      "Run 9 best score:  0.9172088957948917\n",
      "Best params:  params               {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "mean_test_score_0                                        0.91716\n",
      "mean_test_score_1                                       0.917306\n",
      "mean_test_score_2                                       0.916044\n",
      "mean_test_score_3                                       0.916626\n",
      "mean_test_score_4                                       0.917015\n",
      "mean_test_score_5                                       0.915364\n",
      "mean_test_score_6                                       0.917355\n",
      "mean_test_score_7                                       0.915849\n",
      "mean_test_score_8                                       0.917209\n",
      "mean_test_score_9                                       0.916238\n",
      "avg                                                     0.916616\n",
      "Name: 10, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score4 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=117,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch4 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test4,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch4.fit(X_train,y_train)    \n",
    "    if grid_score4.empty:\n",
    "        grid_score4 = pd.DataFrame(gsearch4.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score4.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score4['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch4.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch4.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch4.best_score_)\n",
    "\n",
    "grid_score4['avg'] = grid_score4.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score4.loc[grid_score4.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'colsample_bytree': 0.85, 'subsample': 0.85}\n",
      "Run 0 best score:  0.9177430319510537\n",
      "Run 1 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 1 best score:  0.9173060114596484\n",
      "Run 2 best param:  {'colsample_bytree': 0.8, 'subsample': 0.75}\n",
      "Run 2 best score:  0.9170632222977566\n",
      "Run 3 best param:  {'colsample_bytree': 0.8, 'subsample': 0.75}\n",
      "Run 3 best score:  0.9173545692920269\n",
      "Run 4 best param:  {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "Run 4 best score:  0.9170146644653783\n",
      "Run 5 best param:  {'colsample_bytree': 0.75, 'subsample': 0.75}\n",
      "Run 5 best score:  0.9162862969797029\n",
      "Run 6 best param:  {'colsample_bytree': 0.75, 'subsample': 0.85}\n",
      "Run 6 best score:  0.9174031271244052\n",
      "Run 7 best param:  {'colsample_bytree': 0.85, 'subsample': 0.8}\n",
      "Run 7 best score:  0.9164805283092162\n",
      "Run 8 best param:  {'colsample_bytree': 0.75, 'subsample': 0.8}\n",
      "Run 8 best score:  0.9180343789453239\n",
      "Run 9 best param:  {'colsample_bytree': 0.8, 'subsample': 0.85}\n",
      "Run 9 best score:  0.9173545692920269\n",
      "Best params:  params               {'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "mean_test_score_0                                        0.91716\n",
      "mean_test_score_1                                       0.917306\n",
      "mean_test_score_2                                       0.916044\n",
      "mean_test_score_3                                       0.916626\n",
      "mean_test_score_4                                       0.917015\n",
      "mean_test_score_5                                       0.915364\n",
      "mean_test_score_6                                       0.917355\n",
      "mean_test_score_7                                       0.915849\n",
      "mean_test_score_8                                       0.917209\n",
      "mean_test_score_9                                       0.916238\n",
      "avg                                                     0.916616\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Carefully search for each neighboring 0.05\n",
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(75,90,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score5 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=117,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch5 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test5,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch5.fit(X_train,y_train)    \n",
    "    if grid_score5.empty:\n",
    "        grid_score5 = pd.DataFrame(gsearch5.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score5.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score5['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch5.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch5.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch5.best_score_)\n",
    "\n",
    "grid_score5['avg'] = grid_score5.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score5.loc[grid_score5.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'reg_alpha': 1}\n",
      "Run 0 best score:  0.9172088957948917\n",
      "Run 1 best param:  {'reg_alpha': 0}\n",
      "Run 1 best score:  0.9173060114596484\n",
      "Run 2 best param:  {'reg_alpha': 0.01}\n",
      "Run 2 best score:  0.9161406234825678\n",
      "Run 3 best param:  {'reg_alpha': 0.1}\n",
      "Run 3 best score:  0.9170632222977566\n",
      "Run 4 best param:  {'reg_alpha': 0.01}\n",
      "Run 4 best score:  0.917111780130135\n",
      "Run 5 best param:  {'reg_alpha': 0}\n",
      "Run 5 best score:  0.9153636981645139\n",
      "Run 6 best param:  {'reg_alpha': 0}\n",
      "Run 6 best score:  0.9173545692920269\n",
      "Run 7 best param:  {'reg_alpha': 0}\n",
      "Run 7 best score:  0.9158492764882976\n",
      "Run 8 best param:  {'reg_alpha': 0.1}\n",
      "Run 8 best score:  0.9178401476158105\n",
      "Run 9 best param:  {'reg_alpha': 0}\n",
      "Run 9 best score:  0.9162377391473244\n",
      "Best params:  params               {'reg_alpha': 0}\n",
      "mean_test_score_0             0.91716\n",
      "mean_test_score_1            0.917306\n",
      "mean_test_score_2            0.916044\n",
      "mean_test_score_3            0.916626\n",
      "mean_test_score_4            0.917015\n",
      "mean_test_score_5            0.915364\n",
      "mean_test_score_6            0.917355\n",
      "mean_test_score_7            0.915849\n",
      "mean_test_score_8            0.917209\n",
      "mean_test_score_9            0.916238\n",
      "avg                          0.916616\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[0, 1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score6 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=117,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch6 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test6,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch6.fit(X_train,y_train)    \n",
    "    if grid_score6.empty:\n",
    "        grid_score6 = pd.DataFrame(gsearch6.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score6.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score6['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch6.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch6.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch6.best_score_)\n",
    "\n",
    "grid_score6['avg'] = grid_score6.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score6.loc[grid_score6.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'reg_alpha': 0}\n",
      "Run 0 best score:  0.9171603379625134\n",
      "Run 1 best param:  {'reg_alpha': 0.001}\n",
      "Run 1 best score:  0.9175002427891619\n",
      "Run 2 best param:  {'reg_alpha': 0.08}\n",
      "Run 2 best score:  0.9166262018063513\n",
      "Run 3 best param:  {'reg_alpha': 0.08}\n",
      "Run 3 best score:  0.9171603379625134\n",
      "Run 4 best param:  {'reg_alpha': 0.01}\n",
      "Run 4 best score:  0.917111780130135\n",
      "Run 5 best param:  {'reg_alpha': 0}\n",
      "Run 5 best score:  0.9153636981645139\n",
      "Run 6 best param:  {'reg_alpha': 0}\n",
      "Run 6 best score:  0.9173545692920269\n",
      "Run 7 best param:  {'reg_alpha': 0}\n",
      "Run 7 best score:  0.9158492764882976\n",
      "Run 8 best param:  {'reg_alpha': 0.01}\n",
      "Run 8 best score:  0.9173545692920269\n",
      "Run 9 best param:  {'reg_alpha': 0.05}\n",
      "Run 9 best score:  0.9169175488006216\n",
      "Best params:  params               {'reg_alpha': 0}\n",
      "mean_test_score_0             0.91716\n",
      "mean_test_score_1            0.917306\n",
      "mean_test_score_2            0.916044\n",
      "mean_test_score_3            0.916626\n",
      "mean_test_score_4            0.917015\n",
      "mean_test_score_5            0.915364\n",
      "mean_test_score_6            0.917355\n",
      "mean_test_score_7            0.915849\n",
      "mean_test_score_8            0.917209\n",
      "mean_test_score_9            0.916238\n",
      "avg                          0.916616\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0, 1e-4, 1e-3, 1e-2, 5e-2, 8e-2]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score7 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=117,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=8,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch7 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test7,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch7.fit(X_train,y_train)    \n",
    "    if grid_score7.empty:\n",
    "        grid_score7 = pd.DataFrame(gsearch7.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score7.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score7['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch7.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch7.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch7.best_score_)\n",
    "\n",
    "grid_score7['avg'] = grid_score7.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score7.loc[grid_score7.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the learning rate and tune n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-error-mean</th>\n",
       "      <th>train-error-std</th>\n",
       "      <th>test-error-mean</th>\n",
       "      <th>test-error-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.086421</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.090949</td>\n",
       "      <td>0.003695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.087246</td>\n",
       "      <td>0.004478</td>\n",
       "      <td>0.091434</td>\n",
       "      <td>0.004236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085680</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.089881</td>\n",
       "      <td>0.003358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.086493</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.090755</td>\n",
       "      <td>0.002669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.086312</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.090949</td>\n",
       "      <td>0.002665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.085911</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.091726</td>\n",
       "      <td>0.003091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.087938</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.091920</td>\n",
       "      <td>0.002995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.087404</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.090997</td>\n",
       "      <td>0.003383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.087088</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.091580</td>\n",
       "      <td>0.003372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.086166</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.091143</td>\n",
       "      <td>0.003035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.087853</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.091871</td>\n",
       "      <td>0.003245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.086542</td>\n",
       "      <td>0.002999</td>\n",
       "      <td>0.091677</td>\n",
       "      <td>0.003832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.086384</td>\n",
       "      <td>0.002509</td>\n",
       "      <td>0.090706</td>\n",
       "      <td>0.004049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.086299</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.091289</td>\n",
       "      <td>0.003693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.086627</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.090949</td>\n",
       "      <td>0.003948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.086943</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.090561</td>\n",
       "      <td>0.003377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.086178</td>\n",
       "      <td>0.002623</td>\n",
       "      <td>0.089638</td>\n",
       "      <td>0.003469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train-error-mean  train-error-std  test-error-mean  test-error-std\n",
       "0           0.086421         0.001372         0.090949        0.003695\n",
       "1           0.087246         0.004478         0.091434        0.004236\n",
       "2           0.085680         0.002036         0.089881        0.003358\n",
       "3           0.086493         0.001606         0.090755        0.002669\n",
       "4           0.086312         0.001774         0.090949        0.002665\n",
       "5           0.085911         0.002726         0.091726        0.003091\n",
       "6           0.087938         0.003024         0.091920        0.002995\n",
       "7           0.087404         0.002272         0.090997        0.003383\n",
       "8           0.087088         0.002716         0.091580        0.003372\n",
       "9           0.086166         0.002330         0.091143        0.003035\n",
       "10          0.087853         0.002414         0.091871        0.003245\n",
       "11          0.086542         0.002999         0.091677        0.003832\n",
       "12          0.086384         0.002509         0.090706        0.004049\n",
       "13          0.086299         0.002950         0.091289        0.003693\n",
       "14          0.086627         0.002440         0.090949        0.003948\n",
       "15          0.086943         0.002472         0.090561        0.003377\n",
       "16          0.086178         0.002623         0.089638        0.003469"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(\n",
    "    learning_rate =0.01,\n",
    "    n_estimators=117,\n",
    "    max_depth=4,\n",
    "    min_child_weight=3,\n",
    "    gamma=0.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    objective= 'binary:logistic',\n",
    "    n_jobs=-1,\n",
    "    reg_alpha=0,\n",
    "    scale_pos_weight=1,\n",
    "    seed=0)\n",
    "\n",
    "xgb_param = xgb.get_xgb_params()\n",
    "xgtrain = xgboost.DMatrix(X_train, label=y_train)\n",
    "\n",
    "xgboost.cv(xgb_param, xgtrain, num_boost_round=5000, nfold=5, metrics=['error'],\n",
    "     early_stopping_rounds=50, stratified=True, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'n_estimators': 900}\n",
      "Run 0 best score:  0.9174516849567835\n",
      "Run 1 best param:  {'n_estimators': 900}\n",
      "Run 1 best score:  0.917111780130135\n",
      "Run 2 best param:  {'n_estimators': 800}\n",
      "Run 2 best score:  0.917111780130135\n",
      "Run 3 best param:  {'n_estimators': 800}\n",
      "Run 3 best score:  0.9173060114596484\n",
      "Run 4 best param:  {'n_estimators': 1000}\n",
      "Run 4 best score:  0.916577643973973\n",
      "Run 5 best param:  {'n_estimators': 500}\n",
      "Run 5 best score:  0.9156064873264057\n",
      "Run 6 best param:  {'n_estimators': 700}\n",
      "Run 6 best score:  0.9180829367777023\n",
      "Run 7 best param:  {'n_estimators': 800}\n",
      "Run 7 best score:  0.915897834320676\n",
      "Run 8 best param:  {'n_estimators': 1000}\n",
      "Run 8 best score:  0.9174031271244052\n",
      "Run 9 best param:  {'n_estimators': 600}\n",
      "Run 9 best score:  0.9164805283092162\n",
      "Best params:  params               {'n_estimators': 800}\n",
      "mean_test_score_0                 0.917063\n",
      "mean_test_score_1                 0.917015\n",
      "mean_test_score_2                 0.917112\n",
      "mean_test_score_3                 0.917306\n",
      "mean_test_score_4                 0.916092\n",
      "mean_test_score_5                 0.915169\n",
      "mean_test_score_6                 0.917937\n",
      "mean_test_score_7                 0.915898\n",
      "mean_test_score_8                 0.917257\n",
      "mean_test_score_9                 0.915655\n",
      "avg                                0.91665\n",
      "Name: 7, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test8 = {\n",
    " 'n_estimators':[i for i in range(100, 1100, 100)]+[16]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score8 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=16,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch8 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test8,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch8.fit(X_train,y_train)    \n",
    "    if grid_score8.empty:\n",
    "        grid_score8 = pd.DataFrame(gsearch8.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score8.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score8['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch8.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch8.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch8.best_score_)\n",
    "\n",
    "grid_score8['avg'] = grid_score8.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score8.loc[grid_score8.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 10 times\n",
      "Run 0 best param:  {'n_estimators': 820}\n",
      "Run 0 best score:  0.9173545692920269\n",
      "Run 1 best param:  {'n_estimators': 810}\n",
      "Run 1 best score:  0.917111780130135\n",
      "Run 2 best param:  {'n_estimators': 810}\n",
      "Run 2 best score:  0.9173545692920269\n",
      "Run 3 best param:  {'n_estimators': 780}\n",
      "Run 3 best score:  0.9173545692920269\n",
      "Run 4 best param:  {'n_estimators': 750}\n",
      "Run 4 best score:  0.916577643973973\n",
      "Run 5 best param:  {'n_estimators': 750}\n",
      "Run 5 best score:  0.915509371661649\n",
      "Run 6 best param:  {'n_estimators': 760}\n",
      "Run 6 best score:  0.9181314946100806\n",
      "Run 7 best param:  {'n_estimators': 810}\n",
      "Run 7 best score:  0.9159463921530543\n",
      "Run 8 best param:  {'n_estimators': 850}\n",
      "Run 8 best score:  0.9174031271244052\n",
      "Run 9 best param:  {'n_estimators': 750}\n",
      "Run 9 best score:  0.9159463921530543\n",
      "Best params:  params               {'n_estimators': 810}\n",
      "mean_test_score_0                 0.917209\n",
      "mean_test_score_1                 0.917112\n",
      "mean_test_score_2                 0.917355\n",
      "mean_test_score_3                 0.917355\n",
      "mean_test_score_4                 0.916044\n",
      "mean_test_score_5                 0.915121\n",
      "mean_test_score_6                 0.917743\n",
      "mean_test_score_7                 0.915946\n",
      "mean_test_score_8                 0.917355\n",
      "mean_test_score_9                 0.915752\n",
      "avg                               0.916699\n",
      "Name: 6, dtype: object\n"
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = int(np.ceil(200000/train.shape[0]))\n",
    "param_test9 = {\n",
    " 'n_estimators':[i for i in range(750, 860, 10)]\n",
    "}\n",
    "# Grid search 1 cv result\n",
    "grid_score9 = pd.DataFrame()\n",
    "\n",
    "# Loop for each trial\n",
    "print('Run {} times'.format(NUM_TRIALS))\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=800,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=0)\n",
    "    five_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)\n",
    "    gsearch9 = GridSearchCV(estimator = xgb,\n",
    "                            param_grid = param_test9,\n",
    "                            scoring='accuracy',n_jobs=-1,\n",
    "                            cv=five_folds,\n",
    "                            return_train_score=False)\n",
    "    gsearch9.fit(X_train,y_train)    \n",
    "    if grid_score9.empty:\n",
    "        grid_score9 = pd.DataFrame(gsearch9.cv_results_, columns=['params', 'mean_test_score'])\n",
    "        grid_score9.columns = ['params', 'mean_test_score_0']\n",
    "    else:\n",
    "        grid_score9['mean_test_score_{}'.format(i)] = pd.DataFrame(gsearch9.cv_results_).mean_test_score\n",
    "    print('Run {} best param: '.format(i), gsearch9.best_params_)\n",
    "    print('Run {} best score: '.format(i), gsearch9.best_score_)\n",
    "\n",
    "grid_score9['avg'] = grid_score9.sum(axis=1)/NUM_TRIALS\n",
    "print('Best params: ', grid_score9.loc[grid_score9.avg.idxmax(), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Test on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0: 91.66%\n",
      "Accuracy 1: 91.67%\n",
      "Accuracy 2: 91.69%\n",
      "Accuracy 3: 91.62%\n",
      "Accuracy 4: 91.71%\n",
      "Accuracy 5: 91.68%\n",
      "Accuracy 6: 91.72%\n",
      "Accuracy 7: 91.71%\n",
      "Accuracy 8: 91.65%\n",
      "Accuracy 9: 91.69%\n",
      "Average accuracy is: 91.68%\n"
     ]
    }
   ],
   "source": [
    "accuracy_array = []\n",
    "for i in range(NUM_TRIALS):\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate =0.01,\n",
    "        n_estimators=810,\n",
    "        max_depth=4,\n",
    "        min_child_weight=3,\n",
    "        gamma=0.0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective= 'binary:logistic',\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=i\n",
    "    )\n",
    "    model = xgb.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_array.append(accuracy)\n",
    "    print('Accuracy {}: %.2f%%'.format(i) % (accuracy * 100.0))\n",
    "mean_accuracy_score = sum(accuracy_array) / NUM_TRIALS\n",
    "print('Average accuracy is: %.2f%%' % (mean_accuracy_score * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
